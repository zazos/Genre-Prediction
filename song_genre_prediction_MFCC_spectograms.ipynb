{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Query 1"
      ],
      "metadata": {
        "id": "_DhsjwueTvIz"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DZti-eERXKo"
      },
      "source": [
        "# STEP 1: Data Loading (mfccs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DUlXxW1NRXKp",
        "outputId": "53b7ac62-f6ca-4d1f-85b2-5928a1a494d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  music_genre_data_di.zip\n",
            "  inflating: music_genre_data_di/test/melgrams/labels.npy  \n",
            "  inflating: music_genre_data_di/test/melgrams/X.npy  \n",
            "  inflating: music_genre_data_di/test/mfccs/labels.npy  \n",
            "  inflating: music_genre_data_di/test/mfccs/X.npy  \n",
            "  inflating: music_genre_data_di/train/melgrams/labels.npy  \n",
            "  inflating: music_genre_data_di/train/melgrams/X.npy  \n",
            "  inflating: music_genre_data_di/train/mfccs/labels.npy  \n",
            "  inflating: music_genre_data_di/train/mfccs/X.npy  \n",
            "  inflating: music_genre_data_di/val/melgrams/labels.npy  \n",
            "  inflating: music_genre_data_di/val/melgrams/X.npy  \n",
            "  inflating: music_genre_data_di/val/mfccs/labels.npy  \n",
            "  inflating: music_genre_data_di/val/mfccs/X.npy  \n",
            "Labels: ['blues' 'classical' 'hiphop' 'rock_metal_hardrock']\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import os\n",
        "import pandas as pd\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "from sklearn.metrics import classification_report\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import matplotlib.pyplot as plt\n",
        "import torch as torch\n",
        "import pandas as pd\n",
        "from torch import optim as optim\n",
        "import torch.nn as nn\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
        "import librosa.display\n",
        "\n",
        "!unzip -o music_genre_data_di.zip\n",
        "\n",
        "X_train_melgrams = np.load('music_genre_data_di/train/melgrams/X.npy')\n",
        "y_train_melgrams = np.load('music_genre_data_di/train/melgrams/labels.npy')\n",
        "\n",
        "X_val_melgrams = np.load('music_genre_data_di/val/melgrams/X.npy')\n",
        "y_val_melgrams = np.load('music_genre_data_di/val/melgrams/labels.npy')\n",
        "\n",
        "X_test_melgrams = np.load('music_genre_data_di/test/melgrams/X.npy')\n",
        "y_test_melgrams = np.load('music_genre_data_di/test/melgrams/labels.npy')\n",
        "\n",
        "X_train_mfccs = np.load('music_genre_data_di/train/mfccs/X.npy')\n",
        "y_train_mfccs = np.load('music_genre_data_di/train/mfccs/labels.npy')\n",
        "\n",
        "X_val_mfccs = np.load('music_genre_data_di/val/mfccs/X.npy')\n",
        "y_val_mfccs = np.load('music_genre_data_di/val/mfccs/labels.npy')\n",
        "\n",
        "X_test_mfccs = np.load('music_genre_data_di/test/mfccs/X.npy')\n",
        "y_test_mfccs = np.load('music_genre_data_di/test/mfccs/labels.npy')\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "train_melgrams_labels = label_encoder.fit_transform(y_train_melgrams)\n",
        "val_melgrams_labels = label_encoder.transform(y_val_melgrams)\n",
        "test_melgrams_labels = label_encoder.transform(y_test_melgrams)\n",
        "\n",
        "train_mfccs_labels = label_encoder.fit_transform(y_train_mfccs)\n",
        "val_mfccs_labels = label_encoder.transform(y_val_mfccs)\n",
        "test_mfccs_labels = label_encoder.transform(y_test_mfccs)\n",
        "\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "train_dataset_mfcss = TensorDataset(torch.tensor(X_train_mfccs, dtype=torch.float32), torch.tensor(train_mfccs_labels, dtype=torch.long))\n",
        "val_dataset_mfcss = TensorDataset(torch.tensor(X_val_mfccs, dtype=torch.float32), torch.tensor(val_mfccs_labels, dtype=torch.long))\n",
        "test_dataset_mfcss = TensorDataset(torch.tensor(X_test_mfccs, dtype=torch.float32), torch.tensor(test_mfccs_labels, dtype=torch.long))\n",
        "\n",
        "train_loader_mfcss = DataLoader(train_dataset_mfcss, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader_mfcss = DataLoader(val_dataset_mfcss, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader_mfcss = DataLoader(test_dataset_mfcss, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "print(f\"Labels: {label_encoder.classes_}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "q_v_sLBnRXKq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d23e44f2-852e-436d-ee91-ba86ee0b01f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample 1 - MFCCs:\n",
            "[-2.00571480e+01  1.14845967e+00  1.63008675e-01  1.37694076e-01\n",
            "  1.88128635e-01  3.74476731e-01  4.14388590e-02 -4.02111793e-03\n",
            " -1.40163481e-01 -9.97298136e-02 -2.84972936e-01  2.07955137e-01\n",
            "  7.63480663e-02  2.88492024e-01  1.88307568e-01  1.83999613e-01\n",
            "  1.71379402e-01  1.52930766e-01  2.94320583e-01  2.35651895e-01\n",
            "  1.58832505e-01  2.00819522e-01  1.80156574e-01  1.01926975e-01\n",
            "  1.69139713e-01  1.75065205e-01]\n",
            "Label: 3\n",
            "Sample 2 - MFCCs:\n",
            "[-2.1455748e+01  1.3518907e+00 -1.8830597e-01  6.3494790e-01\n",
            "  5.9023663e-02 -4.6485245e-01 -1.9929364e-01  1.6416946e-01\n",
            " -2.6687047e-01  2.8228839e-03  2.9179800e-01 -1.5691857e-01\n",
            " -2.2347954e-01  7.9653865e-01  4.5895812e-01  4.7369137e-01\n",
            "  2.0781736e-01  2.0862083e-01  2.6906487e-01  2.3442554e-01\n",
            "  2.5946116e-01  2.2056428e-01  1.6505070e-01  2.3479937e-01\n",
            "  1.8292448e-01  2.4815394e-01]\n",
            "Label: 0\n",
            "Sample 3 - MFCCs:\n",
            "[-23.099052     2.2080278    0.5796435    0.07870232  -0.23385742\n",
            "   0.29434007  -0.04499927  -0.06031458   0.24207012  -0.16860121\n",
            "  -0.36589712  -0.05454727  -0.11727321   0.9461844    0.37869018\n",
            "   0.30029768   0.3587777    0.3536333    0.25057003   0.27562207\n",
            "   0.25121078   0.16999525   0.35247552   0.20855637   0.29481882\n",
            "   0.38882938]\n",
            "Label: 0\n",
            "Sample 4 - MFCCs:\n",
            "[-23.138582     2.498752     0.07781474   0.8356006    0.32844162\n",
            "   0.09886018   0.14725707   0.2656513    0.13751645   0.13884014\n",
            "  -0.08822955  -0.0741965   -0.08621228   0.8278803    0.48361322\n",
            "   0.37772226   0.43778798   0.39813927   0.19364323   0.19221452\n",
            "   0.2731574    0.17128819   0.14249973   0.20550933   0.18759756\n",
            "   0.12916864]\n",
            "Label: 3\n",
            "Sample 5 - MFCCs:\n",
            "[-2.2535192e+01  2.5192833e+00 -1.0377764e+00 -5.4848379e-01\n",
            " -5.5737448e-01  2.7375117e-01 -6.1569251e-03 -4.5208305e-01\n",
            " -6.1315519e-01 -5.6181496e-01 -9.3543455e-02  1.2968363e-01\n",
            " -1.8360637e-01  6.3360673e-01  4.7718796e-01  2.4183199e-01\n",
            "  3.1061444e-01  3.6772373e-01  1.6695714e-01  3.9216384e-01\n",
            "  2.2836056e-01  2.7847907e-01  2.5175467e-01  2.0969373e-01\n",
            "  2.6619932e-01  1.5102865e-01]\n",
            "Label: 1\n"
          ]
        }
      ],
      "source": [
        "# Preview the first batch of MFCCs data and labels from the training set using the data loader\n",
        "data_iter = iter(train_loader_mfcss)\n",
        "mfccs_batch, labels_batch = next(data_iter)\n",
        "\n",
        "mfccs_batch = mfccs_batch.numpy()\n",
        "labels_batch = labels_batch.numpy()\n",
        "\n",
        "for i in range(5):\n",
        "    print(f\"Sample {i + 1} - MFCCs:\\n{mfccs_batch[i]}\\nLabel: {labels_batch[i]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BA61G_VrRXKq"
      },
      "source": [
        "Sample 1 - MFCCs: \\\n",
        "[-2.2553576e+01  2.7285593e+00  2.5918977e-02  1.1022884e-01 \\\n",
        " -3.1578171e-01 -2.5006631e-02  1.5556138e-02  1.7535387e-02 \\\n",
        "  3.7025798e-02  1.5487538e-01 -2.2948222e-01  5.0243538e-02 \\\n",
        " -1.9217800e-01  1.0216693e+00  5.4664624e-01  4.9118587e-01 \\\n",
        "  4.8797745e-01  3.1588015e-01  3.1959847e-01  2.6229122e-01 \\\n",
        "  2.3018837e-01  2.4441515e-01  2.9878369e-01  2.6426667e-01 \\\n",
        "  1.9473237e-01  2.8674385e-01] \\\n",
        "Label: 3\\\n",
        "The vector contains 26 values. The first 13 values represent the mean of each MFCC coefficient across the 20 time frames, and the last 13 values represent the standard deviation of each MFCC coefficient across the same time frames."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fy58fEE8RXKq"
      },
      "source": [
        "# STEP 2: Neural Network Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "RLuWPuKdRXKq"
      },
      "outputs": [],
      "source": [
        "class MusicGenreCLF(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MusicGenreCLF, self).__init__()\n",
        "        self.fc_layers = nn.Sequential(\n",
        "            nn.Linear(26, 128),\n",
        "            nn.Linear(128, 32),\n",
        "            nn.Linear(32, 4)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc_layers(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sODupHarRXKq"
      },
      "source": [
        "# STEP 3: Training Process\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "jtfjH8ZuRXKr"
      },
      "outputs": [],
      "source": [
        "def train_loop(dataloader, model, loss_fn, optimizer, device):\n",
        "    device = torch.device(\"cuda\" if device and torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    model.train()\n",
        "    size = len(dataloader.dataset)\n",
        "    total_loss = 0.0\n",
        "    for X, y in dataloader:\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / size\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzEvLidPRXKr"
      },
      "source": [
        "# STEP 4: Evaluation Process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "UbKrmjC5RXKr"
      },
      "outputs": [],
      "source": [
        "def test_loop(dataloader, model, loss_fn, device):\n",
        "    device = torch.device(\"cuda\" if device and torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    model.eval()\n",
        "    preds = []\n",
        "    labels = []\n",
        "    total_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "\n",
        "            pred = model(X)\n",
        "            loss = loss_fn(pred, y)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            _, pred = torch.max(pred, 1)\n",
        "            preds.extend(pred.tolist())\n",
        "            labels.extend(y.tolist())\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    f1 = f1_score(labels, preds, average='weighted')\n",
        "    cm = confusion_matrix(labels, preds)\n",
        "\n",
        "    return avg_loss, acc, f1, cm\n",
        "\n",
        "    from sklearn.metrics import classification_report\n",
        "\n",
        "def evaluate(dataloader, model, loss_fn, device):\n",
        "    device = torch.device(\"cuda\" if device and torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    model.eval()\n",
        "    size = len(dataloader.dataset)\n",
        "    test_loss = 0.0\n",
        "    correct = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "            all_preds.extend(pred.argmax(1).cpu().numpy())\n",
        "            all_labels.extend(y.cpu().numpy())\n",
        "\n",
        "    test_loss /= size\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "\n",
        "    print(classification_report(all_labels, all_preds, target_names=label_encoder.classes_))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jiEpRKbqRXKr"
      },
      "source": [
        "# STEP 5: Train and Evaluation of the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Z7OwqBbVRXKr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dddad627-5cc7-4424-95ca-7890cf8738db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training\n",
            "Epoch [1/30], Train Loss: 0.08653075013309718\n",
            "Epoch [1/30], Val Loss: 1.2688173985481261, Val F1: 0.583871410414616, Val Accuracy: 0.58\n",
            "Epoch [2/30], Train Loss: 0.07914209309965373\n",
            "Epoch [2/30], Val Loss: 1.1521188855171203, Val F1: 0.5694176671664362, Val Accuracy: 0.6\n",
            "Epoch [3/30], Train Loss: 0.07358855783939361\n",
            "Epoch [3/30], Val Loss: 1.034891300201416, Val F1: 0.6071146297814211, Val Accuracy: 0.6225\n",
            "Epoch [4/30], Train Loss: 0.07086151972413063\n",
            "Epoch [4/30], Val Loss: 0.9805024111270905, Val F1: 0.632621953731645, Val Accuracy: 0.6525\n",
            "Epoch [5/30], Train Loss: 0.06837102390825749\n",
            "Epoch [5/30], Val Loss: 0.9606882119178772, Val F1: 0.6011278196377474, Val Accuracy: 0.6275\n",
            "Epoch [6/30], Train Loss: 0.06880351897329091\n",
            "Epoch [6/30], Val Loss: 1.0504537308216095, Val F1: 0.5323326920081485, Val Accuracy: 0.55125\n",
            "Epoch [7/30], Train Loss: 0.06587118532508612\n",
            "Epoch [7/30], Val Loss: 0.9236010694503785, Val F1: 0.6314577736497133, Val Accuracy: 0.64375\n",
            "Epoch [8/30], Train Loss: 0.06461875203996897\n",
            "Epoch [8/30], Val Loss: 0.9216228151321411, Val F1: 0.696389356983655, Val Accuracy: 0.69\n",
            "Epoch [9/30], Train Loss: 0.06412698164582252\n",
            "Epoch [9/30], Val Loss: 0.929318745136261, Val F1: 0.60021702617066, Val Accuracy: 0.63\n",
            "Epoch [10/30], Train Loss: 0.06332392044365406\n",
            "Epoch [10/30], Val Loss: 0.9053836107254029, Val F1: 0.6560232151057499, Val Accuracy: 0.65625\n",
            "Epoch [11/30], Train Loss: 0.06278848191723227\n",
            "Epoch [11/30], Val Loss: 0.891453458070755, Val F1: 0.6507016451766069, Val Accuracy: 0.6575\n",
            "Epoch [12/30], Train Loss: 0.06242656197398901\n",
            "Epoch [12/30], Val Loss: 0.9369772183895111, Val F1: 0.5621262630216306, Val Accuracy: 0.58625\n",
            "Epoch [13/30], Train Loss: 0.06201177893206477\n",
            "Epoch [13/30], Val Loss: 1.0545530307292938, Val F1: 0.45948858507160223, Val Accuracy: 0.5325\n",
            "Epoch [14/30], Train Loss: 0.061961418837308886\n",
            "Epoch [14/30], Val Loss: 0.9959494233131408, Val F1: 0.5608132921486804, Val Accuracy: 0.575\n",
            "Epoch [15/30], Train Loss: 0.06124854326248169\n",
            "Epoch [15/30], Val Loss: 0.8742662036418914, Val F1: 0.6464117473724105, Val Accuracy: 0.66375\n",
            "Epoch [16/30], Train Loss: 0.05983733640983701\n",
            "Epoch [16/30], Val Loss: 0.9846353185176849, Val F1: 0.5246667453984527, Val Accuracy: 0.55\n",
            "Epoch [17/30], Train Loss: 0.060343566089868544\n",
            "Epoch [17/30], Val Loss: 0.872690179347992, Val F1: 0.5975450257445395, Val Accuracy: 0.62875\n",
            "Epoch [18/30], Train Loss: 0.05932886354625225\n",
            "Epoch [18/30], Val Loss: 0.8886069256067276, Val F1: 0.6507028028009421, Val Accuracy: 0.6675\n",
            "Epoch [19/30], Train Loss: 0.0596693573705852\n",
            "Epoch [19/30], Val Loss: 0.8589751601219178, Val F1: 0.6078318476113488, Val Accuracy: 0.6325\n",
            "Epoch [20/30], Train Loss: 0.0590980498213321\n",
            "Epoch [20/30], Val Loss: 0.8418547594547272, Val F1: 0.6836141772729043, Val Accuracy: 0.685\n",
            "Epoch [21/30], Train Loss: 0.05864414012059569\n",
            "Epoch [21/30], Val Loss: 0.9609836614131928, Val F1: 0.5804290124781856, Val Accuracy: 0.58875\n",
            "Epoch [22/30], Train Loss: 0.058392410706728695\n",
            "Epoch [22/30], Val Loss: 0.9107438349723815, Val F1: 0.6178048553864675, Val Accuracy: 0.62125\n",
            "Epoch [23/30], Train Loss: 0.057933668438345194\n",
            "Epoch [23/30], Val Loss: 0.8256850421428681, Val F1: 0.6679090041233067, Val Accuracy: 0.6725\n",
            "Epoch [24/30], Train Loss: 0.05888588949106634\n",
            "Epoch [24/30], Val Loss: 0.8896382141113282, Val F1: 0.5973769921965174, Val Accuracy: 0.61\n",
            "Epoch [25/30], Train Loss: 0.058369013825431464\n",
            "Epoch [25/30], Val Loss: 0.8828701889514923, Val F1: 0.5725917828407965, Val Accuracy: 0.61375\n",
            "Epoch [26/30], Train Loss: 0.0573129128292203\n",
            "Epoch [26/30], Val Loss: 0.8170762753486633, Val F1: 0.6567168903027357, Val Accuracy: 0.66875\n",
            "Epoch [27/30], Train Loss: 0.057589656114578246\n",
            "Epoch [27/30], Val Loss: 0.8057308149337769, Val F1: 0.6987983234017002, Val Accuracy: 0.70125\n",
            "Epoch [28/30], Train Loss: 0.0576576114166528\n",
            "Epoch [28/30], Val Loss: 0.8470053052902222, Val F1: 0.668898256804668, Val Accuracy: 0.67375\n",
            "Epoch [29/30], Train Loss: 0.0570865086093545\n",
            "Epoch [29/30], Val Loss: 0.8137100660800933, Val F1: 0.6995788430333859, Val Accuracy: 0.695\n",
            "Epoch [30/30], Train Loss: 0.05653009347617626\n",
            "Epoch [30/30], Val Loss: 0.8143429166078567, Val F1: 0.6774310105132632, Val Accuracy: 0.6775\n",
            "Testing\n",
            "Test Error: \n",
            " Accuracy: 64.3%, Avg loss: 0.058693 \n",
            "\n",
            "                     precision    recall  f1-score   support\n",
            "\n",
            "              blues       0.44      0.39      0.41       324\n",
            "          classical       0.78      0.80      0.79       297\n",
            "             hiphop       0.63      0.84      0.72       356\n",
            "rock_metal_hardrock       0.72      0.56      0.63       399\n",
            "\n",
            "           accuracy                           0.64      1376\n",
            "          macro avg       0.64      0.65      0.64      1376\n",
            "       weighted avg       0.64      0.64      0.64      1376\n",
            "\n"
          ]
        }
      ],
      "source": [
        "model = MusicGenreCLF()\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=2e-2)\n",
        "EPOCHS = 30\n",
        "\n",
        "print('Training')\n",
        "for epoch in range(EPOCHS):\n",
        "    train_loss = train_loop(train_loader_mfcss, model, loss_fn, optimizer, device=False)\n",
        "    print(f'Epoch [{epoch+1}/{EPOCHS}], Train Loss: {train_loss}')\n",
        "    valid_loss, valid_acc, valid_f1, valid_cm = test_loop(val_loader_mfcss, model, loss_fn, device=False)\n",
        "    print(f'Epoch [{epoch+1}/{EPOCHS}], Val Loss: {valid_loss}, Val F1: {valid_f1}, Val Accuracy: {valid_acc}')\n",
        "\n",
        "print('Testing')\n",
        "evaluate(test_loader_mfcss, model, loss_fn, device=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KawO9bdZRXKr"
      },
      "source": [
        "# STEP 6: Train and Evaluation with GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "-5nrADNhRXKr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68be06b9-6c9a-40f9-a203-7c3ace17bab4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training\n",
            "Epoch [1/30], Loss: 0.0867767545953393\n",
            "Epoch [1/30], Val Loss: 1.3257431721687316, Val F1: 0.10515089585110815, Val Accuracy: 0.2525\n",
            "Epoch [2/30], Loss: 0.07929713539779186\n",
            "Epoch [2/30], Val Loss: 1.1534794974327087, Val F1: 0.6598675299363415, Val Accuracy: 0.65875\n",
            "Epoch [3/30], Loss: 0.07485647335648536\n",
            "Epoch [3/30], Val Loss: 1.4917879009246826, Val F1: 0.15392092017964487, Val Accuracy: 0.2775\n",
            "Epoch [4/30], Loss: 0.07152380371466278\n",
            "Epoch [4/30], Val Loss: 1.00699911236763, Val F1: 0.6692632042647881, Val Accuracy: 0.6725\n",
            "Epoch [5/30], Loss: 0.06839509885758162\n",
            "Epoch [5/30], Val Loss: 1.0884050559997558, Val F1: 0.46534193126904083, Val Accuracy: 0.495\n",
            "Epoch [6/30], Loss: 0.06780358403921127\n",
            "Epoch [6/30], Val Loss: 1.1614686524868012, Val F1: 0.39226013338976806, Val Accuracy: 0.455\n",
            "Epoch [7/30], Loss: 0.0651465354114771\n",
            "Epoch [7/30], Val Loss: 0.9767311418056488, Val F1: 0.5122695422045227, Val Accuracy: 0.54625\n",
            "Epoch [8/30], Loss: 0.065479003097862\n",
            "Epoch [8/30], Val Loss: 1.0355711424350738, Val F1: 0.46612933551185165, Val Accuracy: 0.5225\n",
            "Epoch [9/30], Loss: 0.06412806509062648\n",
            "Epoch [9/30], Val Loss: 0.9138425350189209, Val F1: 0.6500132204329766, Val Accuracy: 0.64875\n",
            "Epoch [10/30], Loss: 0.06339130016043783\n",
            "Epoch [10/30], Val Loss: 1.037074418067932, Val F1: 0.45258461680862716, Val Accuracy: 0.5225\n",
            "Epoch [11/30], Loss: 0.06205975830554962\n",
            "Epoch [11/30], Val Loss: 0.9692905628681183, Val F1: 0.5325556738108745, Val Accuracy: 0.57625\n",
            "Epoch [12/30], Loss: 0.06199704961851239\n",
            "Epoch [12/30], Val Loss: 1.1423629820346832, Val F1: 0.4105218357031392, Val Accuracy: 0.4975\n",
            "Epoch [13/30], Loss: 0.06232122965157032\n",
            "Epoch [13/30], Val Loss: 0.9325174862146377, Val F1: 0.562227453225108, Val Accuracy: 0.60875\n",
            "Epoch [14/30], Loss: 0.06142846342176199\n",
            "Epoch [14/30], Val Loss: 0.8804811120033265, Val F1: 0.6814723639110912, Val Accuracy: 0.67625\n",
            "Epoch [15/30], Loss: 0.060736774448305365\n",
            "Epoch [15/30], Val Loss: 0.9605517518520356, Val F1: 0.5565149839353849, Val Accuracy: 0.585\n",
            "Epoch [16/30], Loss: 0.060790062304586175\n",
            "Epoch [16/30], Val Loss: 0.8474326038360596, Val F1: 0.6663134530868398, Val Accuracy: 0.67625\n",
            "Epoch [17/30], Loss: 0.05973130160011351\n",
            "Epoch [17/30], Val Loss: 0.8665591794252395, Val F1: 0.6292632207153113, Val Accuracy: 0.63875\n",
            "Epoch [18/30], Loss: 0.060558122526854274\n",
            "Epoch [18/30], Val Loss: 0.8912992143630981, Val F1: 0.6230700475709497, Val Accuracy: 0.63875\n",
            "Epoch [19/30], Loss: 0.059329138025641444\n",
            "Epoch [19/30], Val Loss: 0.9248424172401428, Val F1: 0.5622987818860813, Val Accuracy: 0.59875\n",
            "Epoch [20/30], Loss: 0.0588854502607137\n",
            "Epoch [20/30], Val Loss: 0.8891575181484223, Val F1: 0.6359468940168341, Val Accuracy: 0.65125\n",
            "Epoch [21/30], Loss: 0.05865789666771889\n",
            "Epoch [21/30], Val Loss: 0.8746073943376541, Val F1: 0.6062293233198947, Val Accuracy: 0.615\n",
            "Epoch [22/30], Loss: 0.058198074037209155\n",
            "Epoch [22/30], Val Loss: 0.8961112689971924, Val F1: 0.6220937974653181, Val Accuracy: 0.615\n",
            "Epoch [23/30], Loss: 0.05842997977510095\n",
            "Epoch [23/30], Val Loss: 0.8120696341991425, Val F1: 0.7026485739564846, Val Accuracy: 0.70125\n",
            "Epoch [24/30], Loss: 0.05753922686912119\n",
            "Epoch [24/30], Val Loss: 0.8076518380641937, Val F1: 0.7058125062316278, Val Accuracy: 0.71125\n",
            "Epoch [25/30], Loss: 0.05831439019180834\n",
            "Epoch [25/30], Val Loss: 0.853520051240921, Val F1: 0.606458409468501, Val Accuracy: 0.66\n",
            "Epoch [26/30], Loss: 0.05814499305561185\n",
            "Epoch [26/30], Val Loss: 0.8948033344745636, Val F1: 0.6117114717675876, Val Accuracy: 0.62125\n",
            "Epoch [27/30], Loss: 0.05804400070570409\n",
            "Epoch [27/30], Val Loss: 0.822371780872345, Val F1: 0.696440128996518, Val Accuracy: 0.6975\n",
            "Epoch [28/30], Loss: 0.05701852947473526\n",
            "Epoch [28/30], Val Loss: 0.8752215838432312, Val F1: 0.6367992092428618, Val Accuracy: 0.64\n",
            "Epoch [29/30], Loss: 0.05773911128751934\n",
            "Epoch [29/30], Val Loss: 0.8038202118873596, Val F1: 0.6875826230372691, Val Accuracy: 0.68625\n",
            "Epoch [30/30], Loss: 0.05754606876522303\n",
            "Epoch [30/30], Val Loss: 0.9696820008754731, Val F1: 0.5598418919745582, Val Accuracy: 0.57625\n",
            "Testing\n",
            "Test Error: \n",
            " Accuracy: 57.0%, Avg loss: 0.064477 \n",
            "\n",
            "                     precision    recall  f1-score   support\n",
            "\n",
            "              blues       0.39      0.12      0.18       324\n",
            "          classical       0.91      0.55      0.68       297\n",
            "             hiphop       0.68      0.65      0.66       356\n",
            "rock_metal_hardrock       0.47      0.89      0.61       399\n",
            "\n",
            "           accuracy                           0.57      1376\n",
            "          macro avg       0.61      0.55      0.53      1376\n",
            "       weighted avg       0.60      0.57      0.54      1376\n",
            "\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = MusicGenreCLF().to(device)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=2e-2)\n",
        "\n",
        "print('Training')\n",
        "for epoch in range(EPOCHS):\n",
        "    train_loss = train_loop(train_loader_mfcss, model, loss_fn, optimizer, device=True)\n",
        "    print(f'Epoch [{epoch+1}/{EPOCHS}], Loss: {train_loss}')\n",
        "    valid_loss, valid_acc, valid_f1, valid_cm = test_loop(val_loader_mfcss, model, loss_fn, device=True)\n",
        "    print(f'Epoch [{epoch+1}/{EPOCHS}], Val Loss: {valid_loss}, Val F1: {valid_f1}, Val Accuracy: {valid_acc}')\n",
        "\n",
        "print('Testing')\n",
        "evaluate(test_loader_mfcss, model, loss_fn, device=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkJDS_EVRXKr"
      },
      "source": [
        "# STEP 7: Save best model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "WPDF0CAuRXKr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "805172ba-7a55-45f2-e571-f5c3a059948d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training\n",
            "Epoch [1/30], Loss: 0.08809608481824398\n",
            "Epoch [1/30], Val Loss: 1.3079444193840026, Val F1: 0.1746561094662441, Val Accuracy: 0.28375\n",
            "Best model with F1: 0.1746561094662441 saved at epoch 1\n",
            "Epoch [2/30], Loss: 0.07983489764854312\n",
            "Epoch [2/30], Val Loss: 1.163486659526825, Val F1: 0.5449282938564026, Val Accuracy: 0.6\n",
            "Best model with F1: 0.5449282938564026 saved at epoch 2\n",
            "Epoch [3/30], Loss: 0.07473854010924696\n",
            "Epoch [3/30], Val Loss: 1.2456339287757874, Val F1: 0.3068937286697836, Val Accuracy: 0.42\n",
            "Epoch [4/30], Loss: 0.07259864827618003\n",
            "Epoch [4/30], Val Loss: 1.0282171392440795, Val F1: 0.56879868928409, Val Accuracy: 0.60375\n",
            "Best model with F1: 0.56879868928409 saved at epoch 4\n",
            "Epoch [5/30], Loss: 0.07047148266807199\n",
            "Epoch [5/30], Val Loss: 1.2765296697616577, Val F1: 0.2981325788480326, Val Accuracy: 0.4025\n",
            "Epoch [6/30], Loss: 0.06820834152400494\n",
            "Epoch [6/30], Val Loss: 0.9939402735233307, Val F1: 0.5523432737960017, Val Accuracy: 0.57\n",
            "Epoch [7/30], Loss: 0.0664465550519526\n",
            "Epoch [7/30], Val Loss: 0.9892825639247894, Val F1: 0.5185555809435362, Val Accuracy: 0.56875\n",
            "Epoch [8/30], Loss: 0.06563055716454982\n",
            "Epoch [8/30], Val Loss: 0.9643837916851044, Val F1: 0.5929144318778713, Val Accuracy: 0.615\n",
            "Best model with F1: 0.5929144318778713 saved at epoch 8\n",
            "Epoch [9/30], Loss: 0.06402196573093534\n",
            "Epoch [9/30], Val Loss: 0.9890249133110046, Val F1: 0.561092002158249, Val Accuracy: 0.5925\n",
            "Epoch [10/30], Loss: 0.06479470061138272\n",
            "Epoch [10/30], Val Loss: 0.9785426592826844, Val F1: 0.513580605983971, Val Accuracy: 0.55125\n",
            "Epoch [11/30], Loss: 0.06316529002971948\n",
            "Epoch [11/30], Val Loss: 0.9899911260604859, Val F1: 0.500454364700164, Val Accuracy: 0.55125\n",
            "Epoch [12/30], Loss: 0.06253428783267737\n",
            "Epoch [12/30], Val Loss: 0.8725204241275787, Val F1: 0.6743118192618133, Val Accuracy: 0.6825\n",
            "Best model with F1: 0.6743118192618133 saved at epoch 12\n",
            "Epoch [13/30], Loss: 0.061834966950118544\n",
            "Epoch [13/30], Val Loss: 0.9111530411243439, Val F1: 0.5489329686651371, Val Accuracy: 0.60125\n",
            "Epoch [14/30], Loss: 0.0622764341160655\n",
            "Epoch [14/30], Val Loss: 0.8912648975849151, Val F1: 0.6220331767693935, Val Accuracy: 0.64125\n",
            "Epoch [15/30], Loss: 0.06015688171610236\n",
            "Epoch [15/30], Val Loss: 0.9104684627056122, Val F1: 0.588766923272921, Val Accuracy: 0.595\n",
            "Epoch [16/30], Loss: 0.06096780585125089\n",
            "Epoch [16/30], Val Loss: 0.8524945831298828, Val F1: 0.6812499115281865, Val Accuracy: 0.6775\n",
            "Best model with F1: 0.6812499115281865 saved at epoch 16\n",
            "Epoch [17/30], Loss: 0.0600479319691658\n",
            "Epoch [17/30], Val Loss: 0.8670269274711608, Val F1: 0.6641812611064262, Val Accuracy: 0.67\n",
            "Epoch [18/30], Loss: 0.05960787445306778\n",
            "Epoch [18/30], Val Loss: 0.8626554381847381, Val F1: 0.6477025070142095, Val Accuracy: 0.6525\n",
            "Epoch [19/30], Loss: 0.05886577692814171\n",
            "Epoch [19/30], Val Loss: 0.840577255487442, Val F1: 0.6743575954368759, Val Accuracy: 0.68375\n",
            "Epoch [20/30], Loss: 0.05963459827005863\n",
            "Epoch [20/30], Val Loss: 0.8406707370281219, Val F1: 0.6332853138746551, Val Accuracy: 0.675\n",
            "Epoch [21/30], Loss: 0.058766046296805145\n",
            "Epoch [21/30], Val Loss: 0.8465209436416626, Val F1: 0.6279205039542726, Val Accuracy: 0.64875\n",
            "Epoch [22/30], Loss: 0.059350256696343424\n",
            "Epoch [22/30], Val Loss: 0.8403117167949676, Val F1: 0.6661334145286147, Val Accuracy: 0.67625\n",
            "Epoch [23/30], Loss: 0.059599256757646796\n",
            "Epoch [23/30], Val Loss: 0.927272777557373, Val F1: 0.5767974261249708, Val Accuracy: 0.61\n",
            "Epoch [24/30], Loss: 0.05816816164180636\n",
            "Epoch [24/30], Val Loss: 0.8649120914936066, Val F1: 0.5954961340452783, Val Accuracy: 0.6275\n",
            "Epoch [25/30], Loss: 0.05831955336034298\n",
            "Epoch [25/30], Val Loss: 0.8286260378360748, Val F1: 0.6596512205907018, Val Accuracy: 0.66625\n",
            "Epoch [26/30], Loss: 0.05815510670654476\n",
            "Epoch [26/30], Val Loss: 0.8212615311145782, Val F1: 0.6462705960033732, Val Accuracy: 0.6825\n",
            "Epoch [27/30], Loss: 0.05812934162095189\n",
            "Epoch [27/30], Val Loss: 0.890444016456604, Val F1: 0.6173367713662649, Val Accuracy: 0.63125\n",
            "Epoch [28/30], Loss: 0.05780660482123494\n",
            "Epoch [28/30], Val Loss: 0.8181935393810272, Val F1: 0.6987862751722304, Val Accuracy: 0.705\n",
            "Best model with F1: 0.6987862751722304 saved at epoch 28\n",
            "Epoch [29/30], Loss: 0.05784616922028363\n",
            "Epoch [29/30], Val Loss: 0.8793808722496033, Val F1: 0.611610794658177, Val Accuracy: 0.6225\n",
            "Epoch [30/30], Loss: 0.057174443872645496\n",
            "Epoch [30/30], Val Loss: 0.7955786597728729, Val F1: 0.6757510619135901, Val Accuracy: 0.68375\n",
            "Testing\n",
            "Test Error: \n",
            " Accuracy: 64.9%, Avg loss: 0.057033 \n",
            "\n",
            "                     precision    recall  f1-score   support\n",
            "\n",
            "              blues       0.46      0.30      0.36       324\n",
            "          classical       0.81      0.77      0.79       297\n",
            "             hiphop       0.65      0.80      0.72       356\n",
            "rock_metal_hardrock       0.64      0.71      0.67       399\n",
            "\n",
            "           accuracy                           0.65      1376\n",
            "          macro avg       0.64      0.64      0.63      1376\n",
            "       weighted avg       0.64      0.65      0.64      1376\n",
            "\n"
          ]
        }
      ],
      "source": [
        "model = MusicGenreCLF().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=2e-2)\n",
        "\n",
        "best_f1 = 0\n",
        "best_model = None\n",
        "\n",
        "print('Training')\n",
        "for epoch in range(EPOCHS):\n",
        "    total_loss = train_loop(train_loader_mfcss, model, criterion, optimizer, device=True)\n",
        "    print(f'Epoch [{epoch+1}/{EPOCHS}], Loss: {total_loss}')\n",
        "\n",
        "    val_loss, val_acc, val_f1, val_cm = test_loop(val_loader_mfcss, model, criterion, device=True)\n",
        "    print(f'Epoch [{epoch+1}/{EPOCHS}], Val Loss: {val_loss}, Val F1: {val_f1}, Val Accuracy: {val_acc}')\n",
        "\n",
        "    if val_f1 > best_f1:\n",
        "        best_f1 = val_f1\n",
        "        best_model = model.state_dict()\n",
        "        print(f'Best model with F1: {best_f1} saved at epoch {epoch+1}')\n",
        "        torch.save(best_model, 'best_model.pth')\n",
        "\n",
        "model.load_state_dict(best_model)\n",
        "\n",
        "print('Testing')\n",
        "evaluate(test_loader_mfcss, model, criterion, device=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kvLAz2FRXKr"
      },
      "source": [
        "### Using a GPU does not impact the improvement in predictions of any model. But choosing the best model through validation on the validation dataset shows better performance for some classes like, but not optimal for all. Its obvious that this method helped in choosing a model that generalizes better, but there's definitely room for improvement, for instance getting a larger dataset or even tweaking the architecture of the NN."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Query 2"
      ],
      "metadata": {
        "id": "2NdTOM6nTzka"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STEP 1: Data Loading (mfccs)"
      ],
      "metadata": {
        "id": "waHhyjXRcpQ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_melgrams = np.load('music_genre_data_di/train/melgrams/X.npy')\n",
        "y_train_melgrams = np.load('music_genre_data_di/train/melgrams/labels.npy')\n",
        "\n",
        "X_val_melgrams = np.load('music_genre_data_di/val/melgrams/X.npy')\n",
        "y_val_melgrams = np.load('music_genre_data_di/val/melgrams/labels.npy')\n",
        "\n",
        "X_test_melgrams = np.load('music_genre_data_di/test/melgrams/X.npy')\n",
        "y_test_melgrams = np.load('music_genre_data_di/test/melgrams/labels.npy')\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "train_melgrams_labels = label_encoder.fit_transform(y_train_melgrams)\n",
        "val_melgrams_labels = label_encoder.transform(y_val_melgrams)\n",
        "test_melgrams_labels = label_encoder.transform(y_test_melgrams)\n",
        "\n",
        "train_melgrams_labels = label_encoder.fit_transform(y_train_melgrams)\n",
        "val_melgrams_labels = label_encoder.transform(y_val_melgrams)\n",
        "test_melgrams_labels = label_encoder.transform(y_test_melgrams)\n",
        "\n",
        "BATCH_SIZE = 16\n",
        "LR = 1e-3\n",
        "\n",
        "train_dataset_melgrams = TensorDataset(torch.tensor(X_train_melgrams, dtype=torch.float32), torch.tensor(train_melgrams_labels, dtype=torch.long))\n",
        "val_dataset_melgrams = TensorDataset(torch.tensor(X_val_melgrams, dtype=torch.float32), torch.tensor(val_melgrams_labels, dtype=torch.long))\n",
        "test_dataset_melgrams = TensorDataset(torch.tensor(X_test_melgrams, dtype=torch.float32), torch.tensor(test_melgrams_labels, dtype=torch.long))\n",
        "\n",
        "train_loader_melgrams = DataLoader(train_dataset_melgrams, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader_melgrams = DataLoader(val_dataset_melgrams, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader_melgrams = DataLoader(test_dataset_melgrams, batch_size=BATCH_SIZE, shuffle=False)\n"
      ],
      "metadata": {
        "id": "QWM7gAVsT2MI"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_random_samples(X, y, label_encoder):\n",
        "    classes = np.unique(y)\n",
        "    fig, axes = plt.subplots(1, len(classes), figsize=(15, 5))\n",
        "    for i, cls in enumerate(classes):\n",
        "        idx = np.random.choice(np.where(y == cls)[0])\n",
        "        mel_spectrogram = X[idx].squeeze()\n",
        "        ax = axes[i]\n",
        "        img = librosa.display.specshow(mel_spectrogram, sr=22050, x_axis='time', y_axis='mel', ax=ax)\n",
        "        ax.set_title(label_encoder.inverse_transform([cls])[0])\n",
        "        fig.colorbar(img, ax=ax, format='%+2.0f dB')\n",
        "    plt.show()\n",
        "\n",
        "visualize_random_samples(X_train_melgrams, train_melgrams_labels, label_encoder)"
      ],
      "metadata": {
        "id": "OQ38OGnuT3Zq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "outputId": "a0ad9101-6b25-4ae6-ab96-b6159cbf8b6c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x500 with 8 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "source: https://towardsdatascience.com/getting-to-know-the-mel-spectrogram-31bca3e2d9d0\n",
        "\n",
        "Bright yellow areas (0 dB) indicate loud sounds and darker areas represent quieter sounds"
      ],
      "metadata": {
        "id": "WH4_vlsJXcZE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class MusicGenreCNN(nn.Module):\n",
        "    def __init__(self, out_dim):\n",
        "        super(MusicGenreCNN, self).__init__()\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            nn.Conv2d(1, 16, kernel_size=5),\n",
        "            nn.Conv2d(16, 32, kernel_size=5),\n",
        "            nn.Conv2d(32, 64, kernel_size=5),\n",
        "            nn.Conv2d(64, 128, kernel_size=5),\n",
        "        )\n",
        "\n",
        "        self._to_linear = None\n",
        "        self.calculate_linear_output((1, 21, 128))\n",
        "        print(f'Linear output: {self._to_linear}')\n",
        "        self.fc_layers = nn.Sequential(\n",
        "            nn.Linear(self._to_linear, 1024),\n",
        "            nn.Linear(1024, 256),\n",
        "            nn.Linear(256, 32),\n",
        "            nn.Linear(32, out_dim)\n",
        "        )\n",
        "\n",
        "    def calculate_linear_output(self, shape):\n",
        "        # output = torch.ones(1, *shape)\n",
        "        # print(f'Input shape: {output.shape}')\n",
        "        # output = self.conv1(output)\n",
        "        # print(f'Conv1 shape: {output.shape}')\n",
        "        # output = self.conv2(output)\n",
        "        # print(f'Conv2 shape: {output.shape}')\n",
        "        # output = self.conv3(output)\n",
        "        # print(f'Conv3 shape: {output.shape}')\n",
        "        # output = self.conv4(output)\n",
        "        # print(f'Conv4 shape: {output.shape}')\n",
        "        # self._to_linear = output.numel()\n",
        "        output = torch.ones(1, *shape)\n",
        "        print(f'Input shape: {output.shape}')\n",
        "        output = self.conv_layers(output)\n",
        "        print(f'Conv shape: {output.shape}')\n",
        "        self._to_linear = output.numel()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_layers(x)\n",
        "        x = x.view(x.size(0), -1) # Flatten the tensor\n",
        "        x = self.fc_layers(x)\n",
        "        return x\n",
        "\n",
        "num_classes = len(label_encoder.classes_)\n",
        "print(f'Number of classes: {num_classes}')\n",
        "print(f\"Class names {label_encoder.classes_}\")\n",
        "\n",
        "model = MusicGenreCNN(num_classes)\n",
        "\n",
        "random_index = np.random.randint(len(train_dataset_melgrams))\n",
        "random_sample, random_label = train_dataset_melgrams[random_index]\n",
        "print(f'Random sample shape: {random_sample.shape}')\n",
        "random_sample = random_sample.unsqueeze(0).unsqueeze(0) # adding batch and channel dimension\n",
        "\n",
        "output = model(random_sample)\n",
        "\n",
        "\n",
        "print(f'Random sample shape: {random_sample.shape}')\n",
        "print(f'Output shape: {output.shape}')\n"
      ],
      "metadata": {
        "id": "aBXl0Lb-T7S6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad967bb1-0a2b-4601-b9cf-fde67298561a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of classes: 4\n",
            "Class names ['blues' 'classical' 'hiphop' 'rock_metal_hardrock']\n",
            "Input shape: torch.Size([1, 1, 21, 128])\n",
            "Conv shape: torch.Size([1, 128, 5, 112])\n",
            "Linear output: 71680\n",
            "Random sample shape: torch.Size([21, 128])\n",
            "Random sample shape: torch.Size([1, 1, 21, 128])\n",
            "Output shape: torch.Size([1, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Every input sample has a shape of (21, 128) and the dim of the output tensor remain the same for any sample taken:\\\n",
        "Random sample shape: torch.Size([1, 1, 21, 128])\\\n",
        "1 sample, 1 channel, 21 time steps, 128 frequency bins\\\n",
        "Convolutional layers expect 4 dimensions: (batch_size, num_of_channels, input_height, input_width):\\\n",
        "Random sample shape: torch.Size([21, 128]) ->\n",
        "Random sample shape: torch.Size([1, 1, 21, 128])\\\n",
        "Conv1 shape: torch.Size([1, 16, 17, 124])\\\n",
        "Conv2 shape: torch.Size([1, 32, 13, 120])\\\n",
        "Conv3 shape: torch.Size([1, 64, 9, 116])\\\n",
        "Conv4 shape: torch.Size([1, 128, 5, 112])\\\n",
        "Linear output: 71680 <-- Flattened size of the tensor with torch.numel(input) that returns the total number of elements in the input tensor (https://pytorch.org/docs/stable/generated/torch.numel.html)\\\n",
        "Initializing a tensor with ones and multiplied by the tuple (1, 21, 128) (https://pytorch.org/docs/stable/generated/torch.ones.html)\n"
      ],
      "metadata": {
        "id": "XMvbUxakT8no"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Training the CNN"
      ],
      "metadata": {
        "id": "S8-XOoXvcxqp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_loop(dataloader, model, loss_fn, optimizer, device):\n",
        "    device = torch.device(\"cuda\" if device and torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    model.train()\n",
        "    size = len(dataloader.dataset)\n",
        "    loss_total = 0.0\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        pred = model(X.unsqueeze(1))  # Add channel dimension\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        loss_total += loss.item()\n",
        "        if batch % 100 == 0:\n",
        "            current = batch * len(X)\n",
        "            print(f\"loss: {loss.item():>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "    return loss_total / len(dataloader)\n",
        "\n",
        "def test_loop(dataloader, model, loss_fn, device):\n",
        "    device = torch.device(\"cuda\" if device and torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    model.eval()\n",
        "    test_loss = 0.0\n",
        "    correct = 0\n",
        "    size = len(dataloader.dataset)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "\n",
        "            pred = model(X.unsqueeze(1))  # Add channel dimension\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "    test_loss /= size\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "\n",
        "    return 100 * correct, test_loss\n",
        "\n",
        "def evaluate(dataloader, model, loss_fn, device, return_dict=False):\n",
        "    device = torch.device(\"cuda\" if device and torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    model.eval()\n",
        "    size = len(dataloader.dataset)\n",
        "    test_loss = 0.0\n",
        "    correct = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "\n",
        "            pred = model(X.unsqueeze(1))  # Add channel dimension\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "            all_preds.extend(pred.argmax(1).cpu().numpy())\n",
        "            all_labels.extend(y.cpu().numpy())\n",
        "\n",
        "    test_loss /= size\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "\n",
        "    report = classification_report(all_labels, all_preds, target_names=label_encoder.classes_, output_dict=return_dict)\n",
        "\n",
        "    print(report)\n",
        "\n",
        "    return report"
      ],
      "metadata": {
        "id": "vT_TeWgBUCaI"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MusicGenreCNN"
      ],
      "metadata": {
        "id": "Ehu-LJ7xc1bG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
        "EPOCHS = 10\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
        "    train_loss = train_loop(train_loader_melgrams, model, loss_fn, optimizer, device=True)\n",
        "    print(f'Epoch [{epoch+1}/{EPOCHS}], Train Loss: {train_loss}')\n",
        "    test_loop(val_loader_melgrams, model, loss_fn, device=True)\n",
        "\n",
        "print('Testing')\n",
        "report = evaluate(test_loader_melgrams, model, loss_fn, device=True)"
      ],
      "metadata": {
        "id": "6JPulc-GUEEg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b350561-531a-4fad-e73f-a2f444f435c6"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 1.452244  [    0/ 3200]\n",
            "loss: 1.398024  [ 1600/ 3200]\n",
            "Epoch [1/10], Train Loss: 97.15910612523555\n",
            "Test Error: \n",
            " Accuracy: 55.6%, Avg loss: 0.066003 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 0.864174  [    0/ 3200]\n",
            "loss: 0.655541  [ 1600/ 3200]\n",
            "Epoch [2/10], Train Loss: 0.9157060903310775\n",
            "Test Error: \n",
            " Accuracy: 60.5%, Avg loss: 0.061372 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 0.561251  [    0/ 3200]\n",
            "loss: 0.915399  [ 1600/ 3200]\n",
            "Epoch [3/10], Train Loss: 0.7759707625210285\n",
            "Test Error: \n",
            " Accuracy: 58.9%, Avg loss: 0.064568 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 0.722734  [    0/ 3200]\n",
            "loss: 0.743092  [ 1600/ 3200]\n",
            "Epoch [4/10], Train Loss: 0.6675746458023787\n",
            "Test Error: \n",
            " Accuracy: 60.8%, Avg loss: 0.063670 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 0.477370  [    0/ 3200]\n",
            "loss: 0.454248  [ 1600/ 3200]\n",
            "Epoch [5/10], Train Loss: 0.5985027017444372\n",
            "Test Error: \n",
            " Accuracy: 59.6%, Avg loss: 0.068972 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 0.365712  [    0/ 3200]\n",
            "loss: 0.211925  [ 1600/ 3200]\n",
            "Epoch [6/10], Train Loss: 0.5413249107077718\n",
            "Test Error: \n",
            " Accuracy: 58.2%, Avg loss: 0.071696 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 0.421338  [    0/ 3200]\n",
            "loss: 0.621682  [ 1600/ 3200]\n",
            "Epoch [7/10], Train Loss: 0.48401433005928995\n",
            "Test Error: \n",
            " Accuracy: 56.1%, Avg loss: 0.079170 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 0.357171  [    0/ 3200]\n",
            "loss: 0.594675  [ 1600/ 3200]\n",
            "Epoch [8/10], Train Loss: 0.44589694824069737\n",
            "Test Error: \n",
            " Accuracy: 57.0%, Avg loss: 0.086837 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 0.187642  [    0/ 3200]\n",
            "loss: 0.346218  [ 1600/ 3200]\n",
            "Epoch [9/10], Train Loss: 0.40615084126591683\n",
            "Test Error: \n",
            " Accuracy: 55.5%, Avg loss: 0.088759 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 0.320819  [    0/ 3200]\n",
            "loss: 0.405399  [ 1600/ 3200]\n",
            "Epoch [10/10], Train Loss: 0.3486560097336769\n",
            "Test Error: \n",
            " Accuracy: 55.5%, Avg loss: 0.100765 \n",
            "\n",
            "Testing\n",
            "Test Error: \n",
            " Accuracy: 52.0%, Avg loss: 0.121997 \n",
            "\n",
            "                     precision    recall  f1-score   support\n",
            "\n",
            "              blues       0.35      0.40      0.38       324\n",
            "          classical       0.66      0.60      0.62       297\n",
            "             hiphop       0.62      0.55      0.59       356\n",
            "rock_metal_hardrock       0.51      0.53      0.52       399\n",
            "\n",
            "           accuracy                           0.52      1376\n",
            "          macro avg       0.53      0.52      0.53      1376\n",
            "       weighted avg       0.53      0.52      0.53      1376\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "without pred = model(X.unsqueeze(1)) to add channel dimension we get: Given groups=1, weight of size [16, 1, 5, 5], expected input[1, 16, 21, 128] to have 1 channels, but got 16 channels instead\\\n",
        "self.conv1 = nn.Conv2d(1, 16, kernel_size=5) expects the input to have 1 channel."
      ],
      "metadata": {
        "id": "iuaw3at1c5B8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's obvious that CPU times are going to be significantly higher than with GPU. The convolutional operations involve sliding a filter over the input data and performing element-wise multiplications and summations. A rather interesting GIF can be seen here https://www.kaggle.com/code/pavansanagapati/a-simple-cnn-model-beginner-guide.\\\n",
        "These kind of operations benefit immensely from the power of GPUs over CPUs."
      ],
      "metadata": {
        "id": "qWN44-0-UFxA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4: Pooling and Padding"
      ],
      "metadata": {
        "id": "1lfeFrYec7Bu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MusicGenreCNN_Pooling(nn.Module):\n",
        "    def __init__(self, out_dim):\n",
        "        super(MusicGenreCNN_Pooling, self).__init__()\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            nn.Conv2d(1, 16, kernel_size=5, padding=2),\n",
        "            nn.MaxPool2d(2,2),\n",
        "            nn.Conv2d(16, 32, kernel_size=5, padding=2),\n",
        "            nn.MaxPool2d(2,2),\n",
        "            nn.Conv2d(32, 64, kernel_size=5, padding=2),\n",
        "            nn.MaxPool2d(2,2),\n",
        "            nn.Conv2d(64, 128, kernel_size=5, padding=2),\n",
        "            nn.MaxPool2d(2,2)\n",
        "        )\n",
        "\n",
        "        self._to_linear = None\n",
        "        self.calculate_linear_output((1, 21, 128))\n",
        "        print(f'Linear output: {self._to_linear}')\n",
        "\n",
        "        self.fc_layers = nn.Sequential(\n",
        "            nn.Linear(self._to_linear, 1024),\n",
        "            nn.Linear(1024, 256),\n",
        "            nn.Linear(256, 32),\n",
        "            nn.Linear(32, out_dim)\n",
        "        )\n",
        "\n",
        "    def calculate_linear_output(self, shape):\n",
        "        output = torch.ones(1, *shape)\n",
        "        print(f'Input shape: {output.shape}')\n",
        "        output = self.conv_layers(output)\n",
        "        print(f'Conv shape: {output.shape}')\n",
        "        self._to_linear = output.numel()\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_layers(x)\n",
        "        x = x.view(x.size(0), -1) # Flatten the tensor\n",
        "        x = self.fc_layers(x)\n",
        "        return x\n",
        "\n",
        "num_classes = len(label_encoder.classes_)\n",
        "print(f'Number of classes: {num_classes}')\n",
        "print(f\"Class names {label_encoder.classes_}\")\n",
        "\n",
        "model_pooling = MusicGenreCNN_Pooling(num_classes)\n"
      ],
      "metadata": {
        "id": "y1QkXJ5hUKea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "556b7b96-9556-489a-dff9-9b60db81c4b9"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of classes: 4\n",
            "Class names ['blues' 'classical' 'hiphop' 'rock_metal_hardrock']\n",
            "Input shape: torch.Size([1, 1, 21, 128])\n",
            "Conv shape: torch.Size([1, 128, 1, 8])\n",
            "Linear output: 1024\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MusicGenreCNN_Pooling"
      ],
      "metadata": {
        "id": "EV2xEHxuc_Pe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model_pooling.parameters(), lr=LR)\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
        "    train_loss = train_loop(train_loader_melgrams, model_pooling, loss_fn, optimizer, device=True)\n",
        "    print(f'Epoch [{epoch+1}/{EPOCHS}], Train Loss: {train_loss}')\n",
        "    test_loop(val_loader_melgrams, model_pooling, loss_fn, device=True)\n",
        "\n",
        "print('Testing')\n",
        "report = evaluate(test_loader_melgrams, model_pooling, loss_fn, device=True)"
      ],
      "metadata": {
        "id": "XsY7YwlQUL_E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc16c553-4082-485f-d37e-a7f69dea74ee"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 1.381302  [    0/ 3200]\n",
            "loss: 0.910024  [ 1600/ 3200]\n",
            "Epoch [1/10], Train Loss: 1.1813547861576081\n",
            "Test Error: \n",
            " Accuracy: 65.9%, Avg loss: 0.048572 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 0.715927  [    0/ 3200]\n",
            "loss: 0.909853  [ 1600/ 3200]\n",
            "Epoch [2/10], Train Loss: 0.7561468249559402\n",
            "Test Error: \n",
            " Accuracy: 73.8%, Avg loss: 0.041802 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 0.751981  [    0/ 3200]\n",
            "loss: 0.984743  [ 1600/ 3200]\n",
            "Epoch [3/10], Train Loss: 0.6859725250303745\n",
            "Test Error: \n",
            " Accuracy: 70.6%, Avg loss: 0.048376 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 0.285476  [    0/ 3200]\n",
            "loss: 0.465281  [ 1600/ 3200]\n",
            "Epoch [4/10], Train Loss: 0.5657710131630301\n",
            "Test Error: \n",
            " Accuracy: 66.5%, Avg loss: 0.060168 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 0.309189  [    0/ 3200]\n",
            "loss: 0.311374  [ 1600/ 3200]\n",
            "Epoch [5/10], Train Loss: 0.4375979743339121\n",
            "Test Error: \n",
            " Accuracy: 71.2%, Avg loss: 0.052718 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 0.217863  [    0/ 3200]\n",
            "loss: 0.403806  [ 1600/ 3200]\n",
            "Epoch [6/10], Train Loss: 0.3201375608239323\n",
            "Test Error: \n",
            " Accuracy: 69.4%, Avg loss: 0.059292 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 0.168242  [    0/ 3200]\n",
            "loss: 0.149613  [ 1600/ 3200]\n",
            "Epoch [7/10], Train Loss: 0.4113579344935715\n",
            "Test Error: \n",
            " Accuracy: 67.4%, Avg loss: 0.056370 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 0.416584  [    0/ 3200]\n",
            "loss: 0.028129  [ 1600/ 3200]\n",
            "Epoch [8/10], Train Loss: 0.22167272593127563\n",
            "Test Error: \n",
            " Accuracy: 71.4%, Avg loss: 0.069500 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 0.061343  [    0/ 3200]\n",
            "loss: 0.005301  [ 1600/ 3200]\n",
            "Epoch [9/10], Train Loss: 0.09815960683248705\n",
            "Test Error: \n",
            " Accuracy: 67.6%, Avg loss: 0.101055 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 0.095233  [    0/ 3200]\n",
            "loss: 0.107527  [ 1600/ 3200]\n",
            "Epoch [10/10], Train Loss: 0.1992917232970649\n",
            "Test Error: \n",
            " Accuracy: 67.2%, Avg loss: 0.080612 \n",
            "\n",
            "Testing\n",
            "Test Error: \n",
            " Accuracy: 64.0%, Avg loss: 0.087631 \n",
            "\n",
            "                     precision    recall  f1-score   support\n",
            "\n",
            "              blues       0.44      0.55      0.49       324\n",
            "          classical       0.80      0.85      0.82       297\n",
            "             hiphop       0.82      0.65      0.72       356\n",
            "rock_metal_hardrock       0.59      0.55      0.57       399\n",
            "\n",
            "           accuracy                           0.64      1376\n",
            "          macro avg       0.66      0.65      0.65      1376\n",
            "       weighted avg       0.66      0.64      0.64      1376\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5: Different Optimizers"
      ],
      "metadata": {
        "id": "VxVSdjQcUPZw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_evaluate(train_loader, val_loader, test_loader, model_class, loss_fn, optimizers, num_classes, num_epochs=10, lr=1e-3):\n",
        "    results = {'Optimizer': [], 'Accuracy': [], 'F1 Score': []}\n",
        "\n",
        "    for opt_name in optimizers:\n",
        "        print(f\"Training with {opt_name} optimizer...\")\n",
        "        model = model_class(num_classes)\n",
        "        optimizer = getattr(optim, opt_name)(model.parameters(), lr=lr)\n",
        "\n",
        "        for t in range(num_epochs):\n",
        "            print(f\"Epoch {t+1} with {opt_name}\\n-------------------------------\")\n",
        "            train_loop(train_loader, model, loss_fn, optimizer, device=True)\n",
        "            test_loop(val_loader, model, loss_fn, device=True)\n",
        "\n",
        "        report = evaluate(test_loader, model, loss_fn, device=True, return_dict=True)\n",
        "        results['Optimizer'].append(opt_name)\n",
        "        results['Accuracy'].append(report['accuracy'])\n",
        "        results['F1 Score'].append(report['weighted avg']['f1-score'])\n",
        "\n",
        "    return results\n",
        "\n",
        "optimizers = ['SGD', 'Adam', 'RMSprop']\n",
        "results = train_and_evaluate(train_loader_melgrams, val_loader_melgrams, test_loader_melgrams, MusicGenreCNN_Pooling, loss_fn, optimizers, num_classes)\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "print(results_df.transpose())"
      ],
      "metadata": {
        "id": "pPZFFxXhUQPw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2391f4ab-1cc4-41f5-d80f-60bea933b0f3"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with SGD optimizer...\n",
            "Input shape: torch.Size([1, 1, 21, 128])\n",
            "Conv shape: torch.Size([1, 128, 1, 8])\n",
            "Linear output: 1024\n",
            "Epoch 1 with SGD\n",
            "-------------------------------\n",
            "loss: 1.299085  [    0/ 3200]\n",
            "loss: 1.306386  [ 1600/ 3200]\n",
            "Test Error: \n",
            " Accuracy: 33.9%, Avg loss: 0.079004 \n",
            "\n",
            "Epoch 2 with SGD\n",
            "-------------------------------\n",
            "loss: 1.129960  [    0/ 3200]\n",
            "loss: 0.967626  [ 1600/ 3200]\n",
            "Test Error: \n",
            " Accuracy: 50.6%, Avg loss: 0.070424 \n",
            "\n",
            "Epoch 3 with SGD\n",
            "-------------------------------\n",
            "loss: 1.164847  [    0/ 3200]\n",
            "loss: 1.037361  [ 1600/ 3200]\n",
            "Test Error: \n",
            " Accuracy: 54.2%, Avg loss: 0.064419 \n",
            "\n",
            "Epoch 4 with SGD\n",
            "-------------------------------\n",
            "loss: 1.045949  [    0/ 3200]\n",
            "loss: 0.838448  [ 1600/ 3200]\n",
            "Test Error: \n",
            " Accuracy: 44.6%, Avg loss: 0.065856 \n",
            "\n",
            "Epoch 5 with SGD\n",
            "-------------------------------\n",
            "loss: 0.973250  [    0/ 3200]\n",
            "loss: 0.908278  [ 1600/ 3200]\n",
            "Test Error: \n",
            " Accuracy: 64.5%, Avg loss: 0.056226 \n",
            "\n",
            "Epoch 6 with SGD\n",
            "-------------------------------\n",
            "loss: 0.714486  [    0/ 3200]\n",
            "loss: 0.974984  [ 1600/ 3200]\n",
            "Test Error: \n",
            " Accuracy: 49.5%, Avg loss: 0.070289 \n",
            "\n",
            "Epoch 7 with SGD\n",
            "-------------------------------\n",
            "loss: 1.384565  [    0/ 3200]\n",
            "loss: 0.753241  [ 1600/ 3200]\n",
            "Test Error: \n",
            " Accuracy: 64.4%, Avg loss: 0.053562 \n",
            "\n",
            "Epoch 8 with SGD\n",
            "-------------------------------\n",
            "loss: 0.726518  [    0/ 3200]\n",
            "loss: 0.598378  [ 1600/ 3200]\n",
            "Test Error: \n",
            " Accuracy: 66.9%, Avg loss: 0.051548 \n",
            "\n",
            "Epoch 9 with SGD\n",
            "-------------------------------\n",
            "loss: 0.968531  [    0/ 3200]\n",
            "loss: 0.528578  [ 1600/ 3200]\n",
            "Test Error: \n",
            " Accuracy: 61.0%, Avg loss: 0.056670 \n",
            "\n",
            "Epoch 10 with SGD\n",
            "-------------------------------\n",
            "loss: 0.787061  [    0/ 3200]\n",
            "loss: 0.846574  [ 1600/ 3200]\n",
            "Test Error: \n",
            " Accuracy: 60.0%, Avg loss: 0.055420 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 61.6%, Avg loss: 0.055295 \n",
            "\n",
            "{'blues': {'precision': 0.5813953488372093, 'recall': 0.07716049382716049, 'f1-score': 0.13623978201634876, 'support': 324}, 'classical': {'precision': 0.6745283018867925, 'recall': 0.9629629629629629, 'f1-score': 0.7933425797503468, 'support': 297}, 'hiphop': {'precision': 0.7684210526315789, 'recall': 0.6151685393258427, 'f1-score': 0.6833073322932918, 'support': 356}, 'rock_metal_hardrock': {'precision': 0.5080128205128205, 'recall': 0.7944862155388471, 'f1-score': 0.6197458455522972, 'support': 399}, 'accuracy': 0.6155523255813954, 'macro avg': {'precision': 0.6330893809671003, 'recall': 0.6124445529137034, 'f1-score': 0.5581588849030711, 'support': 1376}, 'weighted avg': {'precision': 0.6286061110502111, 'recall': 0.6155523255813954, 'f1-score': 0.5598113649934073, 'support': 1376}}\n",
            "Training with Adam optimizer...\n",
            "Input shape: torch.Size([1, 1, 21, 128])\n",
            "Conv shape: torch.Size([1, 128, 1, 8])\n",
            "Linear output: 1024\n",
            "Epoch 1 with Adam\n",
            "-------------------------------\n",
            "loss: 1.395159  [    0/ 3200]\n",
            "loss: 1.102239  [ 1600/ 3200]\n",
            "Test Error: \n",
            " Accuracy: 59.6%, Avg loss: 0.053765 \n",
            "\n",
            "Epoch 2 with Adam\n",
            "-------------------------------\n",
            "loss: 0.790815  [    0/ 3200]\n",
            "loss: 0.558239  [ 1600/ 3200]\n",
            "Test Error: \n",
            " Accuracy: 67.2%, Avg loss: 0.049276 \n",
            "\n",
            "Epoch 3 with Adam\n",
            "-------------------------------\n",
            "loss: 0.564472  [    0/ 3200]\n",
            "loss: 0.189742  [ 1600/ 3200]\n",
            "Test Error: \n",
            " Accuracy: 68.8%, Avg loss: 0.049194 \n",
            "\n",
            "Epoch 4 with Adam\n",
            "-------------------------------\n",
            "loss: 0.911506  [    0/ 3200]\n",
            "loss: 0.290361  [ 1600/ 3200]\n",
            "Test Error: \n",
            " Accuracy: 69.5%, Avg loss: 0.050070 \n",
            "\n",
            "Epoch 5 with Adam\n",
            "-------------------------------\n",
            "loss: 0.357570  [    0/ 3200]\n",
            "loss: 0.503198  [ 1600/ 3200]\n",
            "Test Error: \n",
            " Accuracy: 66.9%, Avg loss: 0.053147 \n",
            "\n",
            "Epoch 6 with Adam\n",
            "-------------------------------\n",
            "loss: 0.219433  [    0/ 3200]\n",
            "loss: 0.278511  [ 1600/ 3200]\n",
            "Test Error: \n",
            " Accuracy: 69.5%, Avg loss: 0.067091 \n",
            "\n",
            "Epoch 7 with Adam\n",
            "-------------------------------\n",
            "loss: 0.127918  [    0/ 3200]\n",
            "loss: 0.273482  [ 1600/ 3200]\n",
            "Test Error: \n",
            " Accuracy: 73.1%, Avg loss: 0.053562 \n",
            "\n",
            "Epoch 8 with Adam\n",
            "-------------------------------\n",
            "loss: 0.310395  [    0/ 3200]\n",
            "loss: 0.470343  [ 1600/ 3200]\n",
            "Test Error: \n",
            " Accuracy: 68.1%, Avg loss: 0.079132 \n",
            "\n",
            "Epoch 9 with Adam\n",
            "-------------------------------\n",
            "loss: 0.585929  [    0/ 3200]\n",
            "loss: 0.288496  [ 1600/ 3200]\n",
            "Test Error: \n",
            " Accuracy: 70.5%, Avg loss: 0.068762 \n",
            "\n",
            "Epoch 10 with Adam\n",
            "-------------------------------\n",
            "loss: 0.059283  [    0/ 3200]\n",
            "loss: 0.543423  [ 1600/ 3200]\n",
            "Test Error: \n",
            " Accuracy: 68.0%, Avg loss: 0.072421 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 65.3%, Avg loss: 0.086806 \n",
            "\n",
            "{'blues': {'precision': 0.6025641025641025, 'recall': 0.29012345679012347, 'f1-score': 0.39166666666666666, 'support': 324}, 'classical': {'precision': 0.806060606060606, 'recall': 0.8956228956228957, 'f1-score': 0.8484848484848484, 'support': 297}, 'hiphop': {'precision': 0.6958904109589041, 'recall': 0.7134831460674157, 'f1-score': 0.7045769764216367, 'support': 356}, 'rock_metal_hardrock': {'precision': 0.5428571428571428, 'recall': 0.7142857142857143, 'f1-score': 0.6168831168831169, 'support': 399}, 'accuracy': 0.653343023255814, 'macro avg': {'precision': 0.6618430656101888, 'recall': 0.6533788031915373, 'f1-score': 0.6404029021140671, 'support': 1376}, 'weighted avg': {'precision': 0.6533195897762639, 'recall': 0.653343023255814, 'f1-score': 0.6365303541006295, 'support': 1376}}\n",
            "Training with RMSprop optimizer...\n",
            "Input shape: torch.Size([1, 1, 21, 128])\n",
            "Conv shape: torch.Size([1, 128, 1, 8])\n",
            "Linear output: 1024\n",
            "Epoch 1 with RMSprop\n",
            "-------------------------------\n",
            "loss: 1.371418  [    0/ 3200]\n",
            "loss: 36.126823  [ 1600/ 3200]\n",
            "Test Error: \n",
            " Accuracy: 40.4%, Avg loss: 0.429713 \n",
            "\n",
            "Epoch 2 with RMSprop\n",
            "-------------------------------\n",
            "loss: 2.614955  [    0/ 3200]\n",
            "loss: 2.976163  [ 1600/ 3200]\n",
            "Test Error: \n",
            " Accuracy: 46.4%, Avg loss: 0.170283 \n",
            "\n",
            "Epoch 3 with RMSprop\n",
            "-------------------------------\n",
            "loss: 4.613828  [    0/ 3200]\n",
            "loss: 4.476381  [ 1600/ 3200]\n",
            "Test Error: \n",
            " Accuracy: 50.1%, Avg loss: 0.081355 \n",
            "\n",
            "Epoch 4 with RMSprop\n",
            "-------------------------------\n",
            "loss: 1.677342  [    0/ 3200]\n",
            "loss: 1.275756  [ 1600/ 3200]\n",
            "Test Error: \n",
            " Accuracy: 50.6%, Avg loss: 0.066335 \n",
            "\n",
            "Epoch 5 with RMSprop\n",
            "-------------------------------\n",
            "loss: 0.637574  [    0/ 3200]\n",
            "loss: 1.584788  [ 1600/ 3200]\n",
            "Test Error: \n",
            " Accuracy: 60.9%, Avg loss: 0.059133 \n",
            "\n",
            "Epoch 6 with RMSprop\n",
            "-------------------------------\n",
            "loss: 0.762664  [    0/ 3200]\n",
            "loss: 0.615621  [ 1600/ 3200]\n",
            "Test Error: \n",
            " Accuracy: 42.6%, Avg loss: 0.175224 \n",
            "\n",
            "Epoch 7 with RMSprop\n",
            "-------------------------------\n",
            "loss: 4.346478  [    0/ 3200]\n",
            "loss: 0.901884  [ 1600/ 3200]\n",
            "Test Error: \n",
            " Accuracy: 55.0%, Avg loss: 0.064239 \n",
            "\n",
            "Epoch 8 with RMSprop\n",
            "-------------------------------\n",
            "loss: 1.090876  [    0/ 3200]\n",
            "loss: 0.720402  [ 1600/ 3200]\n",
            "Test Error: \n",
            " Accuracy: 56.4%, Avg loss: 0.064326 \n",
            "\n",
            "Epoch 9 with RMSprop\n",
            "-------------------------------\n",
            "loss: 0.827064  [    0/ 3200]\n",
            "loss: 0.670765  [ 1600/ 3200]\n",
            "Test Error: \n",
            " Accuracy: 70.0%, Avg loss: 0.049250 \n",
            "\n",
            "Epoch 10 with RMSprop\n",
            "-------------------------------\n",
            "loss: 0.637063  [    0/ 3200]\n",
            "loss: 1.489833  [ 1600/ 3200]\n",
            "Test Error: \n",
            " Accuracy: 62.9%, Avg loss: 0.052420 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 61.8%, Avg loss: 0.052360 \n",
            "\n",
            "{'blues': {'precision': 0.38657407407407407, 'recall': 0.5154320987654321, 'f1-score': 0.4417989417989418, 'support': 324}, 'classical': {'precision': 0.9507389162561576, 'recall': 0.6498316498316499, 'f1-score': 0.7719999999999999, 'support': 297}, 'hiphop': {'precision': 0.6328871892925431, 'recall': 0.9297752808988764, 'f1-score': 0.7531285551763367, 'support': 356}, 'rock_metal_hardrock': {'precision': 0.7293577981651376, 'recall': 0.39849624060150374, 'f1-score': 0.5153970826580226, 'support': 399}, 'accuracy': 0.6177325581395349, 'macro avg': {'precision': 0.6748894944469781, 'recall': 0.6233838175243656, 'f1-score': 0.6205811449083253, 'support': 1376}, 'weighted avg': {'precision': 0.671468792866362, 'recall': 0.6177325581395349, 'f1-score': 0.614959345033564, 'support': 1376}}\n",
            "                  0         1         2\n",
            "Optimizer       SGD      Adam   RMSprop\n",
            "Accuracy   0.615552  0.653343  0.617733\n",
            "F1 Score   0.559811   0.63653  0.614959\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Query 3"
      ],
      "metadata": {
        "id": "1U821YhnUSEU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seed(seed=42):\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    random.seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed()"
      ],
      "metadata": {
        "id": "G0JBP_ufUS3-"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "manual_seed generates random numbers on all devices (cpu and gpu) https://pytorch.org/docs/stable/generated/torch.manual_seed.html\\\n",
        "torch.backends.cudnn.benchmark = False https://pytorch.org/docs/stable/notes/randomness.html#avoiding-nondeterministic-algorithms Traditionally, the benchmark may select different algorithms on subsequent runs. By disabling this feature we cause cuDNN to deterministically select the same algorithm.\\\n",
        "torch.backends.cudnn.deterministic = True https://pytorch.org/docs/stable/notes/randomness.html#cuda-convolution-determinism Sets the same algorithm each time to be a deterministic one\\\n"
      ],
      "metadata": {
        "id": "OXePgtYzUYPI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_melgrams = np.load('music_genre_data_di/train/melgrams/X.npy')\n",
        "y_train_melgrams = np.load('music_genre_data_di/train/melgrams/labels.npy')\n",
        "\n",
        "X_val_melgrams = np.load('music_genre_data_di/val/melgrams/X.npy')\n",
        "y_val_melgrams = np.load('music_genre_data_di/val/melgrams/labels.npy')\n",
        "\n",
        "X_test_melgrams = np.load('music_genre_data_di/test/melgrams/X.npy')\n",
        "y_test_melgrams = np.load('music_genre_data_di/test/melgrams/labels.npy')\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "train_melgrams_labels = label_encoder.fit_transform(y_train_melgrams)\n",
        "val_melgrams_labels = label_encoder.transform(y_val_melgrams)\n",
        "test_melgrams_labels = label_encoder.transform(y_test_melgrams)\n",
        "\n",
        "BATCH_SIZE = 16\n",
        "LR = 1e-3\n",
        "\n",
        "train_dataset_melgrams = TensorDataset(torch.tensor(X_train_melgrams, dtype=torch.float32), torch.tensor(train_melgrams_labels, dtype=torch.long))\n",
        "val_dataset_melgrams = TensorDataset(torch.tensor(X_val_melgrams, dtype=torch.float32), torch.tensor(val_melgrams_labels, dtype=torch.long))\n",
        "test_dataset_melgrams = TensorDataset(torch.tensor(X_test_melgrams, dtype=torch.float32), torch.tensor(test_melgrams_labels, dtype=torch.long))\n",
        "\n",
        "train_loader_melgrams = DataLoader(train_dataset_melgrams, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader_melgrams = DataLoader(val_dataset_melgrams, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader_melgrams = DataLoader(test_dataset_melgrams, batch_size=BATCH_SIZE, shuffle=False)\n"
      ],
      "metadata": {
        "id": "Wa-cs9VvUY1Y"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_loop(dataloader, model, loss_fn, optimizer, device):\n",
        "    device = torch.device(\"cuda\" if device and torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    model.train()\n",
        "    size = len(dataloader.dataset)\n",
        "    loss_total = 0.0\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        pred = model(X.unsqueeze(1))  # Add channel dimension\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        loss_total += loss.item()\n",
        "        if batch % 100 == 0:\n",
        "            current = batch * len(X)\n",
        "            print(f\"loss: {loss.item():>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "    return loss_total / len(dataloader)\n",
        "\n",
        "def test_loop(dataloader, model, loss_fn, device):\n",
        "    device = torch.device(\"cuda\" if device and torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    preds = []\n",
        "    labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "\n",
        "            pred = model(X.unsqueeze(1))  # Add channel dimension if needed\n",
        "            loss = loss_fn(pred, y)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            _, pred = torch.max(pred, 1)\n",
        "            preds.extend(pred.cpu().tolist())\n",
        "            labels.extend(y.cpu().tolist())\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    f1 = f1_score(labels, preds, average='weighted', zero_division=0)\n",
        "    cm = confusion_matrix(labels, preds)\n",
        "\n",
        "    print(f\"Test Error: \\n Accuracy: {acc:.2f}%, Avg loss: {avg_loss:.4f}, F1 Score: {f1:.4f} \\n\")\n",
        "\n",
        "    return avg_loss, acc, f1, cm\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "def evaluate(dataloader, model, loss_fn, device, return_dict=False):\n",
        "    device = torch.device(\"cuda\" if device and torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    model.eval()\n",
        "    size = len(dataloader.dataset)\n",
        "    test_loss = 0.0\n",
        "    correct = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "\n",
        "            pred = model(X.unsqueeze(1))  # Add channel dimension\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "            all_preds.extend(pred.argmax(1).cpu().numpy())\n",
        "            all_labels.extend(y.cpu().numpy())\n",
        "\n",
        "    test_loss /= size\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "\n",
        "    report = classification_report(all_labels, all_preds, target_names=label_encoder.classes_, output_dict=return_dict, zero_division=0)\n",
        "\n",
        "    print(report)\n",
        "\n",
        "    return report"
      ],
      "metadata": {
        "id": "xZMegSN2Ua_O"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MusicGenreCNN_Pooling(nn.Module):\n",
        "    def __init__(self, out_dim):\n",
        "        super(MusicGenreCNN_Pooling, self).__init__()\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            nn.Conv2d(1, 16, kernel_size=5, padding=2),\n",
        "            nn.MaxPool2d(2,2),\n",
        "            nn.Conv2d(16, 32, kernel_size=5, padding=2),\n",
        "            nn.MaxPool2d(2,2),\n",
        "            nn.Conv2d(32, 64, kernel_size=5, padding=2),\n",
        "            nn.MaxPool2d(2,2),\n",
        "            nn.Conv2d(64, 128, kernel_size=5, padding=2),\n",
        "            nn.MaxPool2d(2,2)\n",
        "        )\n",
        "\n",
        "        self._to_linear = None\n",
        "        self.calculate_linear_output((1, 21, 128))\n",
        "        print(f'Linear output: {self._to_linear}')\n",
        "\n",
        "        self.fc_layers = nn.Sequential(\n",
        "            nn.Linear(self._to_linear, 1024),\n",
        "            nn.Linear(1024, 256),\n",
        "            nn.Linear(256, 32),\n",
        "            nn.Linear(32, out_dim)\n",
        "        )\n",
        "\n",
        "    def calculate_linear_output(self, shape):\n",
        "        output = torch.ones(1, *shape)\n",
        "        print(f'Input shape: {output.shape}')\n",
        "        output = self.conv_layers(output)\n",
        "        print(f'Conv shape: {output.shape}')\n",
        "        self._to_linear = output.numel()\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_layers(x)\n",
        "        x = x.view(x.size(0), -1) # Flatten the tensor\n",
        "        x = self.fc_layers(x)\n",
        "        return x\n",
        "\n",
        "num_classes = len(label_encoder.classes_)\n",
        "print(f'Number of classes: {num_classes}')\n",
        "print(f\"Class names {label_encoder.classes_}\")\n"
      ],
      "metadata": {
        "id": "-s7EwYHvUceC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9875dc99-7011-407e-da59-36d7abbeece2"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of classes: 4\n",
            "Class names ['blues' 'classical' 'hiphop' 'rock_metal_hardrock']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1 Reproducibility"
      ],
      "metadata": {
        "id": "rDCfbqpGVNkP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = MusicGenreCNN_Pooling(num_classes)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
        "\n",
        "EPOCHS = 10\n",
        "loss_train_all = []\n",
        "loss_val_all = []\n",
        "accs = []\n",
        "\n",
        "for run in range(2):\n",
        "    print(f\"Run {run+1}\")\n",
        "    set_seed()\n",
        "    model = MusicGenreCNN_Pooling(num_classes)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LR)\n",
        "    loss_train_all_run = []\n",
        "    loss_val_all_run = []\n",
        "    accs_run = []\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
        "        train_loss = train_loop(train_loader_melgrams, model, loss_fn, optimizer, device=True)\n",
        "        print(f'Epoch [{epoch+1}/{EPOCHS}], Train Loss: {train_loss}')\n",
        "        loss_val, acc, _, _ = test_loop(val_loader_melgrams, model, loss_fn, device=True)\n",
        "\n",
        "        loss_train_all_run.append(train_loss)\n",
        "        loss_val_all_run.append(loss_val)\n",
        "        accs_run.append(acc)\n",
        "\n",
        "    loss_train_all.append(loss_train_all_run)\n",
        "    loss_val_all.append(loss_val_all_run)\n",
        "    accs.append(accs_run)\n",
        "\n",
        "# Comparing\n",
        "assert loss_train_all[0] == loss_train_all[1], \"Training losses are not identical\"\n",
        "assert loss_val_all[0] == loss_val_all[1], \"Validation losses are not identical\"\n",
        "assert accs[0] == accs[1], \"Validation accuracies are not identical\""
      ],
      "metadata": {
        "id": "doJB75WOVOik",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b034dccb-8e7a-43ca-83d7-407e1a35f970"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([1, 1, 21, 128])\n",
            "Conv shape: torch.Size([1, 128, 1, 8])\n",
            "Linear output: 1024\n",
            "Run 1\n",
            "Input shape: torch.Size([1, 1, 21, 128])\n",
            "Conv shape: torch.Size([1, 128, 1, 8])\n",
            "Linear output: 1024\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 1.459406  [    0/ 3200]\n",
            "loss: 1.488379  [ 1600/ 3200]\n",
            "Epoch [1/10], Train Loss: 1.5360872626304627\n",
            "Test Error: \n",
            " Accuracy: 0.48%, Avg loss: 1.0842, F1 Score: 0.4243 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 1.009668  [    0/ 3200]\n",
            "loss: 0.678304  [ 1600/ 3200]\n",
            "Epoch [2/10], Train Loss: 0.8755778025090695\n",
            "Test Error: \n",
            " Accuracy: 0.66%, Avg loss: 0.8230, F1 Score: 0.6511 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 1.089872  [    0/ 3200]\n",
            "loss: 0.885306  [ 1600/ 3200]\n",
            "Epoch [3/10], Train Loss: 0.6936750355362892\n",
            "Test Error: \n",
            " Accuracy: 0.66%, Avg loss: 0.8553, F1 Score: 0.6521 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 0.605087  [    0/ 3200]\n",
            "loss: 0.658113  [ 1600/ 3200]\n",
            "Epoch [4/10], Train Loss: 0.551813046708703\n",
            "Test Error: \n",
            " Accuracy: 0.71%, Avg loss: 0.7874, F1 Score: 0.6934 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 0.243370  [    0/ 3200]\n",
            "loss: 0.418188  [ 1600/ 3200]\n",
            "Epoch [5/10], Train Loss: 0.4488070219196379\n",
            "Test Error: \n",
            " Accuracy: 0.75%, Avg loss: 0.7337, F1 Score: 0.7446 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 0.375839  [    0/ 3200]\n",
            "loss: 0.202618  [ 1600/ 3200]\n",
            "Epoch [6/10], Train Loss: 0.37224121745675803\n",
            "Test Error: \n",
            " Accuracy: 0.67%, Avg loss: 0.9195, F1 Score: 0.6694 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 0.221470  [    0/ 3200]\n",
            "loss: 0.467872  [ 1600/ 3200]\n",
            "Epoch [7/10], Train Loss: 0.40614078560844064\n",
            "Test Error: \n",
            " Accuracy: 0.71%, Avg loss: 0.9207, F1 Score: 0.6890 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 0.337173  [    0/ 3200]\n",
            "loss: 0.214745  [ 1600/ 3200]\n",
            "Epoch [8/10], Train Loss: 0.22331721934489907\n",
            "Test Error: \n",
            " Accuracy: 0.69%, Avg loss: 1.2496, F1 Score: 0.6599 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 0.117222  [    0/ 3200]\n",
            "loss: 0.087540  [ 1600/ 3200]\n",
            "Epoch [9/10], Train Loss: 0.2329035934305284\n",
            "Test Error: \n",
            " Accuracy: 0.67%, Avg loss: 1.0037, F1 Score: 0.6692 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 0.075628  [    0/ 3200]\n",
            "loss: 0.008384  [ 1600/ 3200]\n",
            "Epoch [10/10], Train Loss: 0.16328046151116724\n",
            "Test Error: \n",
            " Accuracy: 0.71%, Avg loss: 1.2363, F1 Score: 0.7127 \n",
            "\n",
            "Run 2\n",
            "Input shape: torch.Size([1, 1, 21, 128])\n",
            "Conv shape: torch.Size([1, 128, 1, 8])\n",
            "Linear output: 1024\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 1.459406  [    0/ 3200]\n",
            "loss: 1.488379  [ 1600/ 3200]\n",
            "Epoch [1/10], Train Loss: 1.5360872626304627\n",
            "Test Error: \n",
            " Accuracy: 0.48%, Avg loss: 1.0842, F1 Score: 0.4243 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 1.009668  [    0/ 3200]\n",
            "loss: 0.678304  [ 1600/ 3200]\n",
            "Epoch [2/10], Train Loss: 0.8755778025090695\n",
            "Test Error: \n",
            " Accuracy: 0.66%, Avg loss: 0.8230, F1 Score: 0.6511 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 1.089872  [    0/ 3200]\n",
            "loss: 0.885306  [ 1600/ 3200]\n",
            "Epoch [3/10], Train Loss: 0.6936750355362892\n",
            "Test Error: \n",
            " Accuracy: 0.66%, Avg loss: 0.8553, F1 Score: 0.6521 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 0.605087  [    0/ 3200]\n",
            "loss: 0.658113  [ 1600/ 3200]\n",
            "Epoch [4/10], Train Loss: 0.551813046708703\n",
            "Test Error: \n",
            " Accuracy: 0.71%, Avg loss: 0.7874, F1 Score: 0.6934 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 0.243370  [    0/ 3200]\n",
            "loss: 0.418188  [ 1600/ 3200]\n",
            "Epoch [5/10], Train Loss: 0.4488070219196379\n",
            "Test Error: \n",
            " Accuracy: 0.75%, Avg loss: 0.7337, F1 Score: 0.7446 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 0.375839  [    0/ 3200]\n",
            "loss: 0.202618  [ 1600/ 3200]\n",
            "Epoch [6/10], Train Loss: 0.37224121745675803\n",
            "Test Error: \n",
            " Accuracy: 0.67%, Avg loss: 0.9195, F1 Score: 0.6694 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 0.221470  [    0/ 3200]\n",
            "loss: 0.467872  [ 1600/ 3200]\n",
            "Epoch [7/10], Train Loss: 0.40614078560844064\n",
            "Test Error: \n",
            " Accuracy: 0.71%, Avg loss: 0.9207, F1 Score: 0.6890 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 0.337173  [    0/ 3200]\n",
            "loss: 0.214745  [ 1600/ 3200]\n",
            "Epoch [8/10], Train Loss: 0.22331721934489907\n",
            "Test Error: \n",
            " Accuracy: 0.69%, Avg loss: 1.2496, F1 Score: 0.6599 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 0.117222  [    0/ 3200]\n",
            "loss: 0.087540  [ 1600/ 3200]\n",
            "Epoch [9/10], Train Loss: 0.2329035934305284\n",
            "Test Error: \n",
            " Accuracy: 0.67%, Avg loss: 1.0037, F1 Score: 0.6692 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 0.075628  [    0/ 3200]\n",
            "loss: 0.008384  [ 1600/ 3200]\n",
            "Epoch [10/10], Train Loss: 0.16328046151116724\n",
            "Test Error: \n",
            " Accuracy: 0.71%, Avg loss: 1.2363, F1 Score: 0.7127 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MusicGenreCNN_Pooling_Activated(nn.Module):\n",
        "    def __init__(self, out_dim, activation_fn):\n",
        "        super(MusicGenreCNN_Pooling_Activated, self).__init__()\n",
        "        self.activation_fn = activation_fn\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            nn.Conv2d(1, 16, kernel_size=5, padding=2),\n",
        "            activation_fn,\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(16, 32, kernel_size=5, padding=2),\n",
        "            activation_fn,\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(32, 64, kernel_size=5, padding=2),\n",
        "            activation_fn,\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(64, 128, kernel_size=5, padding=2),\n",
        "            activation_fn,\n",
        "            nn.MaxPool2d(2, 2)\n",
        "        )\n",
        "\n",
        "        self._to_linear = None\n",
        "        self.calculate_linear_output((1, 21, 128))\n",
        "        print(f'Linear output: {self._to_linear}')\n",
        "\n",
        "        self.fc_layers = nn.Sequential(\n",
        "            nn.Linear(self._to_linear, 1024),\n",
        "            activation_fn,\n",
        "            nn.Linear(1024, 256),\n",
        "            activation_fn,\n",
        "            nn.Linear(256, 32),\n",
        "            activation_fn,\n",
        "            nn.Linear(32, out_dim)\n",
        "        )\n",
        "\n",
        "    def calculate_linear_output(self, shape):\n",
        "        output = torch.ones(1, *shape)\n",
        "        output = self.conv_layers(output)\n",
        "        self._to_linear = output.numel()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_layers(x)\n",
        "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
        "        x = self.fc_layers(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "byAR5ZWwV9n3"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_evaluate(train_loader, val_loader, test_loader, model_class, loss_fn, activation_functions, num_classes, num_epochs=10, lr=LR):\n",
        "    results = {'Activation Function': [], 'Accuracy': [], 'F1 Score': []}\n",
        "\n",
        "    for activation_name, activation_fn in activation_functions.items():\n",
        "        print(f\"Training with {activation_name} activation function...\")\n",
        "        set_seed()\n",
        "        model = model_class(num_classes, activation_fn)\n",
        "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "        best_f1_score = 0.0\n",
        "        best_model_state = None\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            print(f\"Epoch {epoch+1} with {activation_name}\\n-------------------------------\")\n",
        "            train_loss = train_loop(train_loader, model, loss_fn, optimizer, device=True)\n",
        "            print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss}')\n",
        "            report = evaluate(val_loader, model, loss_fn, device=True, return_dict=True)\n",
        "            val_f1_score = report['weighted avg']['f1-score']\n",
        "\n",
        "            if val_f1_score > best_f1_score:\n",
        "                best_f1_score = val_f1_score\n",
        "                best_model_state = model.state_dict()\n",
        "                print(f\"New best model with F1 score: {best_f1_score}\")\n",
        "                torch.save(best_model_state, f'best_model_{activation_name}.pth')\n",
        "\n",
        "        print(f\"Loading the best model for {activation_name} and evaluating on the test set.\")\n",
        "        best_model = model_class(num_classes, activation_fn)\n",
        "        best_model.load_state_dict(torch.load(f'best_model_{activation_name}.pth'))\n",
        "\n",
        "        report = evaluate(test_loader, best_model, loss_fn, device=True, return_dict=True)\n",
        "        print(f\"Final Test - {activation_name}: Accuracy: {report['accuracy']}, F1: {report['weighted avg']['f1-score']}\")\n",
        "\n",
        "        results['Activation Function'].append(activation_name)\n",
        "        results['Accuracy'].append(report['accuracy'])\n",
        "        results['F1 Score'].append(report['weighted avg']['f1-score'])\n",
        "\n",
        "    return results\n",
        "\n",
        "activation_functions = {\n",
        "    'ReLU': nn.ReLU(),\n",
        "    'Sigmoid': nn.Sigmoid(),\n",
        "    'Tanh': nn.Tanh(),\n",
        "    'LeakyReLU': nn.LeakyReLU()\n",
        "}\n",
        "\n",
        "results = train_and_evaluate(train_loader_melgrams, val_loader_melgrams, test_loader_melgrams, MusicGenreCNN_Pooling_Activated, loss_fn, activation_functions, num_classes)\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "print(results_df)\n"
      ],
      "metadata": {
        "id": "he7bQW7hV_Ex",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f73a4e50-3a8f-4469-e84c-f2fbb482c506"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with ReLU activation function...\n",
            "Linear output: 1024\n",
            "Epoch 1 with ReLU\n",
            "-------------------------------\n",
            "loss: 1.444518  [    0/ 3200]\n",
            "loss: 1.488093  [ 1600/ 3200]\n",
            "Epoch [1/10], Train Loss: 1.2934224957227707\n",
            "Test Error: \n",
            " Accuracy: 47.8%, Avg loss: 0.067768 \n",
            "\n",
            "{'blues': {'precision': 0.3106060606060606, 'recall': 0.205, 'f1-score': 0.2469879518072289, 'support': 200}, 'classical': {'precision': 0.6091549295774648, 'recall': 0.865, 'f1-score': 0.7148760330578513, 'support': 200}, 'hiphop': {'precision': 0.4375, 'recall': 0.84, 'f1-score': 0.5753424657534247, 'support': 200}, 'rock_metal_hardrock': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 200}, 'accuracy': 0.4775, 'macro avg': {'precision': 0.3393152475458813, 'recall': 0.47750000000000004, 'f1-score': 0.38430161265462626, 'support': 800}, 'weighted avg': {'precision': 0.3393152475458813, 'recall': 0.4775, 'f1-score': 0.3843016126546262, 'support': 800}}\n",
            "New best model with F1 score: 0.3843016126546262\n",
            "Epoch 2 with ReLU\n",
            "-------------------------------\n",
            "loss: 0.983883  [    0/ 3200]\n",
            "loss: 0.910355  [ 1600/ 3200]\n",
            "Epoch [2/10], Train Loss: 1.0034286820888518\n",
            "Test Error: \n",
            " Accuracy: 53.2%, Avg loss: 0.061165 \n",
            "\n",
            "{'blues': {'precision': 0.4188034188034188, 'recall': 0.245, 'f1-score': 0.30914826498422715, 'support': 200}, 'classical': {'precision': 0.8021390374331551, 'recall': 0.75, 'f1-score': 0.7751937984496123, 'support': 200}, 'hiphop': {'precision': 0.4593908629441624, 'recall': 0.905, 'f1-score': 0.6094276094276094, 'support': 200}, 'rock_metal_hardrock': {'precision': 0.45098039215686275, 'recall': 0.23, 'f1-score': 0.304635761589404, 'support': 200}, 'accuracy': 0.5325, 'macro avg': {'precision': 0.5328284278343998, 'recall': 0.5325, 'f1-score': 0.4996013586127132, 'support': 800}, 'weighted avg': {'precision': 0.5328284278343998, 'recall': 0.5325, 'f1-score': 0.49960135861271326, 'support': 800}}\n",
            "New best model with F1 score: 0.49960135861271326\n",
            "Epoch 3 with ReLU\n",
            "-------------------------------\n",
            "loss: 0.947864  [    0/ 3200]\n",
            "loss: 0.998168  [ 1600/ 3200]\n",
            "Epoch [3/10], Train Loss: 0.8449421358108521\n",
            "Test Error: \n",
            " Accuracy: 54.4%, Avg loss: 0.064721 \n",
            "\n",
            "{'blues': {'precision': 0.6, 'recall': 0.255, 'f1-score': 0.35789473684210527, 'support': 200}, 'classical': {'precision': 0.8104265402843602, 'recall': 0.855, 'f1-score': 0.8321167883211679, 'support': 200}, 'hiphop': {'precision': 0.8888888888888888, 'recall': 0.16, 'f1-score': 0.27118644067796616, 'support': 200}, 'rock_metal_hardrock': {'precision': 0.38675213675213677, 'recall': 0.905, 'f1-score': 0.5419161676646707, 'support': 200}, 'accuracy': 0.54375, 'macro avg': {'precision': 0.6715168914813464, 'recall': 0.54375, 'f1-score': 0.5007785333764775, 'support': 800}, 'weighted avg': {'precision': 0.6715168914813464, 'recall': 0.54375, 'f1-score': 0.5007785333764775, 'support': 800}}\n",
            "New best model with F1 score: 0.5007785333764775\n",
            "Epoch 4 with ReLU\n",
            "-------------------------------\n",
            "loss: 1.132704  [    0/ 3200]\n",
            "loss: 0.985505  [ 1600/ 3200]\n",
            "Epoch [4/10], Train Loss: 0.7457002173364162\n",
            "Test Error: \n",
            " Accuracy: 69.4%, Avg loss: 0.051277 \n",
            "\n",
            "{'blues': {'precision': 0.6761904761904762, 'recall': 0.355, 'f1-score': 0.46557377049180326, 'support': 200}, 'classical': {'precision': 0.8133333333333334, 'recall': 0.915, 'f1-score': 0.8611764705882353, 'support': 200}, 'hiphop': {'precision': 0.8181818181818182, 'recall': 0.675, 'f1-score': 0.7397260273972603, 'support': 200}, 'rock_metal_hardrock': {'precision': 0.5442622950819672, 'recall': 0.83, 'f1-score': 0.6574257425742575, 'support': 200}, 'accuracy': 0.69375, 'macro avg': {'precision': 0.7129919806968987, 'recall': 0.69375, 'f1-score': 0.6809755027628891, 'support': 800}, 'weighted avg': {'precision': 0.7129919806968988, 'recall': 0.69375, 'f1-score': 0.6809755027628891, 'support': 800}}\n",
            "New best model with F1 score: 0.6809755027628891\n",
            "Epoch 5 with ReLU\n",
            "-------------------------------\n",
            "loss: 0.355025  [    0/ 3200]\n",
            "loss: 0.718302  [ 1600/ 3200]\n",
            "Epoch [5/10], Train Loss: 0.6294380864500999\n",
            "Test Error: \n",
            " Accuracy: 68.9%, Avg loss: 0.051011 \n",
            "\n",
            "{'blues': {'precision': 0.6052631578947368, 'recall': 0.345, 'f1-score': 0.4394904458598726, 'support': 200}, 'classical': {'precision': 0.8275862068965517, 'recall': 0.96, 'f1-score': 0.888888888888889, 'support': 200}, 'hiphop': {'precision': 0.6006711409395973, 'recall': 0.895, 'f1-score': 0.7188755020080321, 'support': 200}, 'rock_metal_hardrock': {'precision': 0.7115384615384616, 'recall': 0.555, 'f1-score': 0.6235955056179776, 'support': 200}, 'accuracy': 0.68875, 'macro avg': {'precision': 0.6862647418173369, 'recall': 0.6887500000000001, 'f1-score': 0.6677125855936928, 'support': 800}, 'weighted avg': {'precision': 0.686264741817337, 'recall': 0.68875, 'f1-score': 0.6677125855936927, 'support': 800}}\n",
            "Epoch 6 with ReLU\n",
            "-------------------------------\n",
            "loss: 0.758158  [    0/ 3200]\n",
            "loss: 0.710709  [ 1600/ 3200]\n",
            "Epoch [6/10], Train Loss: 0.5669556204229593\n",
            "Test Error: \n",
            " Accuracy: 72.9%, Avg loss: 0.047397 \n",
            "\n",
            "{'blues': {'precision': 0.6230366492146597, 'recall': 0.595, 'f1-score': 0.6086956521739131, 'support': 200}, 'classical': {'precision': 0.954248366013072, 'recall': 0.73, 'f1-score': 0.8271954674220963, 'support': 200}, 'hiphop': {'precision': 0.8288770053475936, 'recall': 0.775, 'f1-score': 0.8010335917312661, 'support': 200}, 'rock_metal_hardrock': {'precision': 0.6059479553903345, 'recall': 0.815, 'f1-score': 0.6950959488272921, 'support': 200}, 'accuracy': 0.72875, 'macro avg': {'precision': 0.753027493991415, 'recall': 0.72875, 'f1-score': 0.7330051650386419, 'support': 800}, 'weighted avg': {'precision': 0.7530274939914149, 'recall': 0.72875, 'f1-score': 0.7330051650386418, 'support': 800}}\n",
            "New best model with F1 score: 0.7330051650386418\n",
            "Epoch 7 with ReLU\n",
            "-------------------------------\n",
            "loss: 0.431936  [    0/ 3200]\n",
            "loss: 0.472140  [ 1600/ 3200]\n",
            "Epoch [7/10], Train Loss: 0.5332191072404384\n",
            "Test Error: \n",
            " Accuracy: 72.2%, Avg loss: 0.050564 \n",
            "\n",
            "{'blues': {'precision': 0.65625, 'recall': 0.42, 'f1-score': 0.5121951219512195, 'support': 200}, 'classical': {'precision': 0.7721518987341772, 'recall': 0.915, 'f1-score': 0.8375286041189931, 'support': 200}, 'hiphop': {'precision': 0.75, 'recall': 0.825, 'f1-score': 0.7857142857142856, 'support': 200}, 'rock_metal_hardrock': {'precision': 0.6790697674418604, 'recall': 0.73, 'f1-score': 0.7036144578313253, 'support': 200}, 'accuracy': 0.7225, 'macro avg': {'precision': 0.7143679165440094, 'recall': 0.7225, 'f1-score': 0.7097631174039559, 'support': 800}, 'weighted avg': {'precision': 0.7143679165440093, 'recall': 0.7225, 'f1-score': 0.7097631174039559, 'support': 800}}\n",
            "Epoch 8 with ReLU\n",
            "-------------------------------\n",
            "loss: 0.371114  [    0/ 3200]\n",
            "loss: 0.490980  [ 1600/ 3200]\n",
            "Epoch [8/10], Train Loss: 0.45370724707841875\n",
            "Test Error: \n",
            " Accuracy: 69.1%, Avg loss: 0.051051 \n",
            "\n",
            "{'blues': {'precision': 0.6196319018404908, 'recall': 0.505, 'f1-score': 0.556473829201102, 'support': 200}, 'classical': {'precision': 0.9, 'recall': 0.765, 'f1-score': 0.827027027027027, 'support': 200}, 'hiphop': {'precision': 0.9007633587786259, 'recall': 0.59, 'f1-score': 0.7129909365558913, 'support': 200}, 'rock_metal_hardrock': {'precision': 0.5386904761904762, 'recall': 0.905, 'f1-score': 0.6753731343283582, 'support': 200}, 'accuracy': 0.69125, 'macro avg': {'precision': 0.7397714342023982, 'recall': 0.6912499999999999, 'f1-score': 0.6929662317780946, 'support': 800}, 'weighted avg': {'precision': 0.7397714342023981, 'recall': 0.69125, 'f1-score': 0.6929662317780947, 'support': 800}}\n",
            "Epoch 9 with ReLU\n",
            "-------------------------------\n",
            "loss: 0.388621  [    0/ 3200]\n",
            "loss: 0.352554  [ 1600/ 3200]\n",
            "Epoch [9/10], Train Loss: 0.40587703822180626\n",
            "Test Error: \n",
            " Accuracy: 73.9%, Avg loss: 0.045489 \n",
            "\n",
            "{'blues': {'precision': 0.7391304347826086, 'recall': 0.425, 'f1-score': 0.5396825396825397, 'support': 200}, 'classical': {'precision': 0.7791164658634538, 'recall': 0.97, 'f1-score': 0.8641425389755011, 'support': 200}, 'hiphop': {'precision': 0.7477064220183486, 'recall': 0.815, 'f1-score': 0.7799043062200957, 'support': 200}, 'rock_metal_hardrock': {'precision': 0.6834862385321101, 'recall': 0.745, 'f1-score': 0.7129186602870814, 'support': 200}, 'accuracy': 0.73875, 'macro avg': {'precision': 0.7373598902991303, 'recall': 0.73875, 'f1-score': 0.7241620112913045, 'support': 800}, 'weighted avg': {'precision': 0.7373598902991303, 'recall': 0.73875, 'f1-score': 0.7241620112913045, 'support': 800}}\n",
            "Epoch 10 with ReLU\n",
            "-------------------------------\n",
            "loss: 0.360269  [    0/ 3200]\n",
            "loss: 0.207111  [ 1600/ 3200]\n",
            "Epoch [10/10], Train Loss: 0.3594032049737871\n",
            "Test Error: \n",
            " Accuracy: 79.1%, Avg loss: 0.042165 \n",
            "\n",
            "{'blues': {'precision': 0.6935483870967742, 'recall': 0.645, 'f1-score': 0.6683937823834197, 'support': 200}, 'classical': {'precision': 0.892018779342723, 'recall': 0.95, 'f1-score': 0.9200968523002421, 'support': 200}, 'hiphop': {'precision': 0.7746478873239436, 'recall': 0.825, 'f1-score': 0.7990314769975787, 'support': 200}, 'rock_metal_hardrock': {'precision': 0.7925531914893617, 'recall': 0.745, 'f1-score': 0.7680412371134021, 'support': 200}, 'accuracy': 0.79125, 'macro avg': {'precision': 0.7881920613132006, 'recall': 0.79125, 'f1-score': 0.7888908371986606, 'support': 800}, 'weighted avg': {'precision': 0.7881920613132007, 'recall': 0.79125, 'f1-score': 0.7888908371986606, 'support': 800}}\n",
            "New best model with F1 score: 0.7888908371986606\n",
            "Loading the best model for ReLU and evaluating on the test set.\n",
            "Linear output: 1024\n",
            "Test Error: \n",
            " Accuracy: 71.7%, Avg loss: 0.051057 \n",
            "\n",
            "{'blues': {'precision': 0.5895061728395061, 'recall': 0.5895061728395061, 'f1-score': 0.5895061728395061, 'support': 324}, 'classical': {'precision': 0.7844827586206896, 'recall': 0.9191919191919192, 'f1-score': 0.8465116279069766, 'support': 297}, 'hiphop': {'precision': 0.7989276139410187, 'recall': 0.8370786516853933, 'f1-score': 0.8175582990397805, 'support': 356}, 'rock_metal_hardrock': {'precision': 0.6797583081570997, 'recall': 0.5639097744360902, 'f1-score': 0.6164383561643835, 'support': 399}, 'accuracy': 0.717296511627907, 'macro avg': {'precision': 0.7131687133895785, 'recall': 0.7274216295382272, 'f1-score': 0.7175036139876617, 'support': 1376}, 'weighted avg': {'precision': 0.7119427142645568, 'recall': 0.717296511627907, 'f1-score': 0.7117904157384615, 'support': 1376}}\n",
            "Final Test - ReLU: Accuracy: 0.717296511627907, F1: 0.7117904157384615\n",
            "Training with Sigmoid activation function...\n",
            "Linear output: 1024\n",
            "Epoch 1 with Sigmoid\n",
            "-------------------------------\n",
            "loss: 1.489955  [    0/ 3200]\n",
            "loss: 1.374879  [ 1600/ 3200]\n",
            "Epoch [1/10], Train Loss: 1.3924164170026778\n",
            "Test Error: \n",
            " Accuracy: 25.0%, Avg loss: 0.086933 \n",
            "\n",
            "{'blues': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 200}, 'classical': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 200}, 'hiphop': {'precision': 0.25, 'recall': 1.0, 'f1-score': 0.4, 'support': 200}, 'rock_metal_hardrock': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 200}, 'accuracy': 0.25, 'macro avg': {'precision': 0.0625, 'recall': 0.25, 'f1-score': 0.1, 'support': 800}, 'weighted avg': {'precision': 0.0625, 'recall': 0.25, 'f1-score': 0.1, 'support': 800}}\n",
            "New best model with F1 score: 0.1\n",
            "Epoch 2 with Sigmoid\n",
            "-------------------------------\n",
            "loss: 1.365894  [    0/ 3200]\n",
            "loss: 1.385437  [ 1600/ 3200]\n",
            "Epoch [2/10], Train Loss: 1.3894331938028335\n",
            "Test Error: \n",
            " Accuracy: 25.0%, Avg loss: 0.086650 \n",
            "\n",
            "{'blues': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 200}, 'classical': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 200}, 'hiphop': {'precision': 0.25, 'recall': 1.0, 'f1-score': 0.4, 'support': 200}, 'rock_metal_hardrock': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 200}, 'accuracy': 0.25, 'macro avg': {'precision': 0.0625, 'recall': 0.25, 'f1-score': 0.1, 'support': 800}, 'weighted avg': {'precision': 0.0625, 'recall': 0.25, 'f1-score': 0.1, 'support': 800}}\n",
            "Epoch 3 with Sigmoid\n",
            "-------------------------------\n",
            "loss: 1.385394  [    0/ 3200]\n",
            "loss: 1.404098  [ 1600/ 3200]\n",
            "Epoch [3/10], Train Loss: 1.388364723920822\n",
            "Test Error: \n",
            " Accuracy: 25.0%, Avg loss: 0.086706 \n",
            "\n",
            "{'blues': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 200}, 'classical': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 200}, 'hiphop': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 200}, 'rock_metal_hardrock': {'precision': 0.25, 'recall': 1.0, 'f1-score': 0.4, 'support': 200}, 'accuracy': 0.25, 'macro avg': {'precision': 0.0625, 'recall': 0.25, 'f1-score': 0.1, 'support': 800}, 'weighted avg': {'precision': 0.0625, 'recall': 0.25, 'f1-score': 0.1, 'support': 800}}\n",
            "Epoch 4 with Sigmoid\n",
            "-------------------------------\n",
            "loss: 1.379894  [    0/ 3200]\n",
            "loss: 1.404062  [ 1600/ 3200]\n",
            "Epoch [4/10], Train Loss: 1.389993843436241\n",
            "Test Error: \n",
            " Accuracy: 25.0%, Avg loss: 0.086670 \n",
            "\n",
            "{'blues': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 200}, 'classical': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 200}, 'hiphop': {'precision': 0.25, 'recall': 1.0, 'f1-score': 0.4, 'support': 200}, 'rock_metal_hardrock': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 200}, 'accuracy': 0.25, 'macro avg': {'precision': 0.0625, 'recall': 0.25, 'f1-score': 0.1, 'support': 800}, 'weighted avg': {'precision': 0.0625, 'recall': 0.25, 'f1-score': 0.1, 'support': 800}}\n",
            "Epoch 5 with Sigmoid\n",
            "-------------------------------\n",
            "loss: 1.376764  [    0/ 3200]\n",
            "loss: 1.375683  [ 1600/ 3200]\n",
            "Epoch [5/10], Train Loss: 1.3886680287122726\n",
            "Test Error: \n",
            " Accuracy: 25.0%, Avg loss: 0.086653 \n",
            "\n",
            "{'blues': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 200}, 'classical': {'precision': 0.25, 'recall': 1.0, 'f1-score': 0.4, 'support': 200}, 'hiphop': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 200}, 'rock_metal_hardrock': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 200}, 'accuracy': 0.25, 'macro avg': {'precision': 0.0625, 'recall': 0.25, 'f1-score': 0.1, 'support': 800}, 'weighted avg': {'precision': 0.0625, 'recall': 0.25, 'f1-score': 0.1, 'support': 800}}\n",
            "Epoch 6 with Sigmoid\n",
            "-------------------------------\n",
            "loss: 1.384061  [    0/ 3200]\n",
            "loss: 1.385964  [ 1600/ 3200]\n",
            "Epoch [6/10], Train Loss: 1.3890004056692122\n",
            "Test Error: \n",
            " Accuracy: 25.0%, Avg loss: 0.086684 \n",
            "\n",
            "{'blues': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 200}, 'classical': {'precision': 0.25, 'recall': 1.0, 'f1-score': 0.4, 'support': 200}, 'hiphop': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 200}, 'rock_metal_hardrock': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 200}, 'accuracy': 0.25, 'macro avg': {'precision': 0.0625, 'recall': 0.25, 'f1-score': 0.1, 'support': 800}, 'weighted avg': {'precision': 0.0625, 'recall': 0.25, 'f1-score': 0.1, 'support': 800}}\n",
            "Epoch 7 with Sigmoid\n",
            "-------------------------------\n",
            "loss: 1.390189  [    0/ 3200]\n",
            "loss: 1.400390  [ 1600/ 3200]\n",
            "Epoch [7/10], Train Loss: 1.3890631306171417\n",
            "Test Error: \n",
            " Accuracy: 25.0%, Avg loss: 0.086684 \n",
            "\n",
            "{'blues': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 200}, 'classical': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 200}, 'hiphop': {'precision': 0.25, 'recall': 1.0, 'f1-score': 0.4, 'support': 200}, 'rock_metal_hardrock': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 200}, 'accuracy': 0.25, 'macro avg': {'precision': 0.0625, 'recall': 0.25, 'f1-score': 0.1, 'support': 800}, 'weighted avg': {'precision': 0.0625, 'recall': 0.25, 'f1-score': 0.1, 'support': 800}}\n",
            "Epoch 8 with Sigmoid\n",
            "-------------------------------\n",
            "loss: 1.381873  [    0/ 3200]\n",
            "loss: 1.386686  [ 1600/ 3200]\n",
            "Epoch [8/10], Train Loss: 1.3886413329839706\n",
            "Test Error: \n",
            " Accuracy: 25.0%, Avg loss: 0.086737 \n",
            "\n",
            "{'blues': {'precision': 0.25, 'recall': 1.0, 'f1-score': 0.4, 'support': 200}, 'classical': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 200}, 'hiphop': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 200}, 'rock_metal_hardrock': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 200}, 'accuracy': 0.25, 'macro avg': {'precision': 0.0625, 'recall': 0.25, 'f1-score': 0.1, 'support': 800}, 'weighted avg': {'precision': 0.0625, 'recall': 0.25, 'f1-score': 0.1, 'support': 800}}\n",
            "Epoch 9 with Sigmoid\n",
            "-------------------------------\n",
            "loss: 1.378524  [    0/ 3200]\n",
            "loss: 1.394634  [ 1600/ 3200]\n",
            "Epoch [9/10], Train Loss: 1.3887213277816772\n",
            "Test Error: \n",
            " Accuracy: 25.0%, Avg loss: 0.086698 \n",
            "\n",
            "{'blues': {'precision': 0.25, 'recall': 1.0, 'f1-score': 0.4, 'support': 200}, 'classical': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 200}, 'hiphop': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 200}, 'rock_metal_hardrock': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 200}, 'accuracy': 0.25, 'macro avg': {'precision': 0.0625, 'recall': 0.25, 'f1-score': 0.1, 'support': 800}, 'weighted avg': {'precision': 0.0625, 'recall': 0.25, 'f1-score': 0.1, 'support': 800}}\n",
            "Epoch 10 with Sigmoid\n",
            "-------------------------------\n",
            "loss: 1.382108  [    0/ 3200]\n",
            "loss: 1.391711  [ 1600/ 3200]\n",
            "Epoch [10/10], Train Loss: 1.3884206473827363\n",
            "Test Error: \n",
            " Accuracy: 25.0%, Avg loss: 0.086675 \n",
            "\n",
            "{'blues': {'precision': 0.25, 'recall': 1.0, 'f1-score': 0.4, 'support': 200}, 'classical': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 200}, 'hiphop': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 200}, 'rock_metal_hardrock': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 200}, 'accuracy': 0.25, 'macro avg': {'precision': 0.0625, 'recall': 0.25, 'f1-score': 0.1, 'support': 800}, 'weighted avg': {'precision': 0.0625, 'recall': 0.25, 'f1-score': 0.1, 'support': 800}}\n",
            "Loading the best model for Sigmoid and evaluating on the test set.\n",
            "Linear output: 1024\n",
            "Test Error: \n",
            " Accuracy: 25.9%, Avg loss: 0.086371 \n",
            "\n",
            "{'blues': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 324}, 'classical': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 297}, 'hiphop': {'precision': 0.25872093023255816, 'recall': 1.0, 'f1-score': 0.41108545034642036, 'support': 356}, 'rock_metal_hardrock': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 399}, 'accuracy': 0.25872093023255816, 'macro avg': {'precision': 0.06468023255813954, 'recall': 0.25, 'f1-score': 0.10277136258660509, 'support': 1376}, 'weighted avg': {'precision': 0.06693651974040021, 'recall': 0.25872093023255816, 'f1-score': 0.10635641011869597, 'support': 1376}}\n",
            "Final Test - Sigmoid: Accuracy: 0.25872093023255816, F1: 0.10635641011869597\n",
            "Training with Tanh activation function...\n",
            "Linear output: 1024\n",
            "Epoch 1 with Tanh\n",
            "-------------------------------\n",
            "loss: 1.401430  [    0/ 3200]\n",
            "loss: 0.958953  [ 1600/ 3200]\n",
            "Epoch [1/10], Train Loss: 1.216002717912197\n",
            "Test Error: \n",
            " Accuracy: 48.8%, Avg loss: 0.068220 \n",
            "\n",
            "{'blues': {'precision': 0.18518518518518517, 'recall': 0.05, 'f1-score': 0.07874015748031496, 'support': 200}, 'classical': {'precision': 0.5866666666666667, 'recall': 0.88, 'f1-score': 0.704, 'support': 200}, 'hiphop': {'precision': 0.463768115942029, 'recall': 0.64, 'f1-score': 0.5378151260504201, 'support': 200}, 'rock_metal_hardrock': {'precision': 0.4470588235294118, 'recall': 0.38, 'f1-score': 0.4108108108108108, 'support': 200}, 'accuracy': 0.4875, 'macro avg': {'precision': 0.42066969783082314, 'recall': 0.48750000000000004, 'f1-score': 0.43284152358538647, 'support': 800}, 'weighted avg': {'precision': 0.4206696978308232, 'recall': 0.4875, 'f1-score': 0.43284152358538647, 'support': 800}}\n",
            "New best model with F1 score: 0.43284152358538647\n",
            "Epoch 2 with Tanh\n",
            "-------------------------------\n",
            "loss: 1.047845  [    0/ 3200]\n",
            "loss: 1.085659  [ 1600/ 3200]\n",
            "Epoch [2/10], Train Loss: 1.058018556535244\n",
            "Test Error: \n",
            " Accuracy: 47.9%, Avg loss: 0.075519 \n",
            "\n",
            "{'blues': {'precision': 0.29850746268656714, 'recall': 0.1, 'f1-score': 0.149812734082397, 'support': 200}, 'classical': {'precision': 0.8571428571428571, 'recall': 0.75, 'f1-score': 0.7999999999999999, 'support': 200}, 'hiphop': {'precision': 0.3667296786389414, 'recall': 0.97, 'f1-score': 0.532235939643347, 'support': 200}, 'rock_metal_hardrock': {'precision': 0.6551724137931034, 'recall': 0.095, 'f1-score': 0.16593886462882096, 'support': 200}, 'accuracy': 0.47875, 'macro avg': {'precision': 0.5443881030653672, 'recall': 0.47874999999999995, 'f1-score': 0.4119968845886412, 'support': 800}, 'weighted avg': {'precision': 0.5443881030653673, 'recall': 0.47875, 'f1-score': 0.41199688458864125, 'support': 800}}\n",
            "Epoch 3 with Tanh\n",
            "-------------------------------\n",
            "loss: 1.116765  [    0/ 3200]\n",
            "loss: 0.815291  [ 1600/ 3200]\n",
            "Epoch [3/10], Train Loss: 0.9713627409934997\n",
            "Test Error: \n",
            " Accuracy: 47.9%, Avg loss: 0.068535 \n",
            "\n",
            "{'blues': {'precision': 0.41847826086956524, 'recall': 0.385, 'f1-score': 0.4010416666666667, 'support': 200}, 'classical': {'precision': 0.875, 'recall': 0.77, 'f1-score': 0.8191489361702129, 'support': 200}, 'hiphop': {'precision': 0.8, 'recall': 0.04, 'f1-score': 0.07619047619047618, 'support': 200}, 'rock_metal_hardrock': {'precision': 0.33488372093023255, 'recall': 0.72, 'f1-score': 0.4571428571428571, 'support': 200}, 'accuracy': 0.47875, 'macro avg': {'precision': 0.6070904954499494, 'recall': 0.47875, 'f1-score': 0.4383809840425532, 'support': 800}, 'weighted avg': {'precision': 0.6070904954499494, 'recall': 0.47875, 'f1-score': 0.4383809840425532, 'support': 800}}\n",
            "New best model with F1 score: 0.4383809840425532\n",
            "Epoch 4 with Tanh\n",
            "-------------------------------\n",
            "loss: 1.142001  [    0/ 3200]\n",
            "loss: 0.876501  [ 1600/ 3200]\n",
            "Epoch [4/10], Train Loss: 0.8585981389880181\n",
            "Test Error: \n",
            " Accuracy: 57.6%, Avg loss: 0.058275 \n",
            "\n",
            "{'blues': {'precision': 0.4715447154471545, 'recall': 0.29, 'f1-score': 0.35913312693498456, 'support': 200}, 'classical': {'precision': 0.7396694214876033, 'recall': 0.895, 'f1-score': 0.8099547511312218, 'support': 200}, 'hiphop': {'precision': 0.8085106382978723, 'recall': 0.38, 'f1-score': 0.5170068027210885, 'support': 200}, 'rock_metal_hardrock': {'precision': 0.4340175953079179, 'recall': 0.74, 'f1-score': 0.5471349353049907, 'support': 200}, 'accuracy': 0.57625, 'macro avg': {'precision': 0.613435592635137, 'recall': 0.5762499999999999, 'f1-score': 0.5583074040230713, 'support': 800}, 'weighted avg': {'precision': 0.6134355926351369, 'recall': 0.57625, 'f1-score': 0.5583074040230714, 'support': 800}}\n",
            "New best model with F1 score: 0.5583074040230714\n",
            "Epoch 5 with Tanh\n",
            "-------------------------------\n",
            "loss: 0.746934  [    0/ 3200]\n",
            "loss: 0.901411  [ 1600/ 3200]\n",
            "Epoch [5/10], Train Loss: 0.8378609359264374\n",
            "Test Error: \n",
            " Accuracy: 58.9%, Avg loss: 0.059613 \n",
            "\n",
            "{'blues': {'precision': 0.3448275862068966, 'recall': 0.15, 'f1-score': 0.20905923344947733, 'support': 200}, 'classical': {'precision': 0.7018867924528301, 'recall': 0.93, 'f1-score': 0.8, 'support': 200}, 'hiphop': {'precision': 0.52046783625731, 'recall': 0.89, 'f1-score': 0.6568265682656826, 'support': 200}, 'rock_metal_hardrock': {'precision': 0.7264150943396226, 'recall': 0.385, 'f1-score': 0.5032679738562091, 'support': 200}, 'accuracy': 0.58875, 'macro avg': {'precision': 0.5733993273141649, 'recall': 0.5887500000000001, 'f1-score': 0.5422884438928424, 'support': 800}, 'weighted avg': {'precision': 0.5733993273141649, 'recall': 0.58875, 'f1-score': 0.5422884438928424, 'support': 800}}\n",
            "Epoch 6 with Tanh\n",
            "-------------------------------\n",
            "loss: 0.646281  [    0/ 3200]\n",
            "loss: 0.686093  [ 1600/ 3200]\n",
            "Epoch [6/10], Train Loss: 0.7663109724223613\n",
            "Test Error: \n",
            " Accuracy: 64.0%, Avg loss: 0.053440 \n",
            "\n",
            "{'blues': {'precision': 0.47058823529411764, 'recall': 0.32, 'f1-score': 0.38095238095238104, 'support': 200}, 'classical': {'precision': 0.841025641025641, 'recall': 0.82, 'f1-score': 0.830379746835443, 'support': 200}, 'hiphop': {'precision': 0.6956521739130435, 'recall': 0.72, 'f1-score': 0.7076167076167076, 'support': 200}, 'rock_metal_hardrock': {'precision': 0.5343511450381679, 'recall': 0.7, 'f1-score': 0.606060606060606, 'support': 200}, 'accuracy': 0.64, 'macro avg': {'precision': 0.6354042988177426, 'recall': 0.6399999999999999, 'f1-score': 0.6312523603662844, 'support': 800}, 'weighted avg': {'precision': 0.6354042988177424, 'recall': 0.64, 'f1-score': 0.6312523603662844, 'support': 800}}\n",
            "New best model with F1 score: 0.6312523603662844\n",
            "Epoch 7 with Tanh\n",
            "-------------------------------\n",
            "loss: 0.547822  [    0/ 3200]\n",
            "loss: 0.562481  [ 1600/ 3200]\n",
            "Epoch [7/10], Train Loss: 0.7142721179127693\n",
            "Test Error: \n",
            " Accuracy: 68.4%, Avg loss: 0.049069 \n",
            "\n",
            "{'blues': {'precision': 0.6355140186915887, 'recall': 0.34, 'f1-score': 0.4429967426710098, 'support': 200}, 'classical': {'precision': 0.8481675392670157, 'recall': 0.81, 'f1-score': 0.8286445012787724, 'support': 200}, 'hiphop': {'precision': 0.6709401709401709, 'recall': 0.785, 'f1-score': 0.7235023041474653, 'support': 200}, 'rock_metal_hardrock': {'precision': 0.5970149253731343, 'recall': 0.8, 'f1-score': 0.6837606837606838, 'support': 200}, 'accuracy': 0.68375, 'macro avg': {'precision': 0.6879091635679774, 'recall': 0.6837500000000001, 'f1-score': 0.6697260579644828, 'support': 800}, 'weighted avg': {'precision': 0.6879091635679774, 'recall': 0.68375, 'f1-score': 0.6697260579644828, 'support': 800}}\n",
            "New best model with F1 score: 0.6697260579644828\n",
            "Epoch 8 with Tanh\n",
            "-------------------------------\n",
            "loss: 0.342324  [    0/ 3200]\n",
            "loss: 0.726857  [ 1600/ 3200]\n",
            "Epoch [8/10], Train Loss: 0.6265141327679157\n",
            "Test Error: \n",
            " Accuracy: 68.6%, Avg loss: 0.051075 \n",
            "\n",
            "{'blues': {'precision': 0.526829268292683, 'recall': 0.54, 'f1-score': 0.5333333333333333, 'support': 200}, 'classical': {'precision': 0.7935779816513762, 'recall': 0.865, 'f1-score': 0.8277511961722489, 'support': 200}, 'hiphop': {'precision': 0.8541666666666666, 'recall': 0.615, 'f1-score': 0.7151162790697674, 'support': 200}, 'rock_metal_hardrock': {'precision': 0.6223175965665236, 'recall': 0.725, 'f1-score': 0.6697459584295612, 'support': 200}, 'accuracy': 0.68625, 'macro avg': {'precision': 0.6992228782943123, 'recall': 0.68625, 'f1-score': 0.6864866917512278, 'support': 800}, 'weighted avg': {'precision': 0.6992228782943123, 'recall': 0.68625, 'f1-score': 0.6864866917512277, 'support': 800}}\n",
            "New best model with F1 score: 0.6864866917512277\n",
            "Epoch 9 with Tanh\n",
            "-------------------------------\n",
            "loss: 0.970260  [    0/ 3200]\n",
            "loss: 0.689972  [ 1600/ 3200]\n",
            "Epoch [9/10], Train Loss: 0.5951215203106404\n",
            "Test Error: \n",
            " Accuracy: 68.1%, Avg loss: 0.048599 \n",
            "\n",
            "{'blues': {'precision': 0.5235602094240838, 'recall': 0.5, 'f1-score': 0.5115089514066496, 'support': 200}, 'classical': {'precision': 0.7953488372093023, 'recall': 0.855, 'f1-score': 0.8240963855421686, 'support': 200}, 'hiphop': {'precision': 0.7928994082840237, 'recall': 0.67, 'f1-score': 0.7262872628726289, 'support': 200}, 'rock_metal_hardrock': {'precision': 0.6222222222222222, 'recall': 0.7, 'f1-score': 0.6588235294117647, 'support': 200}, 'accuracy': 0.68125, 'macro avg': {'precision': 0.683507669284908, 'recall': 0.6812499999999999, 'f1-score': 0.6801790323083029, 'support': 800}, 'weighted avg': {'precision': 0.683507669284908, 'recall': 0.68125, 'f1-score': 0.680179032308303, 'support': 800}}\n",
            "Epoch 10 with Tanh\n",
            "-------------------------------\n",
            "loss: 0.472010  [    0/ 3200]\n",
            "loss: 0.444736  [ 1600/ 3200]\n",
            "Epoch [10/10], Train Loss: 0.549170151501894\n",
            "Test Error: \n",
            " Accuracy: 64.4%, Avg loss: 0.051052 \n",
            "\n",
            "{'blues': {'precision': 0.4666666666666667, 'recall': 0.35, 'f1-score': 0.4, 'support': 200}, 'classical': {'precision': 0.7357723577235772, 'recall': 0.905, 'f1-score': 0.8116591928251122, 'support': 200}, 'hiphop': {'precision': 0.815068493150685, 'recall': 0.595, 'f1-score': 0.6878612716763005, 'support': 200}, 'rock_metal_hardrock': {'precision': 0.562015503875969, 'recall': 0.725, 'f1-score': 0.6331877729257642, 'support': 200}, 'accuracy': 0.64375, 'macro avg': {'precision': 0.6448807553542245, 'recall': 0.6437499999999999, 'f1-score': 0.6331770593567942, 'support': 800}, 'weighted avg': {'precision': 0.6448807553542245, 'recall': 0.64375, 'f1-score': 0.6331770593567942, 'support': 800}}\n",
            "Loading the best model for Tanh and evaluating on the test set.\n",
            "Linear output: 1024\n",
            "Test Error: \n",
            " Accuracy: 67.4%, Avg loss: 0.053273 \n",
            "\n",
            "{'blues': {'precision': 0.5078125, 'recall': 0.6018518518518519, 'f1-score': 0.5508474576271187, 'support': 324}, 'classical': {'precision': 0.7890173410404624, 'recall': 0.9191919191919192, 'f1-score': 0.8491446345256609, 'support': 297}, 'hiphop': {'precision': 0.8611111111111112, 'recall': 0.6095505617977528, 'f1-score': 0.7138157894736842, 'support': 356}, 'rock_metal_hardrock': {'precision': 0.6142131979695431, 'recall': 0.606516290726817, 'f1-score': 0.6103404791929381, 'support': 399}, 'accuracy': 0.6736918604651163, 'macro avg': {'precision': 0.6930385375302792, 'recall': 0.6842776558920852, 'f1-score': 0.6810370902048505, 'support': 1376}, 'weighted avg': {'precision': 0.6907674577285033, 'recall': 0.6736918604651163, 'f1-score': 0.6746473873371523, 'support': 1376}}\n",
            "Final Test - Tanh: Accuracy: 0.6736918604651163, F1: 0.6746473873371523\n",
            "Training with LeakyReLU activation function...\n",
            "Linear output: 1024\n",
            "Epoch 1 with LeakyReLU\n",
            "-------------------------------\n",
            "loss: 1.445329  [    0/ 3200]\n",
            "loss: 1.407190  [ 1600/ 3200]\n",
            "Epoch [1/10], Train Loss: 1.324104479253292\n",
            "Test Error: \n",
            " Accuracy: 46.9%, Avg loss: 0.067487 \n",
            "\n",
            "{'blues': {'precision': 0.4435483870967742, 'recall': 0.275, 'f1-score': 0.3395061728395062, 'support': 200}, 'classical': {'precision': 0.6324786324786325, 'recall': 0.74, 'f1-score': 0.6820276497695852, 'support': 200}, 'hiphop': {'precision': 0.3891402714932127, 'recall': 0.86, 'f1-score': 0.5358255451713396, 'support': 200}, 'rock_metal_hardrock': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 200}, 'accuracy': 0.46875, 'macro avg': {'precision': 0.36629182276715483, 'recall': 0.46875, 'f1-score': 0.38933984194510773, 'support': 800}, 'weighted avg': {'precision': 0.36629182276715483, 'recall': 0.46875, 'f1-score': 0.3893398419451078, 'support': 800}}\n",
            "New best model with F1 score: 0.3893398419451078\n",
            "Epoch 2 with LeakyReLU\n",
            "-------------------------------\n",
            "loss: 1.062834  [    0/ 3200]\n",
            "loss: 1.136487  [ 1600/ 3200]\n",
            "Epoch [2/10], Train Loss: 1.037448872923851\n",
            "Test Error: \n",
            " Accuracy: 50.7%, Avg loss: 0.065203 \n",
            "\n",
            "{'blues': {'precision': 0.36704119850187267, 'recall': 0.49, 'f1-score': 0.4197002141327623, 'support': 200}, 'classical': {'precision': 0.7905405405405406, 'recall': 0.585, 'f1-score': 0.6724137931034483, 'support': 200}, 'hiphop': {'precision': 0.49271137026239065, 'recall': 0.845, 'f1-score': 0.6224677716390423, 'support': 200}, 'rock_metal_hardrock': {'precision': 0.5238095238095238, 'recall': 0.11, 'f1-score': 0.18181818181818182, 'support': 200}, 'accuracy': 0.5075, 'macro avg': {'precision': 0.5435256582785819, 'recall': 0.5075, 'f1-score': 0.4740999901733587, 'support': 800}, 'weighted avg': {'precision': 0.5435256582785819, 'recall': 0.5075, 'f1-score': 0.4740999901733587, 'support': 800}}\n",
            "New best model with F1 score: 0.4740999901733587\n",
            "Epoch 3 with LeakyReLU\n",
            "-------------------------------\n",
            "loss: 0.907440  [    0/ 3200]\n",
            "loss: 0.982477  [ 1600/ 3200]\n",
            "Epoch [3/10], Train Loss: 0.9008803123235702\n",
            "Test Error: \n",
            " Accuracy: 56.8%, Avg loss: 0.061172 \n",
            "\n",
            "{'blues': {'precision': 0.38271604938271603, 'recall': 0.31, 'f1-score': 0.3425414364640884, 'support': 200}, 'classical': {'precision': 0.7528735632183908, 'recall': 0.655, 'f1-score': 0.7005347593582888, 'support': 200}, 'hiphop': {'precision': 0.8421052631578947, 'recall': 0.56, 'f1-score': 0.6726726726726727, 'support': 200}, 'rock_metal_hardrock': {'precision': 0.4501510574018127, 'recall': 0.745, 'f1-score': 0.5612052730696799, 'support': 200}, 'accuracy': 0.5675, 'macro avg': {'precision': 0.6069614832902036, 'recall': 0.5675, 'f1-score': 0.5692385353911824, 'support': 800}, 'weighted avg': {'precision': 0.6069614832902035, 'recall': 0.5675, 'f1-score': 0.5692385353911825, 'support': 800}}\n",
            "New best model with F1 score: 0.5692385353911825\n",
            "Epoch 4 with LeakyReLU\n",
            "-------------------------------\n",
            "loss: 1.190461  [    0/ 3200]\n",
            "loss: 0.536437  [ 1600/ 3200]\n",
            "Epoch [4/10], Train Loss: 0.7803315813839435\n",
            "Test Error: \n",
            " Accuracy: 63.2%, Avg loss: 0.053530 \n",
            "\n",
            "{'blues': {'precision': 0.43670886075949367, 'recall': 0.345, 'f1-score': 0.3854748603351955, 'support': 200}, 'classical': {'precision': 0.8535031847133758, 'recall': 0.67, 'f1-score': 0.7507002801120448, 'support': 200}, 'hiphop': {'precision': 0.8087431693989071, 'recall': 0.74, 'f1-score': 0.7728459530026109, 'support': 200}, 'rock_metal_hardrock': {'precision': 0.5132450331125827, 'recall': 0.775, 'f1-score': 0.6175298804780877, 'support': 200}, 'accuracy': 0.6325, 'macro avg': {'precision': 0.6530500619960898, 'recall': 0.6325000000000001, 'f1-score': 0.6316377434819846, 'support': 800}, 'weighted avg': {'precision': 0.65305006199609, 'recall': 0.6325, 'f1-score': 0.6316377434819848, 'support': 800}}\n",
            "New best model with F1 score: 0.6316377434819848\n",
            "Epoch 5 with LeakyReLU\n",
            "-------------------------------\n",
            "loss: 0.448291  [    0/ 3200]\n",
            "loss: 0.785851  [ 1600/ 3200]\n",
            "Epoch [5/10], Train Loss: 0.7312701858580113\n",
            "Test Error: \n",
            " Accuracy: 62.5%, Avg loss: 0.057720 \n",
            "\n",
            "{'blues': {'precision': 0.4369369369369369, 'recall': 0.485, 'f1-score': 0.4597156398104265, 'support': 200}, 'classical': {'precision': 0.8768115942028986, 'recall': 0.605, 'f1-score': 0.7159763313609467, 'support': 200}, 'hiphop': {'precision': 0.5972696245733788, 'recall': 0.875, 'f1-score': 0.7099391480730224, 'support': 200}, 'rock_metal_hardrock': {'precision': 0.7278911564625851, 'recall': 0.535, 'f1-score': 0.6167146974063401, 'support': 200}, 'accuracy': 0.625, 'macro avg': {'precision': 0.6597273280439498, 'recall': 0.625, 'f1-score': 0.6255864541626839, 'support': 800}, 'weighted avg': {'precision': 0.6597273280439498, 'recall': 0.625, 'f1-score': 0.6255864541626839, 'support': 800}}\n",
            "Epoch 6 with LeakyReLU\n",
            "-------------------------------\n",
            "loss: 0.838703  [    0/ 3200]\n",
            "loss: 0.630245  [ 1600/ 3200]\n",
            "Epoch [6/10], Train Loss: 0.652514453753829\n",
            "Test Error: \n",
            " Accuracy: 68.8%, Avg loss: 0.051015 \n",
            "\n",
            "{'blues': {'precision': 0.5161290322580645, 'recall': 0.48, 'f1-score': 0.4974093264248704, 'support': 200}, 'classical': {'precision': 0.828125, 'recall': 0.795, 'f1-score': 0.8112244897959185, 'support': 200}, 'hiphop': {'precision': 0.8411764705882353, 'recall': 0.715, 'f1-score': 0.772972972972973, 'support': 200}, 'rock_metal_hardrock': {'precision': 0.6031746031746031, 'recall': 0.76, 'f1-score': 0.672566371681416, 'support': 200}, 'accuracy': 0.6875, 'macro avg': {'precision': 0.6971512765052258, 'recall': 0.6875, 'f1-score': 0.6885432902187945, 'support': 800}, 'weighted avg': {'precision': 0.6971512765052258, 'recall': 0.6875, 'f1-score': 0.6885432902187943, 'support': 800}}\n",
            "New best model with F1 score: 0.6885432902187943\n",
            "Epoch 7 with LeakyReLU\n",
            "-------------------------------\n",
            "loss: 0.532191  [    0/ 3200]\n",
            "loss: 0.585326  [ 1600/ 3200]\n",
            "Epoch [7/10], Train Loss: 0.5751295521110297\n",
            "Test Error: \n",
            " Accuracy: 71.2%, Avg loss: 0.044741 \n",
            "\n",
            "{'blues': {'precision': 0.5625, 'recall': 0.54, 'f1-score': 0.5510204081632654, 'support': 200}, 'classical': {'precision': 0.9053254437869822, 'recall': 0.765, 'f1-score': 0.8292682926829268, 'support': 200}, 'hiphop': {'precision': 0.7661691542288557, 'recall': 0.77, 'f1-score': 0.7680798004987532, 'support': 200}, 'rock_metal_hardrock': {'precision': 0.6512605042016807, 'recall': 0.775, 'f1-score': 0.7077625570776255, 'support': 200}, 'accuracy': 0.7125, 'macro avg': {'precision': 0.7213137755543796, 'recall': 0.7125, 'f1-score': 0.7140327646056428, 'support': 800}, 'weighted avg': {'precision': 0.7213137755543797, 'recall': 0.7125, 'f1-score': 0.7140327646056426, 'support': 800}}\n",
            "New best model with F1 score: 0.7140327646056426\n",
            "Epoch 8 with LeakyReLU\n",
            "-------------------------------\n",
            "loss: 0.362675  [    0/ 3200]\n",
            "loss: 0.836116  [ 1600/ 3200]\n",
            "Epoch [8/10], Train Loss: 0.494439114369452\n",
            "Test Error: \n",
            " Accuracy: 72.9%, Avg loss: 0.046452 \n",
            "\n",
            "{'blues': {'precision': 0.5745614035087719, 'recall': 0.655, 'f1-score': 0.6121495327102803, 'support': 200}, 'classical': {'precision': 0.8870056497175142, 'recall': 0.785, 'f1-score': 0.8328912466843502, 'support': 200}, 'hiphop': {'precision': 0.7465437788018433, 'recall': 0.81, 'f1-score': 0.776978417266187, 'support': 200}, 'rock_metal_hardrock': {'precision': 0.7471910112359551, 'recall': 0.665, 'f1-score': 0.7037037037037037, 'support': 200}, 'accuracy': 0.72875, 'macro avg': {'precision': 0.7388254608160212, 'recall': 0.72875, 'f1-score': 0.7314307250911303, 'support': 800}, 'weighted avg': {'precision': 0.7388254608160212, 'recall': 0.72875, 'f1-score': 0.7314307250911304, 'support': 800}}\n",
            "New best model with F1 score: 0.7314307250911304\n",
            "Epoch 9 with LeakyReLU\n",
            "-------------------------------\n",
            "loss: 0.463773  [    0/ 3200]\n",
            "loss: 0.890846  [ 1600/ 3200]\n",
            "Epoch [9/10], Train Loss: 0.464467436671257\n",
            "Test Error: \n",
            " Accuracy: 73.6%, Avg loss: 0.046018 \n",
            "\n",
            "{'blues': {'precision': 0.6375, 'recall': 0.51, 'f1-score': 0.5666666666666667, 'support': 200}, 'classical': {'precision': 0.7805907172995781, 'recall': 0.925, 'f1-score': 0.8466819221967964, 'support': 200}, 'hiphop': {'precision': 0.803030303030303, 'recall': 0.795, 'f1-score': 0.7989949748743719, 'support': 200}, 'rock_metal_hardrock': {'precision': 0.697560975609756, 'recall': 0.715, 'f1-score': 0.7061728395061727, 'support': 200}, 'accuracy': 0.73625, 'macro avg': {'precision': 0.7296704989849092, 'recall': 0.73625, 'f1-score': 0.7296291008110019, 'support': 800}, 'weighted avg': {'precision': 0.7296704989849093, 'recall': 0.73625, 'f1-score': 0.7296291008110019, 'support': 800}}\n",
            "Epoch 10 with LeakyReLU\n",
            "-------------------------------\n",
            "loss: 0.388926  [    0/ 3200]\n",
            "loss: 0.312333  [ 1600/ 3200]\n",
            "Epoch [10/10], Train Loss: 0.388368223272264\n",
            "Test Error: \n",
            " Accuracy: 71.6%, Avg loss: 0.050548 \n",
            "\n",
            "{'blues': {'precision': 0.5566037735849056, 'recall': 0.59, 'f1-score': 0.5728155339805824, 'support': 200}, 'classical': {'precision': 0.9294117647058824, 'recall': 0.79, 'f1-score': 0.8540540540540541, 'support': 200}, 'hiphop': {'precision': 0.6679245283018868, 'recall': 0.885, 'f1-score': 0.7612903225806452, 'support': 200}, 'rock_metal_hardrock': {'precision': 0.7843137254901961, 'recall': 0.6, 'f1-score': 0.6798866855524079, 'support': 200}, 'accuracy': 0.71625, 'macro avg': {'precision': 0.7345634480207177, 'recall': 0.7162499999999999, 'f1-score': 0.7170116490419224, 'support': 800}, 'weighted avg': {'precision': 0.7345634480207177, 'recall': 0.71625, 'f1-score': 0.7170116490419224, 'support': 800}}\n",
            "Loading the best model for LeakyReLU and evaluating on the test set.\n",
            "Linear output: 1024\n",
            "Test Error: \n",
            " Accuracy: 68.6%, Avg loss: 0.046863 \n",
            "\n",
            "{'blues': {'precision': 0.4805194805194805, 'recall': 0.5709876543209876, 'f1-score': 0.5218617771509168, 'support': 324}, 'classical': {'precision': 0.8622950819672132, 'recall': 0.8855218855218855, 'f1-score': 0.8737541528239203, 'support': 297}, 'hiphop': {'precision': 0.7945205479452054, 'recall': 0.8146067415730337, 'f1-score': 0.8044382801664355, 'support': 356}, 'rock_metal_hardrock': {'precision': 0.6417445482866043, 'recall': 0.5162907268170426, 'f1-score': 0.5722222222222222, 'support': 399}, 'accuracy': 0.686046511627907, 'macro avg': {'precision': 0.6947699146796258, 'recall': 0.6968517520582374, 'f1-score': 0.6930691080908737, 'support': 1376}, 'weighted avg': {'precision': 0.6909123116769056, 'recall': 0.686046511627907, 'f1-score': 0.6855268122031388, 'support': 1376}}\n",
            "Final Test - LeakyReLU: Accuracy: 0.686046511627907, F1: 0.6855268122031388\n",
            "  Activation Function  Accuracy  F1 Score\n",
            "0                ReLU  0.717297  0.711790\n",
            "1             Sigmoid  0.258721  0.106356\n",
            "2                Tanh  0.673692  0.674647\n",
            "3           LeakyReLU  0.686047  0.685527\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generally in deep learning problems everyone seems to consider using ReLU as it has the ability to bypass the 'vanishing gradient problem' which occurs when gradients become very small leading to slow learning. In contrast, Sigmoid suffers to the aforementioned problem and its clear when observing the low accuracy and f1 score.\\\n",
        "Embedding a non-linear activation function into our network, we achieve non-liniearity, learning complexing patterns and associations in the data."
      ],
      "metadata": {
        "id": "YTfQ54i-WAGV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3 Learning Rate Scheduler"
      ],
      "metadata": {
        "id": "ljk1hUGMWBX6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "\n",
        "def train_and_evaluate_scheduler(train_loader, val_loader, test_loader, model_class, loss_fn, schedulers, num_classes, num_epochs=10, lr=LR):\n",
        "    results = {'Scheduler': [], 'Accuracy': [], 'F1 Score': []}\n",
        "\n",
        "    for scheduler_name, scheduler_class in schedulers.items():\n",
        "        print(f\"Training with {scheduler_name} Scheduler...\")\n",
        "        set_seed()\n",
        "        model = model_class(num_classes, nn.ReLU())\n",
        "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "        if scheduler_name == 'CosineAnnealingLR':\n",
        "            scheduler = scheduler_class(optimizer, T_max=num_epochs)\n",
        "        elif scheduler_name == 'ExponentialLR':\n",
        "            scheduler = scheduler_class(optimizer, gamma=0.9)\n",
        "        elif scheduler_name == 'StepLR':\n",
        "            scheduler = scheduler_class(optimizer, step_size=5, gamma=0.1)\n",
        "\n",
        "        best_f1_score = 0.0\n",
        "        best_model_state = None\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            print(f\"Epoch {epoch+1} with {scheduler_name}\\n-------------------------------\")\n",
        "            train_loss = train_loop(train_loader, model, loss_fn, optimizer, device=True)\n",
        "            print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss}')\n",
        "\n",
        "            _, _, val_f1_score, _ = test_loop(val_loader, model, loss_fn, device=True)\n",
        "            print(f\"Learning rate after epoch {epoch+1}: {scheduler.get_last_lr()}\")\n",
        "\n",
        "            if val_f1_score > best_f1_score:\n",
        "                best_f1_score = val_f1_score\n",
        "                best_model_state = model.state_dict()\n",
        "                print(f\"New best model with F1 score: {best_f1_score}\")\n",
        "                torch.save(best_model_state, f\"best_model_{scheduler_name}.pth\")\n",
        "\n",
        "            scheduler.step()\n",
        "\n",
        "        print(f\"Loading the best model for {scheduler_name} and evaluating on the test set.\")\n",
        "        best_model = model_class(num_classes, nn.ReLU())\n",
        "        best_model.load_state_dict(torch.load(f\"best_model_{scheduler_name}.pth\"))\n",
        "\n",
        "        _, test_acc, test_f1_score, _ = test_loop(test_loader, best_model, loss_fn, device=True)\n",
        "        print(f\"Final Test - {scheduler_name}: Accuracy: {test_acc:.2f}, F1: {test_f1_score:.4f}\")\n",
        "\n",
        "        results['Scheduler'].append(scheduler_name)\n",
        "        results['Accuracy'].append(test_acc)\n",
        "        results['F1 Score'].append(test_f1_score)\n",
        "\n",
        "    return results\n",
        "\n",
        "schedulers = {\n",
        "    'StepLR': optim.lr_scheduler.StepLR,\n",
        "    'CosineAnnealingLR': torch.optim.lr_scheduler.CosineAnnealingLR,\n",
        "    'ExponentialLR': optim.lr_scheduler.ExponentialLR\n",
        "}\n",
        "\n",
        "results_scheduler = train_and_evaluate_scheduler(train_loader_melgrams, val_loader_melgrams, test_loader_melgrams, MusicGenreCNN_Pooling_Activated, loss_fn, schedulers, num_classes)\n",
        "\n",
        "results_scheduler_df = pd.DataFrame(results_scheduler)\n",
        "\n",
        "print(results_scheduler_df)\n"
      ],
      "metadata": {
        "id": "o-bWJopqWAXl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d20edc6-bcc4-4322-8b42-6e9394b1ec58"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with StepLR Scheduler...\n",
            "Linear output: 1024\n",
            "Epoch 1 with StepLR\n",
            "-------------------------------\n",
            "loss: 1.444518  [    0/ 3200]\n",
            "loss: 1.488093  [ 1600/ 3200]\n",
            "Epoch [1/10], Train Loss: 1.2934224957227707\n",
            "Test Error: \n",
            " Accuracy: 0.48%, Avg loss: 1.0843, F1 Score: 0.3843 \n",
            "\n",
            "Learning rate after epoch 1: [0.001]\n",
            "New best model with F1 score: 0.3843016126546262\n",
            "Epoch 2 with StepLR\n",
            "-------------------------------\n",
            "loss: 0.983883  [    0/ 3200]\n",
            "loss: 0.910355  [ 1600/ 3200]\n",
            "Epoch [2/10], Train Loss: 1.0034286820888518\n",
            "Test Error: \n",
            " Accuracy: 0.53%, Avg loss: 0.9786, F1 Score: 0.4996 \n",
            "\n",
            "Learning rate after epoch 2: [0.001]\n",
            "New best model with F1 score: 0.49960135861271326\n",
            "Epoch 3 with StepLR\n",
            "-------------------------------\n",
            "loss: 0.947864  [    0/ 3200]\n",
            "loss: 0.998168  [ 1600/ 3200]\n",
            "Epoch [3/10], Train Loss: 0.8449421358108521\n",
            "Test Error: \n",
            " Accuracy: 0.54%, Avg loss: 1.0355, F1 Score: 0.5008 \n",
            "\n",
            "Learning rate after epoch 3: [0.001]\n",
            "New best model with F1 score: 0.5007785333764775\n",
            "Epoch 4 with StepLR\n",
            "-------------------------------\n",
            "loss: 1.132704  [    0/ 3200]\n",
            "loss: 0.985505  [ 1600/ 3200]\n",
            "Epoch [4/10], Train Loss: 0.7457002173364162\n",
            "Test Error: \n",
            " Accuracy: 0.69%, Avg loss: 0.8204, F1 Score: 0.6810 \n",
            "\n",
            "Learning rate after epoch 4: [0.001]\n",
            "New best model with F1 score: 0.6809755027628891\n",
            "Epoch 5 with StepLR\n",
            "-------------------------------\n",
            "loss: 0.355025  [    0/ 3200]\n",
            "loss: 0.718302  [ 1600/ 3200]\n",
            "Epoch [5/10], Train Loss: 0.6294380864500999\n",
            "Test Error: \n",
            " Accuracy: 0.69%, Avg loss: 0.8162, F1 Score: 0.6677 \n",
            "\n",
            "Learning rate after epoch 5: [0.001]\n",
            "Epoch 6 with StepLR\n",
            "-------------------------------\n",
            "loss: 0.758158  [    0/ 3200]\n",
            "loss: 0.576244  [ 1600/ 3200]\n",
            "Epoch [6/10], Train Loss: 0.4910033220797777\n",
            "Test Error: \n",
            " Accuracy: 0.73%, Avg loss: 0.7244, F1 Score: 0.7354 \n",
            "\n",
            "Learning rate after epoch 6: [0.0001]\n",
            "New best model with F1 score: 0.7354101140577606\n",
            "Epoch 7 with StepLR\n",
            "-------------------------------\n",
            "loss: 0.352641  [    0/ 3200]\n",
            "loss: 0.253769  [ 1600/ 3200]\n",
            "Epoch [7/10], Train Loss: 0.4350572695210576\n",
            "Test Error: \n",
            " Accuracy: 0.75%, Avg loss: 0.7225, F1 Score: 0.7500 \n",
            "\n",
            "Learning rate after epoch 7: [0.0001]\n",
            "New best model with F1 score: 0.7500341899055002\n",
            "Epoch 8 with StepLR\n",
            "-------------------------------\n",
            "loss: 0.456814  [    0/ 3200]\n",
            "loss: 0.594964  [ 1600/ 3200]\n",
            "Epoch [8/10], Train Loss: 0.4051344408839941\n",
            "Test Error: \n",
            " Accuracy: 0.75%, Avg loss: 0.7054, F1 Score: 0.7514 \n",
            "\n",
            "Learning rate after epoch 8: [0.0001]\n",
            "New best model with F1 score: 0.7514070599571216\n",
            "Epoch 9 with StepLR\n",
            "-------------------------------\n",
            "loss: 0.509527  [    0/ 3200]\n",
            "loss: 0.415725  [ 1600/ 3200]\n",
            "Epoch [9/10], Train Loss: 0.38432516023516655\n",
            "Test Error: \n",
            " Accuracy: 0.74%, Avg loss: 0.7498, F1 Score: 0.7475 \n",
            "\n",
            "Learning rate after epoch 9: [0.0001]\n",
            "Epoch 10 with StepLR\n",
            "-------------------------------\n",
            "loss: 0.235885  [    0/ 3200]\n",
            "loss: 0.090720  [ 1600/ 3200]\n",
            "Epoch [10/10], Train Loss: 0.35658370204269885\n",
            "Test Error: \n",
            " Accuracy: 0.76%, Avg loss: 0.7157, F1 Score: 0.7570 \n",
            "\n",
            "Learning rate after epoch 10: [0.0001]\n",
            "New best model with F1 score: 0.7570323231549753\n",
            "Loading the best model for StepLR and evaluating on the test set.\n",
            "Linear output: 1024\n",
            "Test Error: \n",
            " Accuracy: 0.73%, Avg loss: 0.7550, F1 Score: 0.7280 \n",
            "\n",
            "Final Test - StepLR: Accuracy: 0.73, F1: 0.7280\n",
            "Training with CosineAnnealingLR Scheduler...\n",
            "Linear output: 1024\n",
            "Epoch 1 with CosineAnnealingLR\n",
            "-------------------------------\n",
            "loss: 1.444518  [    0/ 3200]\n",
            "loss: 1.488093  [ 1600/ 3200]\n",
            "Epoch [1/10], Train Loss: 1.2934224957227707\n",
            "Test Error: \n",
            " Accuracy: 0.48%, Avg loss: 1.0843, F1 Score: 0.3843 \n",
            "\n",
            "Learning rate after epoch 1: [0.001]\n",
            "New best model with F1 score: 0.3843016126546262\n",
            "Epoch 2 with CosineAnnealingLR\n",
            "-------------------------------\n",
            "loss: 0.983883  [    0/ 3200]\n",
            "loss: 0.805135  [ 1600/ 3200]\n",
            "Epoch [2/10], Train Loss: 0.9979024523496628\n",
            "Test Error: \n",
            " Accuracy: 0.58%, Avg loss: 0.9322, F1 Score: 0.5544 \n",
            "\n",
            "Learning rate after epoch 2: [0.0009755282581475768]\n",
            "New best model with F1 score: 0.5544319137839931\n",
            "Epoch 3 with CosineAnnealingLR\n",
            "-------------------------------\n",
            "loss: 0.945876  [    0/ 3200]\n",
            "loss: 0.940657  [ 1600/ 3200]\n",
            "Epoch [3/10], Train Loss: 0.8217808908224106\n",
            "Test Error: \n",
            " Accuracy: 0.59%, Avg loss: 1.0354, F1 Score: 0.5749 \n",
            "\n",
            "Learning rate after epoch 3: [0.0009045084971874736]\n",
            "New best model with F1 score: 0.574861531901984\n",
            "Epoch 4 with CosineAnnealingLR\n",
            "-------------------------------\n",
            "loss: 1.116526  [    0/ 3200]\n",
            "loss: 0.548235  [ 1600/ 3200]\n",
            "Epoch [4/10], Train Loss: 0.6860296250134706\n",
            "Test Error: \n",
            " Accuracy: 0.72%, Avg loss: 0.7510, F1 Score: 0.7002 \n",
            "\n",
            "Learning rate after epoch 4: [0.0007938926261462366]\n",
            "New best model with F1 score: 0.7001559008370183\n",
            "Epoch 5 with CosineAnnealingLR\n",
            "-------------------------------\n",
            "loss: 0.356417  [    0/ 3200]\n",
            "loss: 0.510925  [ 1600/ 3200]\n",
            "Epoch [5/10], Train Loss: 0.5854215189814568\n",
            "Test Error: \n",
            " Accuracy: 0.70%, Avg loss: 0.7819, F1 Score: 0.6962 \n",
            "\n",
            "Learning rate after epoch 5: [0.0006545084971874737]\n",
            "Epoch 6 with CosineAnnealingLR\n",
            "-------------------------------\n",
            "loss: 0.767453  [    0/ 3200]\n",
            "loss: 0.671320  [ 1600/ 3200]\n",
            "Epoch [6/10], Train Loss: 0.507887604534626\n",
            "Test Error: \n",
            " Accuracy: 0.73%, Avg loss: 0.7227, F1 Score: 0.7323 \n",
            "\n",
            "Learning rate after epoch 6: [0.0005]\n",
            "New best model with F1 score: 0.7322830978347309\n",
            "Epoch 7 with CosineAnnealingLR\n",
            "-------------------------------\n",
            "loss: 0.352018  [    0/ 3200]\n",
            "loss: 0.182104  [ 1600/ 3200]\n",
            "Epoch [7/10], Train Loss: 0.4269147830829024\n",
            "Test Error: \n",
            " Accuracy: 0.77%, Avg loss: 0.6588, F1 Score: 0.7625 \n",
            "\n",
            "Learning rate after epoch 7: [0.00034549150281252633]\n",
            "New best model with F1 score: 0.7625255712983057\n",
            "Epoch 8 with CosineAnnealingLR\n",
            "-------------------------------\n",
            "loss: 0.432211  [    0/ 3200]\n",
            "loss: 0.442580  [ 1600/ 3200]\n",
            "Epoch [8/10], Train Loss: 0.34219557251781224\n",
            "Test Error: \n",
            " Accuracy: 0.76%, Avg loss: 0.7643, F1 Score: 0.7479 \n",
            "\n",
            "Learning rate after epoch 8: [0.0002061073738537635]\n",
            "Epoch 9 with CosineAnnealingLR\n",
            "-------------------------------\n",
            "loss: 0.639276  [    0/ 3200]\n",
            "loss: 0.205649  [ 1600/ 3200]\n",
            "Epoch [9/10], Train Loss: 0.2928420988097787\n",
            "Test Error: \n",
            " Accuracy: 0.77%, Avg loss: 0.7099, F1 Score: 0.7716 \n",
            "\n",
            "Learning rate after epoch 9: [9.549150281252634e-05]\n",
            "New best model with F1 score: 0.7715661928615506\n",
            "Epoch 10 with CosineAnnealingLR\n",
            "-------------------------------\n",
            "loss: 0.105972  [    0/ 3200]\n",
            "loss: 0.084858  [ 1600/ 3200]\n",
            "Epoch [10/10], Train Loss: 0.25328724190592766\n",
            "Test Error: \n",
            " Accuracy: 0.79%, Avg loss: 0.7113, F1 Score: 0.7828 \n",
            "\n",
            "Learning rate after epoch 10: [2.4471741852423235e-05]\n",
            "New best model with F1 score: 0.7828078752833146\n",
            "Loading the best model for CosineAnnealingLR and evaluating on the test set.\n",
            "Linear output: 1024\n",
            "Test Error: \n",
            " Accuracy: 0.73%, Avg loss: 0.8159, F1 Score: 0.7225 \n",
            "\n",
            "Final Test - CosineAnnealingLR: Accuracy: 0.73, F1: 0.7225\n",
            "Training with ExponentialLR Scheduler...\n",
            "Linear output: 1024\n",
            "Epoch 1 with ExponentialLR\n",
            "-------------------------------\n",
            "loss: 1.444518  [    0/ 3200]\n",
            "loss: 1.488093  [ 1600/ 3200]\n",
            "Epoch [1/10], Train Loss: 1.2934224957227707\n",
            "Test Error: \n",
            " Accuracy: 0.48%, Avg loss: 1.0843, F1 Score: 0.3843 \n",
            "\n",
            "Learning rate after epoch 1: [0.001]\n",
            "New best model with F1 score: 0.3843016126546262\n",
            "Epoch 2 with ExponentialLR\n",
            "-------------------------------\n",
            "loss: 0.983883  [    0/ 3200]\n",
            "loss: 0.900669  [ 1600/ 3200]\n",
            "Epoch [2/10], Train Loss: 1.0048025080561638\n",
            "Test Error: \n",
            " Accuracy: 0.49%, Avg loss: 1.0319, F1 Score: 0.4657 \n",
            "\n",
            "Learning rate after epoch 2: [0.0009000000000000001]\n",
            "New best model with F1 score: 0.46565801857813094\n",
            "Epoch 3 with ExponentialLR\n",
            "-------------------------------\n",
            "loss: 0.855608  [    0/ 3200]\n",
            "loss: 0.900243  [ 1600/ 3200]\n",
            "Epoch [3/10], Train Loss: 0.8426671634614468\n",
            "Test Error: \n",
            " Accuracy: 0.54%, Avg loss: 1.0340, F1 Score: 0.5002 \n",
            "\n",
            "Learning rate after epoch 3: [0.0008100000000000001]\n",
            "New best model with F1 score: 0.5002101232349063\n",
            "Epoch 4 with ExponentialLR\n",
            "-------------------------------\n",
            "loss: 1.204274  [    0/ 3200]\n",
            "loss: 0.757499  [ 1600/ 3200]\n",
            "Epoch [4/10], Train Loss: 0.7252933950722218\n",
            "Test Error: \n",
            " Accuracy: 0.71%, Avg loss: 0.7438, F1 Score: 0.7074 \n",
            "\n",
            "Learning rate after epoch 4: [0.000729]\n",
            "New best model with F1 score: 0.7073560363294834\n",
            "Epoch 5 with ExponentialLR\n",
            "-------------------------------\n",
            "loss: 0.430042  [    0/ 3200]\n",
            "loss: 0.538773  [ 1600/ 3200]\n",
            "Epoch [5/10], Train Loss: 0.5955793495476246\n",
            "Test Error: \n",
            " Accuracy: 0.70%, Avg loss: 0.8012, F1 Score: 0.6909 \n",
            "\n",
            "Learning rate after epoch 5: [0.0006561000000000001]\n",
            "Epoch 6 with ExponentialLR\n",
            "-------------------------------\n",
            "loss: 0.758743  [    0/ 3200]\n",
            "loss: 0.848084  [ 1600/ 3200]\n",
            "Epoch [6/10], Train Loss: 0.5380722495913506\n",
            "Test Error: \n",
            " Accuracy: 0.70%, Avg loss: 0.8261, F1 Score: 0.6914 \n",
            "\n",
            "Learning rate after epoch 6: [0.00059049]\n",
            "Epoch 7 with ExponentialLR\n",
            "-------------------------------\n",
            "loss: 0.560167  [    0/ 3200]\n",
            "loss: 0.268510  [ 1600/ 3200]\n",
            "Epoch [7/10], Train Loss: 0.4776585175842047\n",
            "Test Error: \n",
            " Accuracy: 0.73%, Avg loss: 0.7673, F1 Score: 0.7206 \n",
            "\n",
            "Learning rate after epoch 7: [0.000531441]\n",
            "New best model with F1 score: 0.7205720233078635\n",
            "Epoch 8 with ExponentialLR\n",
            "-------------------------------\n",
            "loss: 0.501687  [    0/ 3200]\n",
            "loss: 0.498110  [ 1600/ 3200]\n",
            "Epoch [8/10], Train Loss: 0.4122551053017378\n",
            "Test Error: \n",
            " Accuracy: 0.69%, Avg loss: 0.8241, F1 Score: 0.6935 \n",
            "\n",
            "Learning rate after epoch 8: [0.0004782969]\n",
            "Epoch 9 with ExponentialLR\n",
            "-------------------------------\n",
            "loss: 0.419908  [    0/ 3200]\n",
            "loss: 0.270992  [ 1600/ 3200]\n",
            "Epoch [9/10], Train Loss: 0.3491708503291011\n",
            "Test Error: \n",
            " Accuracy: 0.73%, Avg loss: 0.7425, F1 Score: 0.7267 \n",
            "\n",
            "Learning rate after epoch 9: [0.00043046721]\n",
            "New best model with F1 score: 0.7266832254175121\n",
            "Epoch 10 with ExponentialLR\n",
            "-------------------------------\n",
            "loss: 0.174360  [    0/ 3200]\n",
            "loss: 0.182487  [ 1600/ 3200]\n",
            "Epoch [10/10], Train Loss: 0.28651674321852627\n",
            "Test Error: \n",
            " Accuracy: 0.70%, Avg loss: 0.9237, F1 Score: 0.7123 \n",
            "\n",
            "Learning rate after epoch 10: [0.000387420489]\n",
            "Loading the best model for ExponentialLR and evaluating on the test set.\n",
            "Linear output: 1024\n",
            "Test Error: \n",
            " Accuracy: 0.72%, Avg loss: 0.7503, F1 Score: 0.7222 \n",
            "\n",
            "Final Test - ExponentialLR: Accuracy: 0.72, F1: 0.7222\n",
            "           Scheduler  Accuracy  F1 Score\n",
            "0             StepLR  0.729651  0.728032\n",
            "1  CosineAnnealingLR  0.725291  0.722511\n",
            "2      ExponentialLR  0.723110  0.722249\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since verbose=True has been deprecated, i used scheduler.get_last_lr() to obtain the learning rate used at every epoch to observe the changes.\\\n",
        "StepLR lowers the learning rate by 10^{-1} every 5 epochs and works best, as it allows a period of stable learning before a jump to a lower learning rate.\\\n",
        "CosineAnnealingLR gradually reduces the rate following a \"cosine decay\" schedule, starting from the static 1e-3 rate and leading to a zero at the end of the training. This achieves smallers jumps, for a more smooth scheduling of the learning rate. \\\n",
        "ExponentialLR reduces the learning rate by a constant factor after every epoch. The exponential decrease mean that the jumps become progressively smaller. This method seems to achieve the worst accuracy and f1 score of the 3."
      ],
      "metadata": {
        "id": "uI77upekWDE9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4 Batch Normalization"
      ],
      "metadata": {
        "id": "t7Wg_O-SWDxX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MusicGenreCNN_Batching(nn.Module):\n",
        "    def __init__(self, out_dim, activation_fn):\n",
        "        super(MusicGenreCNN_Batching, self).__init__()\n",
        "        self.activation_fn = activation_fn\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            nn.Conv2d(1, 16, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm2d(16),\n",
        "            activation_fn,\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(16, 32, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm2d(32),\n",
        "            activation_fn,\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(32, 64, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm2d(64),\n",
        "            activation_fn,\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(64, 128, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm2d(128),\n",
        "            activation_fn,\n",
        "            nn.MaxPool2d(2, 2)\n",
        "        )\n",
        "\n",
        "        self._to_linear = None\n",
        "        self.calculate_linear_output((1, 21, 128))\n",
        "        print(f'Linear output: {self._to_linear}')\n",
        "\n",
        "        self.fc_layers = nn.Sequential(\n",
        "            nn.Linear(self._to_linear, 1024),\n",
        "            nn.BatchNorm1d(1024),\n",
        "            activation_fn,\n",
        "            nn.Linear(1024, 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            activation_fn,\n",
        "            nn.Linear(256, 32),\n",
        "            nn.BatchNorm1d(32),\n",
        "            activation_fn,\n",
        "            nn.Linear(32, out_dim)\n",
        "        )\n",
        "\n",
        "    def calculate_linear_output(self, shape):\n",
        "        output = torch.ones(1, *shape)\n",
        "        output = self.conv_layers(output)\n",
        "        self._to_linear = output.numel()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_layers(x)\n",
        "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
        "        x = self.fc_layers(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "uIT4P3zFWFIP"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_evaluate_batchnorm(train_loader, val_loader, test_loader, model_class, loss_fn, num_classes, num_epochs=10, lr=LR):\n",
        "    results = {'Accuracy': [], 'F1 Score': []}\n",
        "\n",
        "    set_seed()\n",
        "    model = model_class(num_classes, nn.ReLU())\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "\n",
        "    best_f1_score = 0.0\n",
        "    best_model_state = None\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
        "        train_loss = train_loop(train_loader, model, loss_fn, optimizer, device=True)\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss}')\n",
        "        _, _, val_f1_score, _ = test_loop(val_loader, model, loss_fn, device=True)\n",
        "\n",
        "        if val_f1_score > best_f1_score:\n",
        "            best_f1_score = val_f1_score\n",
        "            best_model_state = model.state_dict()\n",
        "            print(f\"New best model with F1 score: {best_f1_score}\")\n",
        "            torch.save(best_model_state, 'best_model_batchnorm.pth')\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "    print(\"Loading the best model and evaluating on the test set.\")\n",
        "    best_model = model_class(num_classes, nn.ReLU())\n",
        "    best_model.load_state_dict(torch.load('best_model_batchnorm.pth'))\n",
        "\n",
        "    _, test_acc, test_f1_score, _ = test_loop(test_loader, best_model, loss_fn, device=True)\n",
        "    print(f\"Final Test: Accuracy: {test_acc:.2f}, F1: {test_f1_score:.4f}\")\n",
        "\n",
        "    results['Accuracy'].append(test_acc)\n",
        "    results['F1 Score'].append(test_f1_score)\n",
        "\n",
        "    return results\n",
        "\n",
        "results_batchnorm = train_and_evaluate_batchnorm(train_loader_melgrams, val_loader_melgrams, test_loader_melgrams, MusicGenreCNN_Batching, loss_fn, num_classes)\n",
        "\n",
        "results_batchnorm_df = pd.DataFrame(results_batchnorm)\n",
        "\n",
        "print(results_batchnorm_df)\n"
      ],
      "metadata": {
        "id": "W_MH5nsoWFy7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "945fbaef-4eca-443f-fc60-217a920af417"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear output: 1024\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 1.417952  [    0/ 3200]\n",
            "loss: 0.674079  [ 1600/ 3200]\n",
            "Epoch [1/10], Train Loss: 0.8153427337110043\n",
            "Test Error: \n",
            " Accuracy: 0.48%, Avg loss: 1.2402, F1 Score: 0.4335 \n",
            "\n",
            "New best model with F1 score: 0.4334780755329313\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 0.394230  [    0/ 3200]\n",
            "loss: 0.380016  [ 1600/ 3200]\n",
            "Epoch [2/10], Train Loss: 0.6556605882942677\n",
            "Test Error: \n",
            " Accuracy: 0.72%, Avg loss: 0.6862, F1 Score: 0.7157 \n",
            "\n",
            "New best model with F1 score: 0.7156910399536042\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 0.565311  [    0/ 3200]\n",
            "loss: 0.798776  [ 1600/ 3200]\n",
            "Epoch [3/10], Train Loss: 0.5716132097691298\n",
            "Test Error: \n",
            " Accuracy: 0.73%, Avg loss: 0.7041, F1 Score: 0.7244 \n",
            "\n",
            "New best model with F1 score: 0.7244000222031747\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 0.621386  [    0/ 3200]\n",
            "loss: 0.803572  [ 1600/ 3200]\n",
            "Epoch [4/10], Train Loss: 0.5118874900788069\n",
            "Test Error: \n",
            " Accuracy: 0.74%, Avg loss: 0.6746, F1 Score: 0.7396 \n",
            "\n",
            "New best model with F1 score: 0.7395927561093433\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 0.258545  [    0/ 3200]\n",
            "loss: 0.354426  [ 1600/ 3200]\n",
            "Epoch [5/10], Train Loss: 0.424554055146873\n",
            "Test Error: \n",
            " Accuracy: 0.74%, Avg loss: 0.6339, F1 Score: 0.7347 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 0.202877  [    0/ 3200]\n",
            "loss: 0.537550  [ 1600/ 3200]\n",
            "Epoch [6/10], Train Loss: 0.3505563412979245\n",
            "Test Error: \n",
            " Accuracy: 0.78%, Avg loss: 0.5681, F1 Score: 0.7771 \n",
            "\n",
            "New best model with F1 score: 0.7771244971172471\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 0.330320  [    0/ 3200]\n",
            "loss: 0.137956  [ 1600/ 3200]\n",
            "Epoch [7/10], Train Loss: 0.24550040800124406\n",
            "Test Error: \n",
            " Accuracy: 0.79%, Avg loss: 0.5530, F1 Score: 0.7853 \n",
            "\n",
            "New best model with F1 score: 0.7852565138281523\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 0.055000  [    0/ 3200]\n",
            "loss: 0.045970  [ 1600/ 3200]\n",
            "Epoch [8/10], Train Loss: 0.19247799392789602\n",
            "Test Error: \n",
            " Accuracy: 0.80%, Avg loss: 0.5436, F1 Score: 0.7997 \n",
            "\n",
            "New best model with F1 score: 0.7996820267869308\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 0.086584  [    0/ 3200]\n",
            "loss: 0.064880  [ 1600/ 3200]\n",
            "Epoch [9/10], Train Loss: 0.12775056555867195\n",
            "Test Error: \n",
            " Accuracy: 0.81%, Avg loss: 0.5263, F1 Score: 0.8140 \n",
            "\n",
            "New best model with F1 score: 0.8140058586984987\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 0.046939  [    0/ 3200]\n",
            "loss: 0.168843  [ 1600/ 3200]\n",
            "Epoch [10/10], Train Loss: 0.10419173894450068\n",
            "Test Error: \n",
            " Accuracy: 0.81%, Avg loss: 0.5320, F1 Score: 0.8089 \n",
            "\n",
            "Loading the best model and evaluating on the test set.\n",
            "Linear output: 1024\n",
            "Test Error: \n",
            " Accuracy: 0.79%, Avg loss: 0.6544, F1 Score: 0.7897 \n",
            "\n",
            "Final Test: Accuracy: 0.79, F1: 0.7897\n",
            "   Accuracy  F1 Score\n",
            "0  0.789244  0.789657\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A small detail here is that BatchNorm2d is needed at the convolution side of the network, as the input is 4D. However, at the Linear layers, BatchNorm1d was used as the data had been flattened."
      ],
      "metadata": {
        "id": "fWbms_0gWG6D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5 Regularization"
      ],
      "metadata": {
        "id": "_W6_aAwcWHV3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MusicGenreCNN_Dropout(nn.Module):\n",
        "    def __init__(self, out_dim, activation_fn, dropout=0.0):\n",
        "        super(MusicGenreCNN_Dropout, self).__init__()\n",
        "        self.activation_fn = activation_fn\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            nn.Conv2d(1, 16, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm2d(16),\n",
        "            activation_fn,\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(16, 32, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm2d(32),\n",
        "            activation_fn,\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(32, 64, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm2d(64),\n",
        "            activation_fn,\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(64, 128, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm2d(128),\n",
        "            activation_fn,\n",
        "            nn.MaxPool2d(2, 2)\n",
        "        )\n",
        "\n",
        "        self._to_linear = None\n",
        "        self.calculate_linear_output((1, 21, 128))\n",
        "        print(f'Linear output: {self._to_linear}')\n",
        "\n",
        "        self.fc_layers = nn.Sequential(\n",
        "            nn.Linear(self._to_linear, 1024),\n",
        "            nn.BatchNorm1d(1024),\n",
        "            nn.Dropout(dropout),\n",
        "            activation_fn,\n",
        "            nn.Linear(1024, 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.Dropout(dropout),\n",
        "            activation_fn,\n",
        "            nn.Linear(256, 32),\n",
        "            nn.BatchNorm1d(32),\n",
        "            nn.Dropout(dropout),\n",
        "            activation_fn,\n",
        "            nn.Linear(32, out_dim)\n",
        "        )\n",
        "\n",
        "    def calculate_linear_output(self, shape):\n",
        "        output = torch.ones(1, *shape)\n",
        "        output = self.conv_layers(output)\n",
        "        self._to_linear = output.numel()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_layers(x)\n",
        "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
        "        x = self.fc_layers(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "-p_LnMqUWIHV"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_evaluate_regularized(train_loader, val_loader, test_loader, model_class, loss_fn, num_classes, num_epochs, lr=LR, weight_decay=0.0, dropout=0.0):\n",
        "    results = {'Accuracy': [], 'F1 Score': []}\n",
        "\n",
        "    # Initialize model, optimizer, scheduler\n",
        "    model = model_class(num_classes, nn.ReLU(), dropout)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "\n",
        "    best_f1_score = 0.0\n",
        "    best_model_state = None\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
        "        train_loss = train_loop(train_loader, model, loss_fn, optimizer, device=True)\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss}')\n",
        "        _, _, val_f1_score, _ = test_loop(val_loader, model, loss_fn, device=True)\n",
        "\n",
        "        if val_f1_score > best_f1_score:\n",
        "            best_f1_score = val_f1_score\n",
        "            best_model_state = model.state_dict()\n",
        "            print(f\"New best model with F1 score: {best_f1_score}\")\n",
        "            torch.save(best_model_state, 'best_model_regularized.pth')\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "    print(\"Loading the best model and evaluating on the test set.\")\n",
        "    best_model = model_class(num_classes, nn.ReLU(), dropout)\n",
        "    best_model.load_state_dict(torch.load('best_model_regularized.pth'))\n",
        "\n",
        "    _, test_acc, test_f1_score, _ = test_loop(test_loader, best_model, loss_fn, device=True)\n",
        "    print(f\"Final Test: Accuracy: {test_acc:.2f}, F1: {test_f1_score:.4f}\")\n",
        "\n",
        "    results['Accuracy'].append(test_acc)\n",
        "    results['F1 Score'].append(test_f1_score)\n",
        "\n",
        "    avg_accuracy = sum(results['Accuracy']) / len(results['Accuracy'])\n",
        "    avg_f1_score = sum(results['F1 Score']) / len(results['F1 Score'])\n",
        "\n",
        "    return {'Average Accuracy': avg_accuracy, 'Average F1 Score': avg_f1_score}\n",
        "\n",
        "configurations = [\n",
        "    {'epochs': 30, 'weight_decay': 0.0, 'dropout': 0.0},\n",
        "    {'epochs': 30, 'weight_decay': 1e-3, 'dropout': 0.0},\n",
        "    {'epochs': 30, 'weight_decay': 0.0, 'dropout': 0.2},\n",
        "    {'epochs': 30, 'weight_decay': 1e-3, 'dropout': 0.2},\n",
        "    {'epochs': 60, 'weight_decay': 0.0, 'dropout': 0.0},\n",
        "    {'epochs': 60, 'weight_decay': 1e-3, 'dropout': 0.0},\n",
        "    {'epochs': 60, 'weight_decay': 0.0, 'dropout': 0.2},\n",
        "    {'epochs': 60, 'weight_decay': 1e-3, 'dropout': 0.2},\n",
        "]\n",
        "\n",
        "all_results = []\n",
        "\n",
        "for config in configurations:\n",
        "    results = train_and_evaluate_regularized(train_loader_melgrams, val_loader_melgrams, test_loader_melgrams, MusicGenreCNN_Dropout,\n",
        "                                             loss_fn, num_classes, num_epochs=config['epochs'],\n",
        "                                             lr=LR, weight_decay=config['weight_decay'], dropout=config['dropout'])\n",
        "    results['Epochs'] = config['epochs']\n",
        "    results['Weight Decay'] = config['weight_decay']\n",
        "    results['Dropout'] = config['dropout']\n",
        "    all_results.append(results)\n",
        "\n",
        "results_regularized = pd.DataFrame(all_results)\n",
        "\n",
        "sorted_results = results_regularized.sort_values(by='Average F1 Score', ascending=False)\n",
        "\n",
        "print(sorted_results)\n"
      ],
      "metadata": {
        "id": "O4r16WFzWJe9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1b253b3-d61a-463f-b07d-7ffc61137874"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear output: 1024\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 1.589903  [    0/ 3200]\n",
            "loss: 0.916665  [ 1600/ 3200]\n",
            "Epoch [1/30], Train Loss: 0.7872544793784618\n",
            "Test Error: \n",
            " Accuracy: 0.68%, Avg loss: 0.7800, F1 Score: 0.6852 \n",
            "\n",
            "New best model with F1 score: 0.6851581797961154\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 1.143978  [    0/ 3200]\n",
            "loss: 0.481731  [ 1600/ 3200]\n",
            "Epoch [2/30], Train Loss: 0.6391179043054581\n",
            "Test Error: \n",
            " Accuracy: 0.63%, Avg loss: 0.9236, F1 Score: 0.5883 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 0.743412  [    0/ 3200]\n",
            "loss: 0.464451  [ 1600/ 3200]\n",
            "Epoch [3/30], Train Loss: 0.5769937452673912\n",
            "Test Error: \n",
            " Accuracy: 0.70%, Avg loss: 0.7038, F1 Score: 0.7037 \n",
            "\n",
            "New best model with F1 score: 0.703695204486602\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 0.237400  [    0/ 3200]\n",
            "loss: 0.623581  [ 1600/ 3200]\n",
            "Epoch [4/30], Train Loss: 0.5065891900658608\n",
            "Test Error: \n",
            " Accuracy: 0.68%, Avg loss: 0.8432, F1 Score: 0.6086 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 0.655418  [    0/ 3200]\n",
            "loss: 0.525443  [ 1600/ 3200]\n",
            "Epoch [5/30], Train Loss: 0.4293727084249258\n",
            "Test Error: \n",
            " Accuracy: 0.60%, Avg loss: 1.1038, F1 Score: 0.5923 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 0.620500  [    0/ 3200]\n",
            "loss: 0.253364  [ 1600/ 3200]\n",
            "Epoch [6/30], Train Loss: 0.39709524225443604\n",
            "Test Error: \n",
            " Accuracy: 0.78%, Avg loss: 0.5690, F1 Score: 0.7744 \n",
            "\n",
            "New best model with F1 score: 0.7743954937353216\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 0.482874  [    0/ 3200]\n",
            "loss: 0.606523  [ 1600/ 3200]\n",
            "Epoch [7/30], Train Loss: 0.3505168171040714\n",
            "Test Error: \n",
            " Accuracy: 0.81%, Avg loss: 0.5367, F1 Score: 0.8112 \n",
            "\n",
            "New best model with F1 score: 0.8112006881340205\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 0.519320  [    0/ 3200]\n",
            "loss: 0.596041  [ 1600/ 3200]\n",
            "Epoch [8/30], Train Loss: 0.3051711972244084\n",
            "Test Error: \n",
            " Accuracy: 0.71%, Avg loss: 0.8009, F1 Score: 0.6907 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 0.094881  [    0/ 3200]\n",
            "loss: 0.205089  [ 1600/ 3200]\n",
            "Epoch [9/30], Train Loss: 0.22664066566154362\n",
            "Test Error: \n",
            " Accuracy: 0.76%, Avg loss: 0.7024, F1 Score: 0.7529 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 0.079843  [    0/ 3200]\n",
            "loss: 0.183121  [ 1600/ 3200]\n",
            "Epoch [10/30], Train Loss: 0.18599581809714436\n",
            "Test Error: \n",
            " Accuracy: 0.78%, Avg loss: 0.6504, F1 Score: 0.7694 \n",
            "\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "loss: 0.023166  [    0/ 3200]\n",
            "loss: 0.125392  [ 1600/ 3200]\n",
            "Epoch [11/30], Train Loss: 0.13585972934961318\n",
            "Test Error: \n",
            " Accuracy: 0.70%, Avg loss: 0.9577, F1 Score: 0.6637 \n",
            "\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "loss: 0.020611  [    0/ 3200]\n",
            "loss: 0.307012  [ 1600/ 3200]\n",
            "Epoch [12/30], Train Loss: 0.1160644962079823\n",
            "Test Error: \n",
            " Accuracy: 0.76%, Avg loss: 0.7823, F1 Score: 0.7580 \n",
            "\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "loss: 0.010924  [    0/ 3200]\n",
            "loss: 0.205601  [ 1600/ 3200]\n",
            "Epoch [13/30], Train Loss: 0.08677271995227784\n",
            "Test Error: \n",
            " Accuracy: 0.78%, Avg loss: 0.7371, F1 Score: 0.7750 \n",
            "\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "loss: 0.034756  [    0/ 3200]\n",
            "loss: 0.067946  [ 1600/ 3200]\n",
            "Epoch [14/30], Train Loss: 0.08727868483634665\n",
            "Test Error: \n",
            " Accuracy: 0.77%, Avg loss: 0.7691, F1 Score: 0.7568 \n",
            "\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "loss: 0.027319  [    0/ 3200]\n",
            "loss: 0.009515  [ 1600/ 3200]\n",
            "Epoch [15/30], Train Loss: 0.08130358246853575\n",
            "Test Error: \n",
            " Accuracy: 0.77%, Avg loss: 0.7339, F1 Score: 0.7715 \n",
            "\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "loss: 0.032047  [    0/ 3200]\n",
            "loss: 0.128233  [ 1600/ 3200]\n",
            "Epoch [16/30], Train Loss: 0.055873507546493784\n",
            "Test Error: \n",
            " Accuracy: 0.76%, Avg loss: 0.7884, F1 Score: 0.7502 \n",
            "\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "loss: 0.008868  [    0/ 3200]\n",
            "loss: 0.005154  [ 1600/ 3200]\n",
            "Epoch [17/30], Train Loss: 0.03920514490455389\n",
            "Test Error: \n",
            " Accuracy: 0.78%, Avg loss: 0.7190, F1 Score: 0.7779 \n",
            "\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "loss: 0.029164  [    0/ 3200]\n",
            "loss: 0.033900  [ 1600/ 3200]\n",
            "Epoch [18/30], Train Loss: 0.02828557985718362\n",
            "Test Error: \n",
            " Accuracy: 0.77%, Avg loss: 0.7432, F1 Score: 0.7658 \n",
            "\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "loss: 0.041232  [    0/ 3200]\n",
            "loss: 0.166362  [ 1600/ 3200]\n",
            "Epoch [19/30], Train Loss: 0.025647503251675516\n",
            "Test Error: \n",
            " Accuracy: 0.77%, Avg loss: 0.7704, F1 Score: 0.7659 \n",
            "\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "loss: 0.004226  [    0/ 3200]\n",
            "loss: 0.015071  [ 1600/ 3200]\n",
            "Epoch [20/30], Train Loss: 0.02002410179411527\n",
            "Test Error: \n",
            " Accuracy: 0.78%, Avg loss: 0.7492, F1 Score: 0.7766 \n",
            "\n",
            "Epoch 21\n",
            "-------------------------------\n",
            "loss: 0.003217  [    0/ 3200]\n",
            "loss: 0.052685  [ 1600/ 3200]\n",
            "Epoch [21/30], Train Loss: 0.019129580270382574\n",
            "Test Error: \n",
            " Accuracy: 0.78%, Avg loss: 0.8093, F1 Score: 0.7735 \n",
            "\n",
            "Epoch 22\n",
            "-------------------------------\n",
            "loss: 0.001668  [    0/ 3200]\n",
            "loss: 0.005536  [ 1600/ 3200]\n",
            "Epoch [22/30], Train Loss: 0.01801082674297504\n",
            "Test Error: \n",
            " Accuracy: 0.80%, Avg loss: 0.7655, F1 Score: 0.7947 \n",
            "\n",
            "Epoch 23\n",
            "-------------------------------\n",
            "loss: 0.014954  [    0/ 3200]\n",
            "loss: 0.004848  [ 1600/ 3200]\n",
            "Epoch [23/30], Train Loss: 0.014235895124729723\n",
            "Test Error: \n",
            " Accuracy: 0.78%, Avg loss: 0.7756, F1 Score: 0.7714 \n",
            "\n",
            "Epoch 24\n",
            "-------------------------------\n",
            "loss: 0.002280  [    0/ 3200]\n",
            "loss: 0.014053  [ 1600/ 3200]\n",
            "Epoch [24/30], Train Loss: 0.015620062593079638\n",
            "Test Error: \n",
            " Accuracy: 0.77%, Avg loss: 0.7841, F1 Score: 0.7674 \n",
            "\n",
            "Epoch 25\n",
            "-------------------------------\n",
            "loss: 0.048858  [    0/ 3200]\n",
            "loss: 0.371106  [ 1600/ 3200]\n",
            "Epoch [25/30], Train Loss: 0.01077444240450859\n",
            "Test Error: \n",
            " Accuracy: 0.78%, Avg loss: 0.7998, F1 Score: 0.7791 \n",
            "\n",
            "Epoch 26\n",
            "-------------------------------\n",
            "loss: 0.002292  [    0/ 3200]\n",
            "loss: 0.001891  [ 1600/ 3200]\n",
            "Epoch [26/30], Train Loss: 0.007418603862461169\n",
            "Test Error: \n",
            " Accuracy: 0.78%, Avg loss: 0.7848, F1 Score: 0.7691 \n",
            "\n",
            "Epoch 27\n",
            "-------------------------------\n",
            "loss: 0.001911  [    0/ 3200]\n",
            "loss: 0.001883  [ 1600/ 3200]\n",
            "Epoch [27/30], Train Loss: 0.00800610527745448\n",
            "Test Error: \n",
            " Accuracy: 0.78%, Avg loss: 0.8023, F1 Score: 0.7800 \n",
            "\n",
            "Epoch 28\n",
            "-------------------------------\n",
            "loss: 0.001752  [    0/ 3200]\n",
            "loss: 0.001843  [ 1600/ 3200]\n",
            "Epoch [28/30], Train Loss: 0.006819045074807946\n",
            "Test Error: \n",
            " Accuracy: 0.78%, Avg loss: 0.8594, F1 Score: 0.7671 \n",
            "\n",
            "Epoch 29\n",
            "-------------------------------\n",
            "loss: 0.004409  [    0/ 3200]\n",
            "loss: 0.003428  [ 1600/ 3200]\n",
            "Epoch [29/30], Train Loss: 0.0056274486242909915\n",
            "Test Error: \n",
            " Accuracy: 0.78%, Avg loss: 0.7992, F1 Score: 0.7778 \n",
            "\n",
            "Epoch 30\n",
            "-------------------------------\n",
            "loss: 0.001542  [    0/ 3200]\n",
            "loss: 0.003521  [ 1600/ 3200]\n",
            "Epoch [30/30], Train Loss: 0.006316301211190875\n",
            "Test Error: \n",
            " Accuracy: 0.78%, Avg loss: 0.8145, F1 Score: 0.7690 \n",
            "\n",
            "Loading the best model and evaluating on the test set.\n",
            "Linear output: 1024\n",
            "Test Error: \n",
            " Accuracy: 0.76%, Avg loss: 0.7108, F1 Score: 0.7536 \n",
            "\n",
            "Final Test: Accuracy: 0.76, F1: 0.7536\n",
            "Linear output: 1024\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 1.532886  [    0/ 3200]\n",
            "loss: 1.297025  [ 1600/ 3200]\n",
            "Epoch [1/30], Train Loss: 0.8224291242659092\n",
            "Test Error: \n",
            " Accuracy: 0.74%, Avg loss: 0.6808, F1 Score: 0.7464 \n",
            "\n",
            "New best model with F1 score: 0.7464233414366302\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 0.994754  [    0/ 3200]\n",
            "loss: 0.516376  [ 1600/ 3200]\n",
            "Epoch [2/30], Train Loss: 0.6739729714393615\n",
            "Test Error: \n",
            " Accuracy: 0.78%, Avg loss: 0.6101, F1 Score: 0.7823 \n",
            "\n",
            "New best model with F1 score: 0.7823314863097085\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 0.656774  [    0/ 3200]\n",
            "loss: 0.573523  [ 1600/ 3200]\n",
            "Epoch [3/30], Train Loss: 0.6090067078918219\n",
            "Test Error: \n",
            " Accuracy: 0.71%, Avg loss: 0.7295, F1 Score: 0.7072 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 0.346967  [    0/ 3200]\n",
            "loss: 0.688510  [ 1600/ 3200]\n",
            "Epoch [4/30], Train Loss: 0.5566947028785943\n",
            "Test Error: \n",
            " Accuracy: 0.65%, Avg loss: 0.9428, F1 Score: 0.6367 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 0.674997  [    0/ 3200]\n",
            "loss: 0.414569  [ 1600/ 3200]\n",
            "Epoch [5/30], Train Loss: 0.5192338935285806\n",
            "Test Error: \n",
            " Accuracy: 0.53%, Avg loss: 1.7251, F1 Score: 0.4324 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 0.300766  [    0/ 3200]\n",
            "loss: 0.273512  [ 1600/ 3200]\n",
            "Epoch [6/30], Train Loss: 0.46885894406586887\n",
            "Test Error: \n",
            " Accuracy: 0.77%, Avg loss: 0.6021, F1 Score: 0.7716 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 0.164295  [    0/ 3200]\n",
            "loss: 0.121610  [ 1600/ 3200]\n",
            "Epoch [7/30], Train Loss: 0.3924017710611224\n",
            "Test Error: \n",
            " Accuracy: 0.72%, Avg loss: 0.7110, F1 Score: 0.7210 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 0.324089  [    0/ 3200]\n",
            "loss: 0.267361  [ 1600/ 3200]\n",
            "Epoch [8/30], Train Loss: 0.3577847071364522\n",
            "Test Error: \n",
            " Accuracy: 0.37%, Avg loss: 2.4488, F1 Score: 0.2956 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 0.399790  [    0/ 3200]\n",
            "loss: 0.500189  [ 1600/ 3200]\n",
            "Epoch [9/30], Train Loss: 0.31594955038279293\n",
            "Test Error: \n",
            " Accuracy: 0.71%, Avg loss: 0.7661, F1 Score: 0.6917 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 0.144350  [    0/ 3200]\n",
            "loss: 0.274933  [ 1600/ 3200]\n",
            "Epoch [10/30], Train Loss: 0.2849469804763794\n",
            "Test Error: \n",
            " Accuracy: 0.69%, Avg loss: 0.8559, F1 Score: 0.6939 \n",
            "\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "loss: 0.236206  [    0/ 3200]\n",
            "loss: 0.204369  [ 1600/ 3200]\n",
            "Epoch [11/30], Train Loss: 0.2272503693588078\n",
            "Test Error: \n",
            " Accuracy: 0.48%, Avg loss: 2.0614, F1 Score: 0.4189 \n",
            "\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "loss: 0.117101  [    0/ 3200]\n",
            "loss: 0.146673  [ 1600/ 3200]\n",
            "Epoch [12/30], Train Loss: 0.19445233423262834\n",
            "Test Error: \n",
            " Accuracy: 0.68%, Avg loss: 0.9504, F1 Score: 0.6856 \n",
            "\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "loss: 0.048700  [    0/ 3200]\n",
            "loss: 0.039357  [ 1600/ 3200]\n",
            "Epoch [13/30], Train Loss: 0.16450399882160127\n",
            "Test Error: \n",
            " Accuracy: 0.74%, Avg loss: 0.7585, F1 Score: 0.7334 \n",
            "\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "loss: 0.027495  [    0/ 3200]\n",
            "loss: 0.074224  [ 1600/ 3200]\n",
            "Epoch [14/30], Train Loss: 0.1177914276253432\n",
            "Test Error: \n",
            " Accuracy: 0.68%, Avg loss: 0.9382, F1 Score: 0.6816 \n",
            "\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "loss: 0.025015  [    0/ 3200]\n",
            "loss: 0.108747  [ 1600/ 3200]\n",
            "Epoch [15/30], Train Loss: 0.10597896497230977\n",
            "Test Error: \n",
            " Accuracy: 0.72%, Avg loss: 0.9453, F1 Score: 0.7043 \n",
            "\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "loss: 0.056321  [    0/ 3200]\n",
            "loss: 0.229667  [ 1600/ 3200]\n",
            "Epoch [16/30], Train Loss: 0.09676479164976627\n",
            "Test Error: \n",
            " Accuracy: 0.76%, Avg loss: 0.8145, F1 Score: 0.7487 \n",
            "\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "loss: 0.057771  [    0/ 3200]\n",
            "loss: 0.026138  [ 1600/ 3200]\n",
            "Epoch [17/30], Train Loss: 0.05976404914399609\n",
            "Test Error: \n",
            " Accuracy: 0.71%, Avg loss: 0.9241, F1 Score: 0.6927 \n",
            "\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "loss: 0.065107  [    0/ 3200]\n",
            "loss: 0.022267  [ 1600/ 3200]\n",
            "Epoch [18/30], Train Loss: 0.05606264122761786\n",
            "Test Error: \n",
            " Accuracy: 0.76%, Avg loss: 0.8214, F1 Score: 0.7443 \n",
            "\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "loss: 0.041802  [    0/ 3200]\n",
            "loss: 0.007188  [ 1600/ 3200]\n",
            "Epoch [19/30], Train Loss: 0.05410539452917874\n",
            "Test Error: \n",
            " Accuracy: 0.75%, Avg loss: 0.8155, F1 Score: 0.7392 \n",
            "\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "loss: 0.005775  [    0/ 3200]\n",
            "loss: 0.218150  [ 1600/ 3200]\n",
            "Epoch [20/30], Train Loss: 0.04761088984552771\n",
            "Test Error: \n",
            " Accuracy: 0.77%, Avg loss: 0.7486, F1 Score: 0.7647 \n",
            "\n",
            "Epoch 21\n",
            "-------------------------------\n",
            "loss: 0.076500  [    0/ 3200]\n",
            "loss: 0.010918  [ 1600/ 3200]\n",
            "Epoch [21/30], Train Loss: 0.03544701652368531\n",
            "Test Error: \n",
            " Accuracy: 0.75%, Avg loss: 0.8776, F1 Score: 0.7377 \n",
            "\n",
            "Epoch 22\n",
            "-------------------------------\n",
            "loss: 0.020692  [    0/ 3200]\n",
            "loss: 0.012862  [ 1600/ 3200]\n",
            "Epoch [22/30], Train Loss: 0.034742775548947974\n",
            "Test Error: \n",
            " Accuracy: 0.76%, Avg loss: 0.8227, F1 Score: 0.7523 \n",
            "\n",
            "Epoch 23\n",
            "-------------------------------\n",
            "loss: 0.014124  [    0/ 3200]\n",
            "loss: 0.008379  [ 1600/ 3200]\n",
            "Epoch [23/30], Train Loss: 0.023919393530813977\n",
            "Test Error: \n",
            " Accuracy: 0.77%, Avg loss: 0.8906, F1 Score: 0.7528 \n",
            "\n",
            "Epoch 24\n",
            "-------------------------------\n",
            "loss: 0.008386  [    0/ 3200]\n",
            "loss: 0.005439  [ 1600/ 3200]\n",
            "Epoch [24/30], Train Loss: 0.018480970781529323\n",
            "Test Error: \n",
            " Accuracy: 0.78%, Avg loss: 0.7692, F1 Score: 0.7712 \n",
            "\n",
            "Epoch 25\n",
            "-------------------------------\n",
            "loss: 0.020173  [    0/ 3200]\n",
            "loss: 0.008852  [ 1600/ 3200]\n",
            "Epoch [25/30], Train Loss: 0.022405737733934075\n",
            "Test Error: \n",
            " Accuracy: 0.76%, Avg loss: 0.7959, F1 Score: 0.7575 \n",
            "\n",
            "Epoch 26\n",
            "-------------------------------\n",
            "loss: 0.004829  [    0/ 3200]\n",
            "loss: 0.007513  [ 1600/ 3200]\n",
            "Epoch [26/30], Train Loss: 0.027462619879515843\n",
            "Test Error: \n",
            " Accuracy: 0.78%, Avg loss: 0.7685, F1 Score: 0.7703 \n",
            "\n",
            "Epoch 27\n",
            "-------------------------------\n",
            "loss: 0.003313  [    0/ 3200]\n",
            "loss: 0.003422  [ 1600/ 3200]\n",
            "Epoch [27/30], Train Loss: 0.013394267926923931\n",
            "Test Error: \n",
            " Accuracy: 0.78%, Avg loss: 0.7763, F1 Score: 0.7722 \n",
            "\n",
            "Epoch 28\n",
            "-------------------------------\n",
            "loss: 0.006709  [    0/ 3200]\n",
            "loss: 0.018320  [ 1600/ 3200]\n",
            "Epoch [28/30], Train Loss: 0.01029184014420025\n",
            "Test Error: \n",
            " Accuracy: 0.75%, Avg loss: 0.8377, F1 Score: 0.7374 \n",
            "\n",
            "Epoch 29\n",
            "-------------------------------\n",
            "loss: 0.004220  [    0/ 3200]\n",
            "loss: 0.033531  [ 1600/ 3200]\n",
            "Epoch [29/30], Train Loss: 0.01347131947404705\n",
            "Test Error: \n",
            " Accuracy: 0.78%, Avg loss: 0.7813, F1 Score: 0.7709 \n",
            "\n",
            "Epoch 30\n",
            "-------------------------------\n",
            "loss: 0.002666  [    0/ 3200]\n",
            "loss: 0.005447  [ 1600/ 3200]\n",
            "Epoch [30/30], Train Loss: 0.01241335614817217\n",
            "Test Error: \n",
            " Accuracy: 0.77%, Avg loss: 0.7903, F1 Score: 0.7609 \n",
            "\n",
            "Loading the best model and evaluating on the test set.\n",
            "Linear output: 1024\n",
            "Test Error: \n",
            " Accuracy: 0.76%, Avg loss: 0.6747, F1 Score: 0.7625 \n",
            "\n",
            "Final Test: Accuracy: 0.76, F1: 0.7625\n",
            "Linear output: 1024\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 1.425169  [    0/ 3200]\n",
            "loss: 1.079278  [ 1600/ 3200]\n",
            "Epoch [1/30], Train Loss: 0.841722276955843\n",
            "Test Error: \n",
            " Accuracy: 0.74%, Avg loss: 0.6504, F1 Score: 0.7310 \n",
            "\n",
            "New best model with F1 score: 0.7310036308200222\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 0.688218  [    0/ 3200]\n",
            "loss: 0.730047  [ 1600/ 3200]\n",
            "Epoch [2/30], Train Loss: 0.6853963848948479\n",
            "Test Error: \n",
            " Accuracy: 0.75%, Avg loss: 0.6196, F1 Score: 0.7518 \n",
            "\n",
            "New best model with F1 score: 0.7517920959189804\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 0.412768  [    0/ 3200]\n",
            "loss: 0.829085  [ 1600/ 3200]\n",
            "Epoch [3/30], Train Loss: 0.59734205968678\n",
            "Test Error: \n",
            " Accuracy: 0.78%, Avg loss: 0.5749, F1 Score: 0.7822 \n",
            "\n",
            "New best model with F1 score: 0.782222814565658\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 0.961393  [    0/ 3200]\n",
            "loss: 0.394584  [ 1600/ 3200]\n",
            "Epoch [4/30], Train Loss: 0.5542025502026081\n",
            "Test Error: \n",
            " Accuracy: 0.66%, Avg loss: 0.8918, F1 Score: 0.6305 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 0.463544  [    0/ 3200]\n",
            "loss: 0.403949  [ 1600/ 3200]\n",
            "Epoch [5/30], Train Loss: 0.4936407381296158\n",
            "Test Error: \n",
            " Accuracy: 0.74%, Avg loss: 0.7044, F1 Score: 0.6973 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 0.501713  [    0/ 3200]\n",
            "loss: 0.617145  [ 1600/ 3200]\n",
            "Epoch [6/30], Train Loss: 0.47672216072678564\n",
            "Test Error: \n",
            " Accuracy: 0.68%, Avg loss: 0.8920, F1 Score: 0.6663 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 0.142990  [    0/ 3200]\n",
            "loss: 0.772661  [ 1600/ 3200]\n",
            "Epoch [7/30], Train Loss: 0.4138079387694597\n",
            "Test Error: \n",
            " Accuracy: 0.77%, Avg loss: 0.5916, F1 Score: 0.7671 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 0.093704  [    0/ 3200]\n",
            "loss: 0.205463  [ 1600/ 3200]\n",
            "Epoch [8/30], Train Loss: 0.34192750971764324\n",
            "Test Error: \n",
            " Accuracy: 0.75%, Avg loss: 0.6491, F1 Score: 0.7443 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 0.163674  [    0/ 3200]\n",
            "loss: 0.879394  [ 1600/ 3200]\n",
            "Epoch [9/30], Train Loss: 0.3251004377566278\n",
            "Test Error: \n",
            " Accuracy: 0.78%, Avg loss: 0.5562, F1 Score: 0.7783 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 0.286781  [    0/ 3200]\n",
            "loss: 0.407283  [ 1600/ 3200]\n",
            "Epoch [10/30], Train Loss: 0.25097977932542564\n",
            "Test Error: \n",
            " Accuracy: 0.78%, Avg loss: 0.5864, F1 Score: 0.7804 \n",
            "\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "loss: 0.179800  [    0/ 3200]\n",
            "loss: 0.411468  [ 1600/ 3200]\n",
            "Epoch [11/30], Train Loss: 0.19087109979242087\n",
            "Test Error: \n",
            " Accuracy: 0.76%, Avg loss: 0.7392, F1 Score: 0.7381 \n",
            "\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "loss: 0.042250  [    0/ 3200]\n",
            "loss: 0.064500  [ 1600/ 3200]\n",
            "Epoch [12/30], Train Loss: 0.17359425392001868\n",
            "Test Error: \n",
            " Accuracy: 0.77%, Avg loss: 0.6900, F1 Score: 0.7663 \n",
            "\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "loss: 0.153570  [    0/ 3200]\n",
            "loss: 0.242006  [ 1600/ 3200]\n",
            "Epoch [13/30], Train Loss: 0.13206044651102275\n",
            "Test Error: \n",
            " Accuracy: 0.74%, Avg loss: 0.8699, F1 Score: 0.7347 \n",
            "\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "loss: 0.167239  [    0/ 3200]\n",
            "loss: 0.030434  [ 1600/ 3200]\n",
            "Epoch [14/30], Train Loss: 0.11313767387298868\n",
            "Test Error: \n",
            " Accuracy: 0.76%, Avg loss: 0.8022, F1 Score: 0.7462 \n",
            "\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "loss: 0.031315  [    0/ 3200]\n",
            "loss: 0.124430  [ 1600/ 3200]\n",
            "Epoch [15/30], Train Loss: 0.08520467097638175\n",
            "Test Error: \n",
            " Accuracy: 0.77%, Avg loss: 0.7243, F1 Score: 0.7626 \n",
            "\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "loss: 0.084733  [    0/ 3200]\n",
            "loss: 0.180817  [ 1600/ 3200]\n",
            "Epoch [16/30], Train Loss: 0.05892984662903473\n",
            "Test Error: \n",
            " Accuracy: 0.77%, Avg loss: 0.7566, F1 Score: 0.7703 \n",
            "\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "loss: 0.051695  [    0/ 3200]\n",
            "loss: 0.011707  [ 1600/ 3200]\n",
            "Epoch [17/30], Train Loss: 0.06921900241053663\n",
            "Test Error: \n",
            " Accuracy: 0.73%, Avg loss: 0.9779, F1 Score: 0.7290 \n",
            "\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "loss: 0.017150  [    0/ 3200]\n",
            "loss: 0.033198  [ 1600/ 3200]\n",
            "Epoch [18/30], Train Loss: 0.05319484658772126\n",
            "Test Error: \n",
            " Accuracy: 0.77%, Avg loss: 0.8206, F1 Score: 0.7608 \n",
            "\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "loss: 0.041265  [    0/ 3200]\n",
            "loss: 0.009269  [ 1600/ 3200]\n",
            "Epoch [19/30], Train Loss: 0.04593218830879778\n",
            "Test Error: \n",
            " Accuracy: 0.79%, Avg loss: 0.8197, F1 Score: 0.7811 \n",
            "\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "loss: 0.052862  [    0/ 3200]\n",
            "loss: 0.041306  [ 1600/ 3200]\n",
            "Epoch [20/30], Train Loss: 0.03560604557802435\n",
            "Test Error: \n",
            " Accuracy: 0.79%, Avg loss: 0.8115, F1 Score: 0.7901 \n",
            "\n",
            "New best model with F1 score: 0.7900897815038729\n",
            "Epoch 21\n",
            "-------------------------------\n",
            "loss: 0.011146  [    0/ 3200]\n",
            "loss: 0.043045  [ 1600/ 3200]\n",
            "Epoch [21/30], Train Loss: 0.03556626165285706\n",
            "Test Error: \n",
            " Accuracy: 0.79%, Avg loss: 0.8316, F1 Score: 0.7838 \n",
            "\n",
            "Epoch 22\n",
            "-------------------------------\n",
            "loss: 0.006719  [    0/ 3200]\n",
            "loss: 0.005892  [ 1600/ 3200]\n",
            "Epoch [22/30], Train Loss: 0.02908867759339046\n",
            "Test Error: \n",
            " Accuracy: 0.78%, Avg loss: 0.8548, F1 Score: 0.7730 \n",
            "\n",
            "Epoch 23\n",
            "-------------------------------\n",
            "loss: 0.012419  [    0/ 3200]\n",
            "loss: 0.005154  [ 1600/ 3200]\n",
            "Epoch [23/30], Train Loss: 0.02079388906247914\n",
            "Test Error: \n",
            " Accuracy: 0.77%, Avg loss: 0.8843, F1 Score: 0.7608 \n",
            "\n",
            "Epoch 24\n",
            "-------------------------------\n",
            "loss: 0.005920  [    0/ 3200]\n",
            "loss: 0.012260  [ 1600/ 3200]\n",
            "Epoch [24/30], Train Loss: 0.021017676073824987\n",
            "Test Error: \n",
            " Accuracy: 0.76%, Avg loss: 0.9411, F1 Score: 0.7555 \n",
            "\n",
            "Epoch 25\n",
            "-------------------------------\n",
            "loss: 0.008426  [    0/ 3200]\n",
            "loss: 0.017956  [ 1600/ 3200]\n",
            "Epoch [25/30], Train Loss: 0.018136167866759933\n",
            "Test Error: \n",
            " Accuracy: 0.78%, Avg loss: 0.8560, F1 Score: 0.7692 \n",
            "\n",
            "Epoch 26\n",
            "-------------------------------\n",
            "loss: 0.010765  [    0/ 3200]\n",
            "loss: 0.006977  [ 1600/ 3200]\n",
            "Epoch [26/30], Train Loss: 0.020484862328739837\n",
            "Test Error: \n",
            " Accuracy: 0.79%, Avg loss: 0.8287, F1 Score: 0.7817 \n",
            "\n",
            "Epoch 27\n",
            "-------------------------------\n",
            "loss: 0.002072  [    0/ 3200]\n",
            "loss: 0.008432  [ 1600/ 3200]\n",
            "Epoch [27/30], Train Loss: 0.015068046808009968\n",
            "Test Error: \n",
            " Accuracy: 0.77%, Avg loss: 0.9364, F1 Score: 0.7599 \n",
            "\n",
            "Epoch 28\n",
            "-------------------------------\n",
            "loss: 0.003438  [    0/ 3200]\n",
            "loss: 0.004085  [ 1600/ 3200]\n",
            "Epoch [28/30], Train Loss: 0.017414979555178435\n",
            "Test Error: \n",
            " Accuracy: 0.78%, Avg loss: 0.8923, F1 Score: 0.7705 \n",
            "\n",
            "Epoch 29\n",
            "-------------------------------\n",
            "loss: 0.004580  [    0/ 3200]\n",
            "loss: 0.001988  [ 1600/ 3200]\n",
            "Epoch [29/30], Train Loss: 0.017516004625358617\n",
            "Test Error: \n",
            " Accuracy: 0.79%, Avg loss: 0.8629, F1 Score: 0.7801 \n",
            "\n",
            "Epoch 30\n",
            "-------------------------------\n",
            "loss: 0.003841  [    0/ 3200]\n",
            "loss: 0.002457  [ 1600/ 3200]\n",
            "Epoch [30/30], Train Loss: 0.02074877618870232\n",
            "Test Error: \n",
            " Accuracy: 0.78%, Avg loss: 0.8842, F1 Score: 0.7725 \n",
            "\n",
            "Loading the best model and evaluating on the test set.\n",
            "Linear output: 1024\n",
            "Test Error: \n",
            " Accuracy: 0.74%, Avg loss: 1.0352, F1 Score: 0.7442 \n",
            "\n",
            "Final Test: Accuracy: 0.74, F1: 0.7442\n",
            "Linear output: 1024\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 1.365973  [    0/ 3200]\n",
            "loss: 0.801333  [ 1600/ 3200]\n",
            "Epoch [1/30], Train Loss: 0.8436405497789383\n",
            "Test Error: \n",
            " Accuracy: 0.71%, Avg loss: 0.7281, F1 Score: 0.6986 \n",
            "\n",
            "New best model with F1 score: 0.6986379230686052\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 0.768835  [    0/ 3200]\n",
            "loss: 0.608938  [ 1600/ 3200]\n",
            "Epoch [2/30], Train Loss: 0.693776087909937\n",
            "Test Error: \n",
            " Accuracy: 0.61%, Avg loss: 0.9653, F1 Score: 0.5973 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 0.557500  [    0/ 3200]\n",
            "loss: 0.851204  [ 1600/ 3200]\n",
            "Epoch [3/30], Train Loss: 0.6051228258758783\n",
            "Test Error: \n",
            " Accuracy: 0.70%, Avg loss: 0.7141, F1 Score: 0.6904 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 0.475705  [    0/ 3200]\n",
            "loss: 0.528149  [ 1600/ 3200]\n",
            "Epoch [4/30], Train Loss: 0.5719016713649034\n",
            "Test Error: \n",
            " Accuracy: 0.64%, Avg loss: 0.9722, F1 Score: 0.5597 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 0.241661  [    0/ 3200]\n",
            "loss: 0.260496  [ 1600/ 3200]\n",
            "Epoch [5/30], Train Loss: 0.5095401072502136\n",
            "Test Error: \n",
            " Accuracy: 0.69%, Avg loss: 0.8252, F1 Score: 0.6997 \n",
            "\n",
            "New best model with F1 score: 0.69966875448424\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 0.537462  [    0/ 3200]\n",
            "loss: 0.298893  [ 1600/ 3200]\n",
            "Epoch [6/30], Train Loss: 0.4548442694544792\n",
            "Test Error: \n",
            " Accuracy: 0.74%, Avg loss: 0.6979, F1 Score: 0.7034 \n",
            "\n",
            "New best model with F1 score: 0.7034461551663224\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 0.303054  [    0/ 3200]\n",
            "loss: 0.740363  [ 1600/ 3200]\n",
            "Epoch [7/30], Train Loss: 0.4368445958942175\n",
            "Test Error: \n",
            " Accuracy: 0.64%, Avg loss: 0.9564, F1 Score: 0.6248 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 0.318722  [    0/ 3200]\n",
            "loss: 0.219377  [ 1600/ 3200]\n",
            "Epoch [8/30], Train Loss: 0.3935630867630243\n",
            "Test Error: \n",
            " Accuracy: 0.76%, Avg loss: 0.6046, F1 Score: 0.7447 \n",
            "\n",
            "New best model with F1 score: 0.7447218844161797\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 0.377426  [    0/ 3200]\n",
            "loss: 0.289204  [ 1600/ 3200]\n",
            "Epoch [9/30], Train Loss: 0.34695815060287716\n",
            "Test Error: \n",
            " Accuracy: 0.60%, Avg loss: 1.1084, F1 Score: 0.6141 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 0.308168  [    0/ 3200]\n",
            "loss: 0.080420  [ 1600/ 3200]\n",
            "Epoch [10/30], Train Loss: 0.2701023023948073\n",
            "Test Error: \n",
            " Accuracy: 0.70%, Avg loss: 0.8991, F1 Score: 0.6978 \n",
            "\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "loss: 0.396706  [    0/ 3200]\n",
            "loss: 0.099973  [ 1600/ 3200]\n",
            "Epoch [11/30], Train Loss: 0.23390454357489943\n",
            "Test Error: \n",
            " Accuracy: 0.61%, Avg loss: 1.3197, F1 Score: 0.5572 \n",
            "\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "loss: 0.578091  [    0/ 3200]\n",
            "loss: 0.049671  [ 1600/ 3200]\n",
            "Epoch [12/30], Train Loss: 0.22924881254322826\n",
            "Test Error: \n",
            " Accuracy: 0.67%, Avg loss: 1.0739, F1 Score: 0.6726 \n",
            "\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "loss: 0.125157  [    0/ 3200]\n",
            "loss: 0.185942  [ 1600/ 3200]\n",
            "Epoch [13/30], Train Loss: 0.18588342251256107\n",
            "Test Error: \n",
            " Accuracy: 0.66%, Avg loss: 1.1150, F1 Score: 0.6455 \n",
            "\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "loss: 0.325985  [    0/ 3200]\n",
            "loss: 0.180146  [ 1600/ 3200]\n",
            "Epoch [14/30], Train Loss: 0.16143933122977613\n",
            "Test Error: \n",
            " Accuracy: 0.74%, Avg loss: 0.8523, F1 Score: 0.7207 \n",
            "\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "loss: 0.100278  [    0/ 3200]\n",
            "loss: 0.421937  [ 1600/ 3200]\n",
            "Epoch [15/30], Train Loss: 0.14437186351977288\n",
            "Test Error: \n",
            " Accuracy: 0.75%, Avg loss: 0.8281, F1 Score: 0.7322 \n",
            "\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "loss: 0.058441  [    0/ 3200]\n",
            "loss: 0.180086  [ 1600/ 3200]\n",
            "Epoch [16/30], Train Loss: 0.10190762159414589\n",
            "Test Error: \n",
            " Accuracy: 0.74%, Avg loss: 0.8351, F1 Score: 0.7177 \n",
            "\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "loss: 0.019267  [    0/ 3200]\n",
            "loss: 0.043431  [ 1600/ 3200]\n",
            "Epoch [17/30], Train Loss: 0.11043890194036067\n",
            "Test Error: \n",
            " Accuracy: 0.72%, Avg loss: 0.9413, F1 Score: 0.7130 \n",
            "\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "loss: 0.090814  [    0/ 3200]\n",
            "loss: 0.035945  [ 1600/ 3200]\n",
            "Epoch [18/30], Train Loss: 0.09116405279841273\n",
            "Test Error: \n",
            " Accuracy: 0.71%, Avg loss: 0.9594, F1 Score: 0.6988 \n",
            "\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "loss: 0.097533  [    0/ 3200]\n",
            "loss: 0.030038  [ 1600/ 3200]\n",
            "Epoch [19/30], Train Loss: 0.07254330921452493\n",
            "Test Error: \n",
            " Accuracy: 0.77%, Avg loss: 0.7974, F1 Score: 0.7608 \n",
            "\n",
            "New best model with F1 score: 0.7607956895040102\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "loss: 0.233513  [    0/ 3200]\n",
            "loss: 0.033622  [ 1600/ 3200]\n",
            "Epoch [20/30], Train Loss: 0.04491135815856978\n",
            "Test Error: \n",
            " Accuracy: 0.79%, Avg loss: 0.7712, F1 Score: 0.7827 \n",
            "\n",
            "New best model with F1 score: 0.7827490263194776\n",
            "Epoch 21\n",
            "-------------------------------\n",
            "loss: 0.013067  [    0/ 3200]\n",
            "loss: 0.019625  [ 1600/ 3200]\n",
            "Epoch [21/30], Train Loss: 0.036573923921678214\n",
            "Test Error: \n",
            " Accuracy: 0.77%, Avg loss: 0.8648, F1 Score: 0.7614 \n",
            "\n",
            "Epoch 22\n",
            "-------------------------------\n",
            "loss: 0.019574  [    0/ 3200]\n",
            "loss: 0.015398  [ 1600/ 3200]\n",
            "Epoch [22/30], Train Loss: 0.041861251050140706\n",
            "Test Error: \n",
            " Accuracy: 0.78%, Avg loss: 0.7595, F1 Score: 0.7774 \n",
            "\n",
            "Epoch 23\n",
            "-------------------------------\n",
            "loss: 0.046600  [    0/ 3200]\n",
            "loss: 0.007054  [ 1600/ 3200]\n",
            "Epoch [23/30], Train Loss: 0.03863441504188813\n",
            "Test Error: \n",
            " Accuracy: 0.78%, Avg loss: 0.7847, F1 Score: 0.7807 \n",
            "\n",
            "Epoch 24\n",
            "-------------------------------\n",
            "loss: 0.008510  [    0/ 3200]\n",
            "loss: 0.036544  [ 1600/ 3200]\n",
            "Epoch [24/30], Train Loss: 0.033009333272930236\n",
            "Test Error: \n",
            " Accuracy: 0.78%, Avg loss: 0.7955, F1 Score: 0.7711 \n",
            "\n",
            "Epoch 25\n",
            "-------------------------------\n",
            "loss: 0.010474  [    0/ 3200]\n",
            "loss: 0.066739  [ 1600/ 3200]\n",
            "Epoch [25/30], Train Loss: 0.04347936911392026\n",
            "Test Error: \n",
            " Accuracy: 0.79%, Avg loss: 0.7711, F1 Score: 0.7802 \n",
            "\n",
            "Epoch 26\n",
            "-------------------------------\n",
            "loss: 0.013116  [    0/ 3200]\n",
            "loss: 0.020343  [ 1600/ 3200]\n",
            "Epoch [26/30], Train Loss: 0.02535294375848025\n",
            "Test Error: \n",
            " Accuracy: 0.80%, Avg loss: 0.7565, F1 Score: 0.7941 \n",
            "\n",
            "New best model with F1 score: 0.7940643572098842\n",
            "Epoch 27\n",
            "-------------------------------\n",
            "loss: 0.007243  [    0/ 3200]\n",
            "loss: 0.015594  [ 1600/ 3200]\n",
            "Epoch [27/30], Train Loss: 0.023194051237078385\n",
            "Test Error: \n",
            " Accuracy: 0.79%, Avg loss: 0.7728, F1 Score: 0.7856 \n",
            "\n",
            "Epoch 28\n",
            "-------------------------------\n",
            "loss: 0.028374  [    0/ 3200]\n",
            "loss: 0.012882  [ 1600/ 3200]\n",
            "Epoch [28/30], Train Loss: 0.02330993897980079\n",
            "Test Error: \n",
            " Accuracy: 0.79%, Avg loss: 0.7748, F1 Score: 0.7864 \n",
            "\n",
            "Epoch 29\n",
            "-------------------------------\n",
            "loss: 0.005234  [    0/ 3200]\n",
            "loss: 0.008804  [ 1600/ 3200]\n",
            "Epoch [29/30], Train Loss: 0.024192683814908376\n",
            "Test Error: \n",
            " Accuracy: 0.80%, Avg loss: 0.7588, F1 Score: 0.7945 \n",
            "\n",
            "New best model with F1 score: 0.7945208858645213\n",
            "Epoch 30\n",
            "-------------------------------\n",
            "loss: 0.019544  [    0/ 3200]\n",
            "loss: 0.012206  [ 1600/ 3200]\n",
            "Epoch [30/30], Train Loss: 0.01845256817759946\n",
            "Test Error: \n",
            " Accuracy: 0.79%, Avg loss: 0.7601, F1 Score: 0.7877 \n",
            "\n",
            "Loading the best model and evaluating on the test set.\n",
            "Linear output: 1024\n",
            "Test Error: \n",
            " Accuracy: 0.75%, Avg loss: 0.9403, F1 Score: 0.7423 \n",
            "\n",
            "Final Test: Accuracy: 0.75, F1: 0.7423\n",
            "Linear output: 1024\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 1.537600  [    0/ 3200]\n",
            "loss: 1.139290  [ 1600/ 3200]\n",
            "Epoch [1/60], Train Loss: 0.7817351548373699\n",
            "Test Error: \n",
            " Accuracy: 0.72%, Avg loss: 0.7139, F1 Score: 0.7156 \n",
            "\n",
            "New best model with F1 score: 0.7156075270462108\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 0.535209  [    0/ 3200]\n",
            "loss: 0.360859  [ 1600/ 3200]\n",
            "Epoch [2/60], Train Loss: 0.6499189981818199\n",
            "Test Error: \n",
            " Accuracy: 0.64%, Avg loss: 0.8183, F1 Score: 0.6440 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 0.393029  [    0/ 3200]\n",
            "loss: 0.337304  [ 1600/ 3200]\n",
            "Epoch [3/60], Train Loss: 0.5742621670663357\n",
            "Test Error: \n",
            " Accuracy: 0.62%, Avg loss: 0.9353, F1 Score: 0.5594 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 0.522605  [    0/ 3200]\n",
            "loss: 0.414425  [ 1600/ 3200]\n",
            "Epoch [4/60], Train Loss: 0.5059410353004933\n",
            "Test Error: \n",
            " Accuracy: 0.69%, Avg loss: 0.7979, F1 Score: 0.6146 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 0.474431  [    0/ 3200]\n",
            "loss: 0.415591  [ 1600/ 3200]\n",
            "Epoch [5/60], Train Loss: 0.46898218017071486\n",
            "Test Error: \n",
            " Accuracy: 0.71%, Avg loss: 0.6867, F1 Score: 0.7138 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 0.304409  [    0/ 3200]\n",
            "loss: 0.735793  [ 1600/ 3200]\n",
            "Epoch [6/60], Train Loss: 0.38599614772945645\n",
            "Test Error: \n",
            " Accuracy: 0.79%, Avg loss: 0.5561, F1 Score: 0.7795 \n",
            "\n",
            "New best model with F1 score: 0.7794807588400811\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 0.239177  [    0/ 3200]\n",
            "loss: 0.134120  [ 1600/ 3200]\n",
            "Epoch [7/60], Train Loss: 0.33594737423583865\n",
            "Test Error: \n",
            " Accuracy: 0.66%, Avg loss: 0.9556, F1 Score: 0.6545 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 0.148661  [    0/ 3200]\n",
            "loss: 0.131342  [ 1600/ 3200]\n",
            "Epoch [8/60], Train Loss: 0.30555293740704653\n",
            "Test Error: \n",
            " Accuracy: 0.65%, Avg loss: 0.9790, F1 Score: 0.6238 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 0.298357  [    0/ 3200]\n",
            "loss: 0.100440  [ 1600/ 3200]\n",
            "Epoch [9/60], Train Loss: 0.22649270778521896\n",
            "Test Error: \n",
            " Accuracy: 0.73%, Avg loss: 0.7835, F1 Score: 0.7264 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 0.146978  [    0/ 3200]\n",
            "loss: 0.185304  [ 1600/ 3200]\n",
            "Epoch [10/60], Train Loss: 0.21116266542114318\n",
            "Test Error: \n",
            " Accuracy: 0.59%, Avg loss: 1.1735, F1 Score: 0.5926 \n",
            "\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "loss: 0.067976  [    0/ 3200]\n",
            "loss: 0.075740  [ 1600/ 3200]\n",
            "Epoch [11/60], Train Loss: 0.15786001798696817\n",
            "Test Error: \n",
            " Accuracy: 0.78%, Avg loss: 0.6376, F1 Score: 0.7754 \n",
            "\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "loss: 0.016002  [    0/ 3200]\n",
            "loss: 0.138037  [ 1600/ 3200]\n",
            "Epoch [12/60], Train Loss: 0.14755835877731444\n",
            "Test Error: \n",
            " Accuracy: 0.73%, Avg loss: 0.7924, F1 Score: 0.7329 \n",
            "\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "loss: 0.205647  [    0/ 3200]\n",
            "loss: 0.370558  [ 1600/ 3200]\n",
            "Epoch [13/60], Train Loss: 0.11573257454205305\n",
            "Test Error: \n",
            " Accuracy: 0.77%, Avg loss: 0.7028, F1 Score: 0.7704 \n",
            "\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "loss: 0.062973  [    0/ 3200]\n",
            "loss: 0.231178  [ 1600/ 3200]\n",
            "Epoch [14/60], Train Loss: 0.1351045530475676\n",
            "Test Error: \n",
            " Accuracy: 0.66%, Avg loss: 1.1330, F1 Score: 0.6496 \n",
            "\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "loss: 0.029722  [    0/ 3200]\n",
            "loss: 0.126235  [ 1600/ 3200]\n",
            "Epoch [15/60], Train Loss: 0.11077274879906326\n",
            "Test Error: \n",
            " Accuracy: 0.77%, Avg loss: 0.7020, F1 Score: 0.7714 \n",
            "\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "loss: 0.018868  [    0/ 3200]\n",
            "loss: 0.011472  [ 1600/ 3200]\n",
            "Epoch [16/60], Train Loss: 0.09396219739690423\n",
            "Test Error: \n",
            " Accuracy: 0.74%, Avg loss: 0.8161, F1 Score: 0.7458 \n",
            "\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "loss: 0.122562  [    0/ 3200]\n",
            "loss: 0.083190  [ 1600/ 3200]\n",
            "Epoch [17/60], Train Loss: 0.08147530342219397\n",
            "Test Error: \n",
            " Accuracy: 0.73%, Avg loss: 0.8753, F1 Score: 0.7124 \n",
            "\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "loss: 0.020535  [    0/ 3200]\n",
            "loss: 0.100398  [ 1600/ 3200]\n",
            "Epoch [18/60], Train Loss: 0.051977283569285646\n",
            "Test Error: \n",
            " Accuracy: 0.68%, Avg loss: 1.0976, F1 Score: 0.6741 \n",
            "\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "loss: 0.020691  [    0/ 3200]\n",
            "loss: 0.118510  [ 1600/ 3200]\n",
            "Epoch [19/60], Train Loss: 0.05491504019708373\n",
            "Test Error: \n",
            " Accuracy: 0.80%, Avg loss: 0.7637, F1 Score: 0.7892 \n",
            "\n",
            "New best model with F1 score: 0.7891847473004455\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "loss: 0.021864  [    0/ 3200]\n",
            "loss: 0.049787  [ 1600/ 3200]\n",
            "Epoch [20/60], Train Loss: 0.0622722438571509\n",
            "Test Error: \n",
            " Accuracy: 0.78%, Avg loss: 0.7559, F1 Score: 0.7814 \n",
            "\n",
            "Epoch 21\n",
            "-------------------------------\n",
            "loss: 0.075230  [    0/ 3200]\n",
            "loss: 0.002742  [ 1600/ 3200]\n",
            "Epoch [21/60], Train Loss: 0.038619383999612185\n",
            "Test Error: \n",
            " Accuracy: 0.73%, Avg loss: 0.9860, F1 Score: 0.7136 \n",
            "\n",
            "Epoch 22\n",
            "-------------------------------\n",
            "loss: 0.004143  [    0/ 3200]\n",
            "loss: 0.013283  [ 1600/ 3200]\n",
            "Epoch [22/60], Train Loss: 0.0428598360583419\n",
            "Test Error: \n",
            " Accuracy: 0.76%, Avg loss: 0.8570, F1 Score: 0.7554 \n",
            "\n",
            "Epoch 23\n",
            "-------------------------------\n",
            "loss: 0.003715  [    0/ 3200]\n",
            "loss: 0.012538  [ 1600/ 3200]\n",
            "Epoch [23/60], Train Loss: 0.031159252131474205\n",
            "Test Error: \n",
            " Accuracy: 0.78%, Avg loss: 0.8069, F1 Score: 0.7792 \n",
            "\n",
            "Epoch 24\n",
            "-------------------------------\n",
            "loss: 0.005230  [    0/ 3200]\n",
            "loss: 0.003511  [ 1600/ 3200]\n",
            "Epoch [24/60], Train Loss: 0.0491603858221788\n",
            "Test Error: \n",
            " Accuracy: 0.76%, Avg loss: 0.9109, F1 Score: 0.7459 \n",
            "\n",
            "Epoch 25\n",
            "-------------------------------\n",
            "loss: 0.021726  [    0/ 3200]\n",
            "loss: 0.003979  [ 1600/ 3200]\n",
            "Epoch [25/60], Train Loss: 0.02725192260724725\n",
            "Test Error: \n",
            " Accuracy: 0.77%, Avg loss: 0.8396, F1 Score: 0.7672 \n",
            "\n",
            "Epoch 26\n",
            "-------------------------------\n",
            "loss: 0.001585  [    0/ 3200]\n",
            "loss: 0.079495  [ 1600/ 3200]\n",
            "Epoch [26/60], Train Loss: 0.037760926627670416\n",
            "Test Error: \n",
            " Accuracy: 0.73%, Avg loss: 0.9445, F1 Score: 0.7251 \n",
            "\n",
            "Epoch 27\n",
            "-------------------------------\n",
            "loss: 0.003938  [    0/ 3200]\n",
            "loss: 0.002113  [ 1600/ 3200]\n",
            "Epoch [27/60], Train Loss: 0.020099021137866657\n",
            "Test Error: \n",
            " Accuracy: 0.78%, Avg loss: 0.8922, F1 Score: 0.7692 \n",
            "\n",
            "Epoch 28\n",
            "-------------------------------\n",
            "loss: 0.012004  [    0/ 3200]\n",
            "loss: 0.003759  [ 1600/ 3200]\n",
            "Epoch [28/60], Train Loss: 0.026753897381422576\n",
            "Test Error: \n",
            " Accuracy: 0.75%, Avg loss: 0.8808, F1 Score: 0.7474 \n",
            "\n",
            "Epoch 29\n",
            "-------------------------------\n",
            "loss: 0.031226  [    0/ 3200]\n",
            "loss: 0.009201  [ 1600/ 3200]\n",
            "Epoch [29/60], Train Loss: 0.01775420449077501\n",
            "Test Error: \n",
            " Accuracy: 0.76%, Avg loss: 0.8487, F1 Score: 0.7603 \n",
            "\n",
            "Epoch 30\n",
            "-------------------------------\n",
            "loss: 0.002196  [    0/ 3200]\n",
            "loss: 0.003568  [ 1600/ 3200]\n",
            "Epoch [30/60], Train Loss: 0.01753243700019084\n",
            "Test Error: \n",
            " Accuracy: 0.77%, Avg loss: 0.9139, F1 Score: 0.7589 \n",
            "\n",
            "Epoch 31\n",
            "-------------------------------\n",
            "loss: 0.001579  [    0/ 3200]\n",
            "loss: 0.003993  [ 1600/ 3200]\n",
            "Epoch [31/60], Train Loss: 0.023439505777932936\n",
            "Test Error: \n",
            " Accuracy: 0.78%, Avg loss: 0.8589, F1 Score: 0.7799 \n",
            "\n",
            "Epoch 32\n",
            "-------------------------------\n",
            "loss: 0.025264  [    0/ 3200]\n",
            "loss: 0.001558  [ 1600/ 3200]\n",
            "Epoch [32/60], Train Loss: 0.018605930668127258\n",
            "Test Error: \n",
            " Accuracy: 0.76%, Avg loss: 0.9821, F1 Score: 0.7464 \n",
            "\n",
            "Epoch 33\n",
            "-------------------------------\n",
            "loss: 0.002262  [    0/ 3200]\n",
            "loss: 0.006606  [ 1600/ 3200]\n",
            "Epoch [33/60], Train Loss: 0.008787707024021074\n",
            "Test Error: \n",
            " Accuracy: 0.74%, Avg loss: 1.0877, F1 Score: 0.7305 \n",
            "\n",
            "Epoch 34\n",
            "-------------------------------\n",
            "loss: 0.000963  [    0/ 3200]\n",
            "loss: 0.025577  [ 1600/ 3200]\n",
            "Epoch [34/60], Train Loss: 0.014658971883909544\n",
            "Test Error: \n",
            " Accuracy: 0.79%, Avg loss: 0.8811, F1 Score: 0.7875 \n",
            "\n",
            "Epoch 35\n",
            "-------------------------------\n",
            "loss: 0.081152  [    0/ 3200]\n",
            "loss: 0.001442  [ 1600/ 3200]\n",
            "Epoch [35/60], Train Loss: 0.01420405824726913\n",
            "Test Error: \n",
            " Accuracy: 0.78%, Avg loss: 0.8859, F1 Score: 0.7772 \n",
            "\n",
            "Epoch 36\n",
            "-------------------------------\n",
            "loss: 0.000673  [    0/ 3200]\n",
            "loss: 0.015239  [ 1600/ 3200]\n",
            "Epoch [36/60], Train Loss: 0.00823562062723795\n",
            "Test Error: \n",
            " Accuracy: 0.78%, Avg loss: 0.8778, F1 Score: 0.7800 \n",
            "\n",
            "Epoch 37\n",
            "-------------------------------\n",
            "loss: 0.000522  [    0/ 3200]\n",
            "loss: 0.001700  [ 1600/ 3200]\n",
            "Epoch [37/60], Train Loss: 0.00749928472112515\n",
            "Test Error: \n",
            " Accuracy: 0.77%, Avg loss: 0.9004, F1 Score: 0.7598 \n",
            "\n",
            "Epoch 38\n",
            "-------------------------------\n",
            "loss: 0.000809  [    0/ 3200]\n",
            "loss: 0.000871  [ 1600/ 3200]\n",
            "Epoch [38/60], Train Loss: 0.006463329213802354\n",
            "Test Error: \n",
            " Accuracy: 0.78%, Avg loss: 0.9222, F1 Score: 0.7748 \n",
            "\n",
            "Epoch 39\n",
            "-------------------------------\n",
            "loss: 0.000860  [    0/ 3200]\n",
            "loss: 0.001973  [ 1600/ 3200]\n",
            "Epoch [39/60], Train Loss: 0.004986268411448691\n",
            "Test Error: \n",
            " Accuracy: 0.79%, Avg loss: 0.9040, F1 Score: 0.7815 \n",
            "\n",
            "Epoch 40\n",
            "-------------------------------\n",
            "loss: 0.001288  [    0/ 3200]\n",
            "loss: 0.002395  [ 1600/ 3200]\n",
            "Epoch [40/60], Train Loss: 0.0076177777149860045\n",
            "Test Error: \n",
            " Accuracy: 0.78%, Avg loss: 0.9499, F1 Score: 0.7686 \n",
            "\n",
            "Epoch 41\n",
            "-------------------------------\n",
            "loss: 0.001189  [    0/ 3200]\n",
            "loss: 0.000174  [ 1600/ 3200]\n",
            "Epoch [41/60], Train Loss: 0.003656658338804846\n",
            "Test Error: \n",
            " Accuracy: 0.77%, Avg loss: 0.9706, F1 Score: 0.7674 \n",
            "\n",
            "Epoch 42\n",
            "-------------------------------\n",
            "loss: 0.000293  [    0/ 3200]\n",
            "loss: 0.000463  [ 1600/ 3200]\n",
            "Epoch [42/60], Train Loss: 0.002154560613380454\n",
            "Test Error: \n",
            " Accuracy: 0.78%, Avg loss: 0.9520, F1 Score: 0.7734 \n",
            "\n",
            "Epoch 43\n",
            "-------------------------------\n",
            "loss: 0.000694  [    0/ 3200]\n",
            "loss: 0.000552  [ 1600/ 3200]\n",
            "Epoch [43/60], Train Loss: 0.004375894092408999\n",
            "Test Error: \n",
            " Accuracy: 0.77%, Avg loss: 1.0565, F1 Score: 0.7600 \n",
            "\n",
            "Epoch 44\n",
            "-------------------------------\n",
            "loss: 0.000256  [    0/ 3200]\n",
            "loss: 0.000301  [ 1600/ 3200]\n",
            "Epoch [44/60], Train Loss: 0.0034727701775409516\n",
            "Test Error: \n",
            " Accuracy: 0.79%, Avg loss: 0.9697, F1 Score: 0.7876 \n",
            "\n",
            "Epoch 45\n",
            "-------------------------------\n",
            "loss: 0.000566  [    0/ 3200]\n",
            "loss: 0.000625  [ 1600/ 3200]\n",
            "Epoch [45/60], Train Loss: 0.007015680394906667\n",
            "Test Error: \n",
            " Accuracy: 0.76%, Avg loss: 1.0363, F1 Score: 0.7558 \n",
            "\n",
            "Epoch 46\n",
            "-------------------------------\n",
            "loss: 0.000379  [    0/ 3200]\n",
            "loss: 0.000287  [ 1600/ 3200]\n",
            "Epoch [46/60], Train Loss: 0.0034348151595622765\n",
            "Test Error: \n",
            " Accuracy: 0.77%, Avg loss: 1.0141, F1 Score: 0.7657 \n",
            "\n",
            "Epoch 47\n",
            "-------------------------------\n",
            "loss: 0.001249  [    0/ 3200]\n",
            "loss: 0.000920  [ 1600/ 3200]\n",
            "Epoch [47/60], Train Loss: 0.0027682153756177284\n",
            "Test Error: \n",
            " Accuracy: 0.76%, Avg loss: 1.0765, F1 Score: 0.7599 \n",
            "\n",
            "Epoch 48\n",
            "-------------------------------\n",
            "loss: 0.004317  [    0/ 3200]\n",
            "loss: 0.000283  [ 1600/ 3200]\n",
            "Epoch [48/60], Train Loss: 0.005655387482293008\n",
            "Test Error: \n",
            " Accuracy: 0.78%, Avg loss: 0.9934, F1 Score: 0.7738 \n",
            "\n",
            "Epoch 49\n",
            "-------------------------------\n",
            "loss: 0.000432  [    0/ 3200]\n",
            "loss: 0.000940  [ 1600/ 3200]\n",
            "Epoch [49/60], Train Loss: 0.0017082792356086429\n",
            "Test Error: \n",
            " Accuracy: 0.78%, Avg loss: 0.9949, F1 Score: 0.7708 \n",
            "\n",
            "Epoch 50\n",
            "-------------------------------\n",
            "loss: 0.004144  [    0/ 3200]\n",
            "loss: 0.000357  [ 1600/ 3200]\n",
            "Epoch [50/60], Train Loss: 0.0013338838588242653\n",
            "Test Error: \n",
            " Accuracy: 0.78%, Avg loss: 1.0142, F1 Score: 0.7693 \n",
            "\n",
            "Epoch 51\n",
            "-------------------------------\n",
            "loss: 0.005247  [    0/ 3200]\n",
            "loss: 0.000541  [ 1600/ 3200]\n",
            "Epoch [51/60], Train Loss: 0.0028908572200816705\n",
            "Test Error: \n",
            " Accuracy: 0.77%, Avg loss: 0.9959, F1 Score: 0.7674 \n",
            "\n",
            "Epoch 52\n",
            "-------------------------------\n",
            "loss: 0.002384  [    0/ 3200]\n",
            "loss: 0.000201  [ 1600/ 3200]\n",
            "Epoch [52/60], Train Loss: 0.0018668762599190814\n",
            "Test Error: \n",
            " Accuracy: 0.78%, Avg loss: 1.0015, F1 Score: 0.7722 \n",
            "\n",
            "Epoch 53\n",
            "-------------------------------\n",
            "loss: 0.000197  [    0/ 3200]\n",
            "loss: 0.000412  [ 1600/ 3200]\n",
            "Epoch [53/60], Train Loss: 0.0010254404239094584\n",
            "Test Error: \n",
            " Accuracy: 0.78%, Avg loss: 0.9929, F1 Score: 0.7784 \n",
            "\n",
            "Epoch 54\n",
            "-------------------------------\n",
            "loss: 0.000279  [    0/ 3200]\n",
            "loss: 0.000629  [ 1600/ 3200]\n",
            "Epoch [54/60], Train Loss: 0.004166380310452951\n",
            "Test Error: \n",
            " Accuracy: 0.78%, Avg loss: 1.0116, F1 Score: 0.7750 \n",
            "\n",
            "Epoch 55\n",
            "-------------------------------\n",
            "loss: 0.000113  [    0/ 3200]\n",
            "loss: 0.000416  [ 1600/ 3200]\n",
            "Epoch [55/60], Train Loss: 0.0011378389028323\n",
            "Test Error: \n",
            " Accuracy: 0.78%, Avg loss: 0.9884, F1 Score: 0.7785 \n",
            "\n",
            "Epoch 56\n",
            "-------------------------------\n",
            "loss: 0.000491  [    0/ 3200]\n",
            "loss: 0.000658  [ 1600/ 3200]\n",
            "Epoch [56/60], Train Loss: 0.0014653482289577369\n",
            "Test Error: \n",
            " Accuracy: 0.79%, Avg loss: 0.9915, F1 Score: 0.7809 \n",
            "\n",
            "Epoch 57\n",
            "-------------------------------\n",
            "loss: 0.000699  [    0/ 3200]\n",
            "loss: 0.000649  [ 1600/ 3200]\n",
            "Epoch [57/60], Train Loss: 0.00322603944419825\n",
            "Test Error: \n",
            " Accuracy: 0.78%, Avg loss: 0.9912, F1 Score: 0.7772 \n",
            "\n",
            "Epoch 58\n",
            "-------------------------------\n",
            "loss: 0.000344  [    0/ 3200]\n",
            "loss: 0.000176  [ 1600/ 3200]\n",
            "Epoch [58/60], Train Loss: 0.0022735042458225507\n",
            "Test Error: \n",
            " Accuracy: 0.78%, Avg loss: 0.9912, F1 Score: 0.7762 \n",
            "\n",
            "Epoch 59\n",
            "-------------------------------\n",
            "loss: 0.000188  [    0/ 3200]\n",
            "loss: 0.000217  [ 1600/ 3200]\n",
            "Epoch [59/60], Train Loss: 0.0018139156583129078\n",
            "Test Error: \n",
            " Accuracy: 0.78%, Avg loss: 0.9971, F1 Score: 0.7771 \n",
            "\n",
            "Epoch 60\n",
            "-------------------------------\n",
            "loss: 0.000133  [    0/ 3200]\n",
            "loss: 0.000481  [ 1600/ 3200]\n",
            "Epoch [60/60], Train Loss: 0.0014168150696787052\n",
            "Test Error: \n",
            " Accuracy: 0.78%, Avg loss: 1.0001, F1 Score: 0.7743 \n",
            "\n",
            "Loading the best model and evaluating on the test set.\n",
            "Linear output: 1024\n",
            "Test Error: \n",
            " Accuracy: 0.78%, Avg loss: 0.8496, F1 Score: 0.7788 \n",
            "\n",
            "Final Test: Accuracy: 0.78, F1: 0.7788\n",
            "Linear output: 1024\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 1.429384  [    0/ 3200]\n",
            "loss: 1.257669  [ 1600/ 3200]\n",
            "Epoch [1/60], Train Loss: 0.8044645568728447\n",
            "Test Error: \n",
            " Accuracy: 0.74%, Avg loss: 0.6900, F1 Score: 0.7426 \n",
            "\n",
            "New best model with F1 score: 0.7425528914985263\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 0.536139  [    0/ 3200]\n",
            "loss: 0.420525  [ 1600/ 3200]\n",
            "Epoch [2/60], Train Loss: 0.6600355057418347\n",
            "Test Error: \n",
            " Accuracy: 0.41%, Avg loss: 1.8302, F1 Score: 0.3564 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 0.678507  [    0/ 3200]\n",
            "loss: 0.439892  [ 1600/ 3200]\n",
            "Epoch [3/60], Train Loss: 0.6121988423168659\n",
            "Test Error: \n",
            " Accuracy: 0.76%, Avg loss: 0.6144, F1 Score: 0.7568 \n",
            "\n",
            "New best model with F1 score: 0.7568230580349377\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 0.552448  [    0/ 3200]\n",
            "loss: 0.286268  [ 1600/ 3200]\n",
            "Epoch [4/60], Train Loss: 0.5659961400926113\n",
            "Test Error: \n",
            " Accuracy: 0.80%, Avg loss: 0.5762, F1 Score: 0.7939 \n",
            "\n",
            "New best model with F1 score: 0.7939181657497454\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 0.635814  [    0/ 3200]\n",
            "loss: 0.699831  [ 1600/ 3200]\n",
            "Epoch [5/60], Train Loss: 0.5142865922302008\n",
            "Test Error: \n",
            " Accuracy: 0.70%, Avg loss: 0.7395, F1 Score: 0.6526 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 0.303083  [    0/ 3200]\n",
            "loss: 0.569735  [ 1600/ 3200]\n",
            "Epoch [6/60], Train Loss: 0.48265689454972743\n",
            "Test Error: \n",
            " Accuracy: 0.45%, Avg loss: 1.6282, F1 Score: 0.4244 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 0.532010  [    0/ 3200]\n",
            "loss: 0.534661  [ 1600/ 3200]\n",
            "Epoch [7/60], Train Loss: 0.470434061139822\n",
            "Test Error: \n",
            " Accuracy: 0.75%, Avg loss: 0.6860, F1 Score: 0.7453 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 0.472689  [    0/ 3200]\n",
            "loss: 0.342562  [ 1600/ 3200]\n",
            "Epoch [8/60], Train Loss: 0.3817845944687724\n",
            "Test Error: \n",
            " Accuracy: 0.71%, Avg loss: 0.8013, F1 Score: 0.6917 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 0.407042  [    0/ 3200]\n",
            "loss: 0.218759  [ 1600/ 3200]\n",
            "Epoch [9/60], Train Loss: 0.3556008140742779\n",
            "Test Error: \n",
            " Accuracy: 0.55%, Avg loss: 1.3469, F1 Score: 0.5252 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 0.453981  [    0/ 3200]\n",
            "loss: 0.227866  [ 1600/ 3200]\n",
            "Epoch [10/60], Train Loss: 0.3207738147303462\n",
            "Test Error: \n",
            " Accuracy: 0.72%, Avg loss: 0.7425, F1 Score: 0.7181 \n",
            "\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "loss: 0.132387  [    0/ 3200]\n",
            "loss: 0.401122  [ 1600/ 3200]\n",
            "Epoch [11/60], Train Loss: 0.24163939900696277\n",
            "Test Error: \n",
            " Accuracy: 0.71%, Avg loss: 0.7894, F1 Score: 0.7171 \n",
            "\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "loss: 0.237203  [    0/ 3200]\n",
            "loss: 0.184964  [ 1600/ 3200]\n",
            "Epoch [12/60], Train Loss: 0.22286044530570506\n",
            "Test Error: \n",
            " Accuracy: 0.72%, Avg loss: 0.9503, F1 Score: 0.6843 \n",
            "\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "loss: 0.079058  [    0/ 3200]\n",
            "loss: 0.333757  [ 1600/ 3200]\n",
            "Epoch [13/60], Train Loss: 0.24843834684230387\n",
            "Test Error: \n",
            " Accuracy: 0.73%, Avg loss: 0.7836, F1 Score: 0.7071 \n",
            "\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "loss: 0.121524  [    0/ 3200]\n",
            "loss: 0.036531  [ 1600/ 3200]\n",
            "Epoch [14/60], Train Loss: 0.16385991809889675\n",
            "Test Error: \n",
            " Accuracy: 0.75%, Avg loss: 0.7687, F1 Score: 0.7449 \n",
            "\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "loss: 0.097675  [    0/ 3200]\n",
            "loss: 0.040540  [ 1600/ 3200]\n",
            "Epoch [15/60], Train Loss: 0.1544655567733571\n",
            "Test Error: \n",
            " Accuracy: 0.78%, Avg loss: 0.6441, F1 Score: 0.7795 \n",
            "\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "loss: 0.056872  [    0/ 3200]\n",
            "loss: 0.083844  [ 1600/ 3200]\n",
            "Epoch [16/60], Train Loss: 0.13175691653508692\n",
            "Test Error: \n",
            " Accuracy: 0.73%, Avg loss: 0.9236, F1 Score: 0.7136 \n",
            "\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "loss: 0.160145  [    0/ 3200]\n",
            "loss: 0.057944  [ 1600/ 3200]\n",
            "Epoch [17/60], Train Loss: 0.11197852471843361\n",
            "Test Error: \n",
            " Accuracy: 0.74%, Avg loss: 0.8468, F1 Score: 0.7275 \n",
            "\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "loss: 0.157581  [    0/ 3200]\n",
            "loss: 0.063217  [ 1600/ 3200]\n",
            "Epoch [18/60], Train Loss: 0.11788596034515648\n",
            "Test Error: \n",
            " Accuracy: 0.77%, Avg loss: 0.7596, F1 Score: 0.7602 \n",
            "\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "loss: 0.087639  [    0/ 3200]\n",
            "loss: 0.021227  [ 1600/ 3200]\n",
            "Epoch [19/60], Train Loss: 0.10400449011940509\n",
            "Test Error: \n",
            " Accuracy: 0.72%, Avg loss: 0.9347, F1 Score: 0.7094 \n",
            "\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "loss: 0.090841  [    0/ 3200]\n",
            "loss: 0.011119  [ 1600/ 3200]\n",
            "Epoch [20/60], Train Loss: 0.11327270640758798\n",
            "Test Error: \n",
            " Accuracy: 0.73%, Avg loss: 0.8872, F1 Score: 0.7047 \n",
            "\n",
            "Epoch 21\n",
            "-------------------------------\n",
            "loss: 0.053534  [    0/ 3200]\n",
            "loss: 0.012639  [ 1600/ 3200]\n",
            "Epoch [21/60], Train Loss: 0.07357177892699837\n",
            "Test Error: \n",
            " Accuracy: 0.71%, Avg loss: 0.9234, F1 Score: 0.7031 \n",
            "\n",
            "Epoch 22\n",
            "-------------------------------\n",
            "loss: 0.077626  [    0/ 3200]\n",
            "loss: 0.015820  [ 1600/ 3200]\n",
            "Epoch [22/60], Train Loss: 0.06284394143149256\n",
            "Test Error: \n",
            " Accuracy: 0.75%, Avg loss: 0.7999, F1 Score: 0.7526 \n",
            "\n",
            "Epoch 23\n",
            "-------------------------------\n",
            "loss: 0.026897  [    0/ 3200]\n",
            "loss: 0.022567  [ 1600/ 3200]\n",
            "Epoch [23/60], Train Loss: 0.057825591313885524\n",
            "Test Error: \n",
            " Accuracy: 0.75%, Avg loss: 0.8602, F1 Score: 0.7323 \n",
            "\n",
            "Epoch 24\n",
            "-------------------------------\n",
            "loss: 0.009001  [    0/ 3200]\n",
            "loss: 0.159833  [ 1600/ 3200]\n",
            "Epoch [24/60], Train Loss: 0.07200253728893585\n",
            "Test Error: \n",
            " Accuracy: 0.76%, Avg loss: 0.8748, F1 Score: 0.7385 \n",
            "\n",
            "Epoch 25\n",
            "-------------------------------\n",
            "loss: 0.009903  [    0/ 3200]\n",
            "loss: 0.008142  [ 1600/ 3200]\n",
            "Epoch [25/60], Train Loss: 0.04528637787909247\n",
            "Test Error: \n",
            " Accuracy: 0.76%, Avg loss: 0.8041, F1 Score: 0.7583 \n",
            "\n",
            "Epoch 26\n",
            "-------------------------------\n",
            "loss: 0.005097  [    0/ 3200]\n",
            "loss: 0.023145  [ 1600/ 3200]\n",
            "Epoch [26/60], Train Loss: 0.0782483403745573\n",
            "Test Error: \n",
            " Accuracy: 0.78%, Avg loss: 0.7243, F1 Score: 0.7660 \n",
            "\n",
            "Epoch 27\n",
            "-------------------------------\n",
            "loss: 0.097134  [    0/ 3200]\n",
            "loss: 0.008908  [ 1600/ 3200]\n",
            "Epoch [27/60], Train Loss: 0.04842890445142985\n",
            "Test Error: \n",
            " Accuracy: 0.76%, Avg loss: 0.8208, F1 Score: 0.7625 \n",
            "\n",
            "Epoch 28\n",
            "-------------------------------\n",
            "loss: 0.017038  [    0/ 3200]\n",
            "loss: 0.015055  [ 1600/ 3200]\n",
            "Epoch [28/60], Train Loss: 0.03560683076852001\n",
            "Test Error: \n",
            " Accuracy: 0.76%, Avg loss: 0.8019, F1 Score: 0.7609 \n",
            "\n",
            "Epoch 29\n",
            "-------------------------------\n",
            "loss: 0.008242  [    0/ 3200]\n",
            "loss: 0.004735  [ 1600/ 3200]\n",
            "Epoch [29/60], Train Loss: 0.04208047406747937\n",
            "Test Error: \n",
            " Accuracy: 0.74%, Avg loss: 0.8733, F1 Score: 0.7514 \n",
            "\n",
            "Epoch 30\n",
            "-------------------------------\n",
            "loss: 0.012142  [    0/ 3200]\n",
            "loss: 0.005357  [ 1600/ 3200]\n",
            "Epoch [30/60], Train Loss: 0.039193171602673826\n",
            "Test Error: \n",
            " Accuracy: 0.78%, Avg loss: 0.8070, F1 Score: 0.7747 \n",
            "\n",
            "Epoch 31\n",
            "-------------------------------\n",
            "loss: 0.005968  [    0/ 3200]\n",
            "loss: 0.007884  [ 1600/ 3200]\n",
            "Epoch [31/60], Train Loss: 0.037997720482526344\n",
            "Test Error: \n",
            " Accuracy: 0.75%, Avg loss: 0.8785, F1 Score: 0.7472 \n",
            "\n",
            "Epoch 32\n",
            "-------------------------------\n",
            "loss: 0.020306  [    0/ 3200]\n",
            "loss: 0.041581  [ 1600/ 3200]\n",
            "Epoch [32/60], Train Loss: 0.02620996446115896\n",
            "Test Error: \n",
            " Accuracy: 0.77%, Avg loss: 0.7859, F1 Score: 0.7641 \n",
            "\n",
            "Epoch 33\n",
            "-------------------------------\n",
            "loss: 0.020390  [    0/ 3200]\n",
            "loss: 0.007537  [ 1600/ 3200]\n",
            "Epoch [33/60], Train Loss: 0.018453750340850093\n",
            "Test Error: \n",
            " Accuracy: 0.77%, Avg loss: 0.8457, F1 Score: 0.7544 \n",
            "\n",
            "Epoch 34\n",
            "-------------------------------\n",
            "loss: 0.033508  [    0/ 3200]\n",
            "loss: 0.069072  [ 1600/ 3200]\n",
            "Epoch [34/60], Train Loss: 0.016766451250296088\n",
            "Test Error: \n",
            " Accuracy: 0.76%, Avg loss: 0.8241, F1 Score: 0.7589 \n",
            "\n",
            "Epoch 35\n",
            "-------------------------------\n",
            "loss: 0.009998  [    0/ 3200]\n",
            "loss: 0.003624  [ 1600/ 3200]\n",
            "Epoch [35/60], Train Loss: 0.016691761784604752\n",
            "Test Error: \n",
            " Accuracy: 0.76%, Avg loss: 0.8294, F1 Score: 0.7566 \n",
            "\n",
            "Epoch 36\n",
            "-------------------------------\n",
            "loss: 0.037728  [    0/ 3200]\n",
            "loss: 0.005709  [ 1600/ 3200]\n",
            "Epoch [36/60], Train Loss: 0.02024161344161257\n",
            "Test Error: \n",
            " Accuracy: 0.76%, Avg loss: 0.7968, F1 Score: 0.7643 \n",
            "\n",
            "Epoch 37\n",
            "-------------------------------\n",
            "loss: 0.005777  [    0/ 3200]\n",
            "loss: 0.006898  [ 1600/ 3200]\n",
            "Epoch [37/60], Train Loss: 0.01962414395355154\n",
            "Test Error: \n",
            " Accuracy: 0.75%, Avg loss: 0.8899, F1 Score: 0.7401 \n",
            "\n",
            "Epoch 38\n",
            "-------------------------------\n",
            "loss: 0.011228  [    0/ 3200]\n",
            "loss: 0.016908  [ 1600/ 3200]\n",
            "Epoch [38/60], Train Loss: 0.009982161725056359\n",
            "Test Error: \n",
            " Accuracy: 0.78%, Avg loss: 0.7400, F1 Score: 0.7771 \n",
            "\n",
            "Epoch 39\n",
            "-------------------------------\n",
            "loss: 0.005528  [    0/ 3200]\n",
            "loss: 0.001856  [ 1600/ 3200]\n",
            "Epoch [39/60], Train Loss: 0.009018591383937746\n",
            "Test Error: \n",
            " Accuracy: 0.79%, Avg loss: 0.7560, F1 Score: 0.7867 \n",
            "\n",
            "Epoch 40\n",
            "-------------------------------\n",
            "loss: 0.025602  [    0/ 3200]\n",
            "loss: 0.003325  [ 1600/ 3200]\n",
            "Epoch [40/60], Train Loss: 0.004630074341548607\n",
            "Test Error: \n",
            " Accuracy: 0.79%, Avg loss: 0.8112, F1 Score: 0.7854 \n",
            "\n",
            "Epoch 41\n",
            "-------------------------------\n",
            "loss: 0.001447  [    0/ 3200]\n",
            "loss: 0.002710  [ 1600/ 3200]\n",
            "Epoch [41/60], Train Loss: 0.005943833776400425\n",
            "Test Error: \n",
            " Accuracy: 0.77%, Avg loss: 0.8811, F1 Score: 0.7551 \n",
            "\n",
            "Epoch 42\n",
            "-------------------------------\n",
            "loss: 0.002553  [    0/ 3200]\n",
            "loss: 0.002002  [ 1600/ 3200]\n",
            "Epoch [42/60], Train Loss: 0.008519238980661613\n",
            "Test Error: \n",
            " Accuracy: 0.78%, Avg loss: 0.7597, F1 Score: 0.7785 \n",
            "\n",
            "Epoch 43\n",
            "-------------------------------\n",
            "loss: 0.001906  [    0/ 3200]\n",
            "loss: 0.005703  [ 1600/ 3200]\n",
            "Epoch [43/60], Train Loss: 0.011103704048437066\n",
            "Test Error: \n",
            " Accuracy: 0.74%, Avg loss: 0.9509, F1 Score: 0.7249 \n",
            "\n",
            "Epoch 44\n",
            "-------------------------------\n",
            "loss: 0.003965  [    0/ 3200]\n",
            "loss: 1.039917  [ 1600/ 3200]\n",
            "Epoch [44/60], Train Loss: 0.01737806692312006\n",
            "Test Error: \n",
            " Accuracy: 0.78%, Avg loss: 0.7816, F1 Score: 0.7726 \n",
            "\n",
            "Epoch 45\n",
            "-------------------------------\n",
            "loss: 0.012428  [    0/ 3200]\n",
            "loss: 0.004953  [ 1600/ 3200]\n",
            "Epoch [45/60], Train Loss: 0.011918807999463751\n",
            "Test Error: \n",
            " Accuracy: 0.78%, Avg loss: 0.8024, F1 Score: 0.7681 \n",
            "\n",
            "Epoch 46\n",
            "-------------------------------\n",
            "loss: 0.001870  [    0/ 3200]\n",
            "loss: 0.002809  [ 1600/ 3200]\n",
            "Epoch [46/60], Train Loss: 0.010744487305055372\n",
            "Test Error: \n",
            " Accuracy: 0.78%, Avg loss: 0.7839, F1 Score: 0.7739 \n",
            "\n",
            "Epoch 47\n",
            "-------------------------------\n",
            "loss: 0.009411  [    0/ 3200]\n",
            "loss: 0.001365  [ 1600/ 3200]\n",
            "Epoch [47/60], Train Loss: 0.010812939722673037\n",
            "Test Error: \n",
            " Accuracy: 0.78%, Avg loss: 0.8032, F1 Score: 0.7752 \n",
            "\n",
            "Epoch 48\n",
            "-------------------------------\n",
            "loss: 0.006995  [    0/ 3200]\n",
            "loss: 0.002410  [ 1600/ 3200]\n",
            "Epoch [48/60], Train Loss: 0.004632015942479484\n",
            "Test Error: \n",
            " Accuracy: 0.79%, Avg loss: 0.7510, F1 Score: 0.7903 \n",
            "\n",
            "Epoch 49\n",
            "-------------------------------\n",
            "loss: 0.002247  [    0/ 3200]\n",
            "loss: 0.002084  [ 1600/ 3200]\n",
            "Epoch [49/60], Train Loss: 0.004121591990697198\n",
            "Test Error: \n",
            " Accuracy: 0.78%, Avg loss: 0.7915, F1 Score: 0.7716 \n",
            "\n",
            "Epoch 50\n",
            "-------------------------------\n",
            "loss: 0.002916  [    0/ 3200]\n",
            "loss: 0.005466  [ 1600/ 3200]\n",
            "Epoch [50/60], Train Loss: 0.0032099612336605786\n",
            "Test Error: \n",
            " Accuracy: 0.80%, Avg loss: 0.7540, F1 Score: 0.7910 \n",
            "\n",
            "Epoch 51\n",
            "-------------------------------\n",
            "loss: 0.001712  [    0/ 3200]\n",
            "loss: 0.001529  [ 1600/ 3200]\n",
            "Epoch [51/60], Train Loss: 0.0029606777409208005\n",
            "Test Error: \n",
            " Accuracy: 0.79%, Avg loss: 0.8030, F1 Score: 0.7770 \n",
            "\n",
            "Epoch 52\n",
            "-------------------------------\n",
            "loss: 0.005375  [    0/ 3200]\n",
            "loss: 0.001946  [ 1600/ 3200]\n",
            "Epoch [52/60], Train Loss: 0.003269586304668337\n",
            "Test Error: \n",
            " Accuracy: 0.79%, Avg loss: 0.7683, F1 Score: 0.7858 \n",
            "\n",
            "Epoch 53\n",
            "-------------------------------\n",
            "loss: 0.002639  [    0/ 3200]\n",
            "loss: 0.002026  [ 1600/ 3200]\n",
            "Epoch [53/60], Train Loss: 0.0034894086548592895\n",
            "Test Error: \n",
            " Accuracy: 0.80%, Avg loss: 0.7434, F1 Score: 0.7916 \n",
            "\n",
            "Epoch 54\n",
            "-------------------------------\n",
            "loss: 0.001504  [    0/ 3200]\n",
            "loss: 0.004542  [ 1600/ 3200]\n",
            "Epoch [54/60], Train Loss: 0.0034131345839705316\n",
            "Test Error: \n",
            " Accuracy: 0.79%, Avg loss: 0.7405, F1 Score: 0.7902 \n",
            "\n",
            "Epoch 55\n",
            "-------------------------------\n",
            "loss: 0.001854  [    0/ 3200]\n",
            "loss: 0.003513  [ 1600/ 3200]\n",
            "Epoch [55/60], Train Loss: 0.0038401696167420594\n",
            "Test Error: \n",
            " Accuracy: 0.79%, Avg loss: 0.7328, F1 Score: 0.7837 \n",
            "\n",
            "Epoch 56\n",
            "-------------------------------\n",
            "loss: 0.160655  [    0/ 3200]\n",
            "loss: 0.001553  [ 1600/ 3200]\n",
            "Epoch [56/60], Train Loss: 0.0035920613602502273\n",
            "Test Error: \n",
            " Accuracy: 0.78%, Avg loss: 0.7821, F1 Score: 0.7793 \n",
            "\n",
            "Epoch 57\n",
            "-------------------------------\n",
            "loss: 0.001775  [    0/ 3200]\n",
            "loss: 0.003076  [ 1600/ 3200]\n",
            "Epoch [57/60], Train Loss: 0.0029541257512755693\n",
            "Test Error: \n",
            " Accuracy: 0.78%, Avg loss: 0.7960, F1 Score: 0.7767 \n",
            "\n",
            "Epoch 58\n",
            "-------------------------------\n",
            "loss: 0.001640  [    0/ 3200]\n",
            "loss: 0.001177  [ 1600/ 3200]\n",
            "Epoch [58/60], Train Loss: 0.0025126485351938755\n",
            "Test Error: \n",
            " Accuracy: 0.80%, Avg loss: 0.7221, F1 Score: 0.7899 \n",
            "\n",
            "Epoch 59\n",
            "-------------------------------\n",
            "loss: 0.002332  [    0/ 3200]\n",
            "loss: 0.001811  [ 1600/ 3200]\n",
            "Epoch [59/60], Train Loss: 0.0034136028771172278\n",
            "Test Error: \n",
            " Accuracy: 0.79%, Avg loss: 0.7703, F1 Score: 0.7853 \n",
            "\n",
            "Epoch 60\n",
            "-------------------------------\n",
            "loss: 0.001732  [    0/ 3200]\n",
            "loss: 0.001709  [ 1600/ 3200]\n",
            "Epoch [60/60], Train Loss: 0.005800733315118123\n",
            "Test Error: \n",
            " Accuracy: 0.79%, Avg loss: 0.7460, F1 Score: 0.7806 \n",
            "\n",
            "Loading the best model and evaluating on the test set.\n",
            "Linear output: 1024\n",
            "Test Error: \n",
            " Accuracy: 0.73%, Avg loss: 0.7108, F1 Score: 0.7217 \n",
            "\n",
            "Final Test: Accuracy: 0.73, F1: 0.7217\n",
            "Linear output: 1024\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 1.480674  [    0/ 3200]\n",
            "loss: 0.671673  [ 1600/ 3200]\n",
            "Epoch [1/60], Train Loss: 0.8742865772545337\n",
            "Test Error: \n",
            " Accuracy: 0.73%, Avg loss: 0.7450, F1 Score: 0.7341 \n",
            "\n",
            "New best model with F1 score: 0.734125005471294\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 0.544837  [    0/ 3200]\n",
            "loss: 0.698044  [ 1600/ 3200]\n",
            "Epoch [2/60], Train Loss: 0.6840399996936322\n",
            "Test Error: \n",
            " Accuracy: 0.67%, Avg loss: 0.7702, F1 Score: 0.6408 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 0.699539  [    0/ 3200]\n",
            "loss: 0.740755  [ 1600/ 3200]\n",
            "Epoch [3/60], Train Loss: 0.638246478587389\n",
            "Test Error: \n",
            " Accuracy: 0.60%, Avg loss: 1.0192, F1 Score: 0.5327 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 0.728838  [    0/ 3200]\n",
            "loss: 0.413355  [ 1600/ 3200]\n",
            "Epoch [4/60], Train Loss: 0.5605598244071007\n",
            "Test Error: \n",
            " Accuracy: 0.40%, Avg loss: 2.0293, F1 Score: 0.3251 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 0.229142  [    0/ 3200]\n",
            "loss: 0.472491  [ 1600/ 3200]\n",
            "Epoch [5/60], Train Loss: 0.5238048023730516\n",
            "Test Error: \n",
            " Accuracy: 0.72%, Avg loss: 0.7022, F1 Score: 0.7268 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 0.344509  [    0/ 3200]\n",
            "loss: 0.688977  [ 1600/ 3200]\n",
            "Epoch [6/60], Train Loss: 0.46076928205788137\n",
            "Test Error: \n",
            " Accuracy: 0.77%, Avg loss: 0.6029, F1 Score: 0.7565 \n",
            "\n",
            "New best model with F1 score: 0.7565266338246652\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 0.630337  [    0/ 3200]\n",
            "loss: 0.558344  [ 1600/ 3200]\n",
            "Epoch [7/60], Train Loss: 0.4115734279155731\n",
            "Test Error: \n",
            " Accuracy: 0.73%, Avg loss: 0.6711, F1 Score: 0.7406 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 0.227986  [    0/ 3200]\n",
            "loss: 1.122627  [ 1600/ 3200]\n",
            "Epoch [8/60], Train Loss: 0.3744319624453783\n",
            "Test Error: \n",
            " Accuracy: 0.74%, Avg loss: 0.6281, F1 Score: 0.7362 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 0.206048  [    0/ 3200]\n",
            "loss: 0.628141  [ 1600/ 3200]\n",
            "Epoch [9/60], Train Loss: 0.3232496383227408\n",
            "Test Error: \n",
            " Accuracy: 0.69%, Avg loss: 0.8445, F1 Score: 0.7065 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 0.386154  [    0/ 3200]\n",
            "loss: 0.215739  [ 1600/ 3200]\n",
            "Epoch [10/60], Train Loss: 0.27109569411724804\n",
            "Test Error: \n",
            " Accuracy: 0.76%, Avg loss: 0.6623, F1 Score: 0.7549 \n",
            "\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "loss: 0.179059  [    0/ 3200]\n",
            "loss: 0.124282  [ 1600/ 3200]\n",
            "Epoch [11/60], Train Loss: 0.2513559712097049\n",
            "Test Error: \n",
            " Accuracy: 0.76%, Avg loss: 0.7076, F1 Score: 0.7508 \n",
            "\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "loss: 0.284948  [    0/ 3200]\n",
            "loss: 0.236134  [ 1600/ 3200]\n",
            "Epoch [12/60], Train Loss: 0.23448841438628734\n",
            "Test Error: \n",
            " Accuracy: 0.71%, Avg loss: 0.8440, F1 Score: 0.7177 \n",
            "\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "loss: 0.156967  [    0/ 3200]\n",
            "loss: 0.043284  [ 1600/ 3200]\n",
            "Epoch [13/60], Train Loss: 0.17024348018690943\n",
            "Test Error: \n",
            " Accuracy: 0.76%, Avg loss: 0.7576, F1 Score: 0.7411 \n",
            "\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "loss: 0.056371  [    0/ 3200]\n",
            "loss: 0.024018  [ 1600/ 3200]\n",
            "Epoch [14/60], Train Loss: 0.14739827304147185\n",
            "Test Error: \n",
            " Accuracy: 0.76%, Avg loss: 0.7866, F1 Score: 0.7422 \n",
            "\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "loss: 0.625788  [    0/ 3200]\n",
            "loss: 0.104131  [ 1600/ 3200]\n",
            "Epoch [15/60], Train Loss: 0.13000491383019835\n",
            "Test Error: \n",
            " Accuracy: 0.79%, Avg loss: 0.6336, F1 Score: 0.7867 \n",
            "\n",
            "New best model with F1 score: 0.7867314931272519\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "loss: 0.042904  [    0/ 3200]\n",
            "loss: 0.014445  [ 1600/ 3200]\n",
            "Epoch [16/60], Train Loss: 0.12526293627452106\n",
            "Test Error: \n",
            " Accuracy: 0.66%, Avg loss: 1.1556, F1 Score: 0.6413 \n",
            "\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "loss: 0.176548  [    0/ 3200]\n",
            "loss: 0.073721  [ 1600/ 3200]\n",
            "Epoch [17/60], Train Loss: 0.1113671202142723\n",
            "Test Error: \n",
            " Accuracy: 0.75%, Avg loss: 0.8095, F1 Score: 0.7433 \n",
            "\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "loss: 0.129879  [    0/ 3200]\n",
            "loss: 0.192737  [ 1600/ 3200]\n",
            "Epoch [18/60], Train Loss: 0.11754945430438965\n",
            "Test Error: \n",
            " Accuracy: 0.74%, Avg loss: 0.9242, F1 Score: 0.7499 \n",
            "\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "loss: 0.082001  [    0/ 3200]\n",
            "loss: 0.015789  [ 1600/ 3200]\n",
            "Epoch [19/60], Train Loss: 0.06698340812930838\n",
            "Test Error: \n",
            " Accuracy: 0.74%, Avg loss: 0.9531, F1 Score: 0.7322 \n",
            "\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "loss: 0.107155  [    0/ 3200]\n",
            "loss: 0.008137  [ 1600/ 3200]\n",
            "Epoch [20/60], Train Loss: 0.09761216950719245\n",
            "Test Error: \n",
            " Accuracy: 0.73%, Avg loss: 0.9624, F1 Score: 0.7305 \n",
            "\n",
            "Epoch 21\n",
            "-------------------------------\n",
            "loss: 0.038220  [    0/ 3200]\n",
            "loss: 0.034047  [ 1600/ 3200]\n",
            "Epoch [21/60], Train Loss: 0.0788972461328376\n",
            "Test Error: \n",
            " Accuracy: 0.76%, Avg loss: 0.8111, F1 Score: 0.7611 \n",
            "\n",
            "Epoch 22\n",
            "-------------------------------\n",
            "loss: 0.012062  [    0/ 3200]\n",
            "loss: 0.012877  [ 1600/ 3200]\n",
            "Epoch [22/60], Train Loss: 0.06746227741474285\n",
            "Test Error: \n",
            " Accuracy: 0.75%, Avg loss: 0.9097, F1 Score: 0.7460 \n",
            "\n",
            "Epoch 23\n",
            "-------------------------------\n",
            "loss: 0.010479  [    0/ 3200]\n",
            "loss: 0.006559  [ 1600/ 3200]\n",
            "Epoch [23/60], Train Loss: 0.0710244142904412\n",
            "Test Error: \n",
            " Accuracy: 0.70%, Avg loss: 1.1146, F1 Score: 0.6782 \n",
            "\n",
            "Epoch 24\n",
            "-------------------------------\n",
            "loss: 0.010514  [    0/ 3200]\n",
            "loss: 0.011879  [ 1600/ 3200]\n",
            "Epoch [24/60], Train Loss: 0.045940523652825506\n",
            "Test Error: \n",
            " Accuracy: 0.76%, Avg loss: 0.8806, F1 Score: 0.7541 \n",
            "\n",
            "Epoch 25\n",
            "-------------------------------\n",
            "loss: 0.008518  [    0/ 3200]\n",
            "loss: 0.049116  [ 1600/ 3200]\n",
            "Epoch [25/60], Train Loss: 0.04980425816203933\n",
            "Test Error: \n",
            " Accuracy: 0.75%, Avg loss: 0.8549, F1 Score: 0.7488 \n",
            "\n",
            "Epoch 26\n",
            "-------------------------------\n",
            "loss: 0.268602  [    0/ 3200]\n",
            "loss: 0.059219  [ 1600/ 3200]\n",
            "Epoch [26/60], Train Loss: 0.040332937822677196\n",
            "Test Error: \n",
            " Accuracy: 0.75%, Avg loss: 0.9593, F1 Score: 0.7541 \n",
            "\n",
            "Epoch 27\n",
            "-------------------------------\n",
            "loss: 0.002305  [    0/ 3200]\n",
            "loss: 0.013145  [ 1600/ 3200]\n",
            "Epoch [27/60], Train Loss: 0.03750722539611161\n",
            "Test Error: \n",
            " Accuracy: 0.73%, Avg loss: 1.0793, F1 Score: 0.7166 \n",
            "\n",
            "Epoch 28\n",
            "-------------------------------\n",
            "loss: 0.008522  [    0/ 3200]\n",
            "loss: 0.001889  [ 1600/ 3200]\n",
            "Epoch [28/60], Train Loss: 0.0379050859293784\n",
            "Test Error: \n",
            " Accuracy: 0.70%, Avg loss: 1.0712, F1 Score: 0.6994 \n",
            "\n",
            "Epoch 29\n",
            "-------------------------------\n",
            "loss: 0.083609  [    0/ 3200]\n",
            "loss: 0.003463  [ 1600/ 3200]\n",
            "Epoch [29/60], Train Loss: 0.036737655338656625\n",
            "Test Error: \n",
            " Accuracy: 0.73%, Avg loss: 1.0245, F1 Score: 0.7303 \n",
            "\n",
            "Epoch 30\n",
            "-------------------------------\n",
            "loss: 0.006418  [    0/ 3200]\n",
            "loss: 0.011780  [ 1600/ 3200]\n",
            "Epoch [30/60], Train Loss: 0.028732148564595262\n",
            "Test Error: \n",
            " Accuracy: 0.77%, Avg loss: 0.9528, F1 Score: 0.7638 \n",
            "\n",
            "Epoch 31\n",
            "-------------------------------\n",
            "loss: 0.112932  [    0/ 3200]\n",
            "loss: 0.006844  [ 1600/ 3200]\n",
            "Epoch [31/60], Train Loss: 0.029759611016488635\n",
            "Test Error: \n",
            " Accuracy: 0.74%, Avg loss: 0.9782, F1 Score: 0.7390 \n",
            "\n",
            "Epoch 32\n",
            "-------------------------------\n",
            "loss: 0.003413  [    0/ 3200]\n",
            "loss: 0.016552  [ 1600/ 3200]\n",
            "Epoch [32/60], Train Loss: 0.02584679710358614\n",
            "Test Error: \n",
            " Accuracy: 0.75%, Avg loss: 0.9419, F1 Score: 0.7492 \n",
            "\n",
            "Epoch 33\n",
            "-------------------------------\n",
            "loss: 0.015040  [    0/ 3200]\n",
            "loss: 0.015026  [ 1600/ 3200]\n",
            "Epoch [33/60], Train Loss: 0.026899083688040264\n",
            "Test Error: \n",
            " Accuracy: 0.78%, Avg loss: 0.8428, F1 Score: 0.7779 \n",
            "\n",
            "Epoch 34\n",
            "-------------------------------\n",
            "loss: 0.002933  [    0/ 3200]\n",
            "loss: 0.323881  [ 1600/ 3200]\n",
            "Epoch [34/60], Train Loss: 0.02604694599081995\n",
            "Test Error: \n",
            " Accuracy: 0.78%, Avg loss: 0.9158, F1 Score: 0.7707 \n",
            "\n",
            "Epoch 35\n",
            "-------------------------------\n",
            "loss: 0.005506  [    0/ 3200]\n",
            "loss: 0.001428  [ 1600/ 3200]\n",
            "Epoch [35/60], Train Loss: 0.016550087373761926\n",
            "Test Error: \n",
            " Accuracy: 0.77%, Avg loss: 1.0290, F1 Score: 0.7621 \n",
            "\n",
            "Epoch 36\n",
            "-------------------------------\n",
            "loss: 0.004553  [    0/ 3200]\n",
            "loss: 0.000841  [ 1600/ 3200]\n",
            "Epoch [36/60], Train Loss: 0.01913004151036148\n",
            "Test Error: \n",
            " Accuracy: 0.77%, Avg loss: 0.9287, F1 Score: 0.7687 \n",
            "\n",
            "Epoch 37\n",
            "-------------------------------\n",
            "loss: 0.007626  [    0/ 3200]\n",
            "loss: 0.019947  [ 1600/ 3200]\n",
            "Epoch [37/60], Train Loss: 0.009186626403970877\n",
            "Test Error: \n",
            " Accuracy: 0.76%, Avg loss: 0.9595, F1 Score: 0.7579 \n",
            "\n",
            "Epoch 38\n",
            "-------------------------------\n",
            "loss: 0.003820  [    0/ 3200]\n",
            "loss: 0.019519  [ 1600/ 3200]\n",
            "Epoch [38/60], Train Loss: 0.00881159186406876\n",
            "Test Error: \n",
            " Accuracy: 0.77%, Avg loss: 0.9640, F1 Score: 0.7683 \n",
            "\n",
            "Epoch 39\n",
            "-------------------------------\n",
            "loss: 0.016334  [    0/ 3200]\n",
            "loss: 0.000528  [ 1600/ 3200]\n",
            "Epoch [39/60], Train Loss: 0.011049587504239754\n",
            "Test Error: \n",
            " Accuracy: 0.76%, Avg loss: 0.9193, F1 Score: 0.7625 \n",
            "\n",
            "Epoch 40\n",
            "-------------------------------\n",
            "loss: 0.008824  [    0/ 3200]\n",
            "loss: 0.082571  [ 1600/ 3200]\n",
            "Epoch [40/60], Train Loss: 0.020079535345794285\n",
            "Test Error: \n",
            " Accuracy: 0.78%, Avg loss: 0.9092, F1 Score: 0.7750 \n",
            "\n",
            "Epoch 41\n",
            "-------------------------------\n",
            "loss: 0.001081  [    0/ 3200]\n",
            "loss: 0.001159  [ 1600/ 3200]\n",
            "Epoch [41/60], Train Loss: 0.012343559883447597\n",
            "Test Error: \n",
            " Accuracy: 0.79%, Avg loss: 0.9052, F1 Score: 0.7897 \n",
            "\n",
            "New best model with F1 score: 0.7896859587660908\n",
            "Epoch 42\n",
            "-------------------------------\n",
            "loss: 0.001788  [    0/ 3200]\n",
            "loss: 0.000375  [ 1600/ 3200]\n",
            "Epoch [42/60], Train Loss: 0.018643118047039026\n",
            "Test Error: \n",
            " Accuracy: 0.78%, Avg loss: 0.9379, F1 Score: 0.7766 \n",
            "\n",
            "Epoch 43\n",
            "-------------------------------\n",
            "loss: 0.009677  [    0/ 3200]\n",
            "loss: 0.001083  [ 1600/ 3200]\n",
            "Epoch [43/60], Train Loss: 0.009269562445479097\n",
            "Test Error: \n",
            " Accuracy: 0.79%, Avg loss: 0.9198, F1 Score: 0.7904 \n",
            "\n",
            "New best model with F1 score: 0.7903886462370754\n",
            "Epoch 44\n",
            "-------------------------------\n",
            "loss: 0.000667  [    0/ 3200]\n",
            "loss: 0.000287  [ 1600/ 3200]\n",
            "Epoch [44/60], Train Loss: 0.015231926050473703\n",
            "Test Error: \n",
            " Accuracy: 0.79%, Avg loss: 0.9439, F1 Score: 0.7855 \n",
            "\n",
            "Epoch 45\n",
            "-------------------------------\n",
            "loss: 0.000564  [    0/ 3200]\n",
            "loss: 0.001938  [ 1600/ 3200]\n",
            "Epoch [45/60], Train Loss: 0.006716059341852087\n",
            "Test Error: \n",
            " Accuracy: 0.78%, Avg loss: 0.9757, F1 Score: 0.7728 \n",
            "\n",
            "Epoch 46\n",
            "-------------------------------\n",
            "loss: 0.005314  [    0/ 3200]\n",
            "loss: 0.001185  [ 1600/ 3200]\n",
            "Epoch [46/60], Train Loss: 0.010793731903177103\n",
            "Test Error: \n",
            " Accuracy: 0.79%, Avg loss: 0.9343, F1 Score: 0.7880 \n",
            "\n",
            "Epoch 47\n",
            "-------------------------------\n",
            "loss: 0.025731  [    0/ 3200]\n",
            "loss: 0.481186  [ 1600/ 3200]\n",
            "Epoch [47/60], Train Loss: 0.01314325334256864\n",
            "Test Error: \n",
            " Accuracy: 0.78%, Avg loss: 0.9272, F1 Score: 0.7818 \n",
            "\n",
            "Epoch 48\n",
            "-------------------------------\n",
            "loss: 0.001503  [    0/ 3200]\n",
            "loss: 0.034197  [ 1600/ 3200]\n",
            "Epoch [48/60], Train Loss: 0.007328668696936802\n",
            "Test Error: \n",
            " Accuracy: 0.78%, Avg loss: 0.9941, F1 Score: 0.7723 \n",
            "\n",
            "Epoch 49\n",
            "-------------------------------\n",
            "loss: 0.002472  [    0/ 3200]\n",
            "loss: 0.013292  [ 1600/ 3200]\n",
            "Epoch [49/60], Train Loss: 0.008625561448789086\n",
            "Test Error: \n",
            " Accuracy: 0.79%, Avg loss: 0.9408, F1 Score: 0.7881 \n",
            "\n",
            "Epoch 50\n",
            "-------------------------------\n",
            "loss: 0.003009  [    0/ 3200]\n",
            "loss: 0.001102  [ 1600/ 3200]\n",
            "Epoch [50/60], Train Loss: 0.00478438579953945\n",
            "Test Error: \n",
            " Accuracy: 0.80%, Avg loss: 0.9425, F1 Score: 0.7915 \n",
            "\n",
            "New best model with F1 score: 0.7914671450574172\n",
            "Epoch 51\n",
            "-------------------------------\n",
            "loss: 0.004100  [    0/ 3200]\n",
            "loss: 0.001710  [ 1600/ 3200]\n",
            "Epoch [51/60], Train Loss: 0.005281918291220791\n",
            "Test Error: \n",
            " Accuracy: 0.79%, Avg loss: 0.9712, F1 Score: 0.7836 \n",
            "\n",
            "Epoch 52\n",
            "-------------------------------\n",
            "loss: 0.001081  [    0/ 3200]\n",
            "loss: 0.000373  [ 1600/ 3200]\n",
            "Epoch [52/60], Train Loss: 0.011528363488141622\n",
            "Test Error: \n",
            " Accuracy: 0.79%, Avg loss: 0.9444, F1 Score: 0.7846 \n",
            "\n",
            "Epoch 53\n",
            "-------------------------------\n",
            "loss: 0.001392  [    0/ 3200]\n",
            "loss: 0.000529  [ 1600/ 3200]\n",
            "Epoch [53/60], Train Loss: 0.008399322554760147\n",
            "Test Error: \n",
            " Accuracy: 0.78%, Avg loss: 0.9685, F1 Score: 0.7830 \n",
            "\n",
            "Epoch 54\n",
            "-------------------------------\n",
            "loss: 0.001322  [    0/ 3200]\n",
            "loss: 0.004518  [ 1600/ 3200]\n",
            "Epoch [54/60], Train Loss: 0.007562033530848567\n",
            "Test Error: \n",
            " Accuracy: 0.80%, Avg loss: 0.9361, F1 Score: 0.7931 \n",
            "\n",
            "New best model with F1 score: 0.7931237523503978\n",
            "Epoch 55\n",
            "-------------------------------\n",
            "loss: 0.000073  [    0/ 3200]\n",
            "loss: 0.001857  [ 1600/ 3200]\n",
            "Epoch [55/60], Train Loss: 0.003244180233705265\n",
            "Test Error: \n",
            " Accuracy: 0.79%, Avg loss: 0.9282, F1 Score: 0.7899 \n",
            "\n",
            "Epoch 56\n",
            "-------------------------------\n",
            "loss: 0.001126  [    0/ 3200]\n",
            "loss: 0.008714  [ 1600/ 3200]\n",
            "Epoch [56/60], Train Loss: 0.005080316744861193\n",
            "Test Error: \n",
            " Accuracy: 0.78%, Avg loss: 0.9893, F1 Score: 0.7800 \n",
            "\n",
            "Epoch 57\n",
            "-------------------------------\n",
            "loss: 0.001846  [    0/ 3200]\n",
            "loss: 0.000261  [ 1600/ 3200]\n",
            "Epoch [57/60], Train Loss: 0.004894981981342425\n",
            "Test Error: \n",
            " Accuracy: 0.79%, Avg loss: 0.9391, F1 Score: 0.7839 \n",
            "\n",
            "Epoch 58\n",
            "-------------------------------\n",
            "loss: 0.000644  [    0/ 3200]\n",
            "loss: 0.003495  [ 1600/ 3200]\n",
            "Epoch [58/60], Train Loss: 0.0031741735141622486\n",
            "Test Error: \n",
            " Accuracy: 0.78%, Avg loss: 0.9881, F1 Score: 0.7756 \n",
            "\n",
            "Epoch 59\n",
            "-------------------------------\n",
            "loss: 0.000814  [    0/ 3200]\n",
            "loss: 0.005097  [ 1600/ 3200]\n",
            "Epoch [59/60], Train Loss: 0.005122761472375714\n",
            "Test Error: \n",
            " Accuracy: 0.78%, Avg loss: 0.9375, F1 Score: 0.7816 \n",
            "\n",
            "Epoch 60\n",
            "-------------------------------\n",
            "loss: 0.000316  [    0/ 3200]\n",
            "loss: 0.003298  [ 1600/ 3200]\n",
            "Epoch [60/60], Train Loss: 0.004592390660000092\n",
            "Test Error: \n",
            " Accuracy: 0.78%, Avg loss: 0.9479, F1 Score: 0.7791 \n",
            "\n",
            "Loading the best model and evaluating on the test set.\n",
            "Linear output: 1024\n",
            "Test Error: \n",
            " Accuracy: 0.75%, Avg loss: 1.2001, F1 Score: 0.7441 \n",
            "\n",
            "Final Test: Accuracy: 0.75, F1: 0.7441\n",
            "Linear output: 1024\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 1.428154  [    0/ 3200]\n",
            "loss: 0.856005  [ 1600/ 3200]\n",
            "Epoch [1/60], Train Loss: 0.8204298987984657\n",
            "Test Error: \n",
            " Accuracy: 0.69%, Avg loss: 0.7159, F1 Score: 0.6722 \n",
            "\n",
            "New best model with F1 score: 0.6721948461516507\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 1.073569  [    0/ 3200]\n",
            "loss: 0.580927  [ 1600/ 3200]\n",
            "Epoch [2/60], Train Loss: 0.6674769639968872\n",
            "Test Error: \n",
            " Accuracy: 0.74%, Avg loss: 0.6409, F1 Score: 0.7392 \n",
            "\n",
            "New best model with F1 score: 0.7391759260817874\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 0.925395  [    0/ 3200]\n",
            "loss: 0.542116  [ 1600/ 3200]\n",
            "Epoch [3/60], Train Loss: 0.6170275737345219\n",
            "Test Error: \n",
            " Accuracy: 0.70%, Avg loss: 0.7023, F1 Score: 0.7041 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 0.500559  [    0/ 3200]\n",
            "loss: 0.271440  [ 1600/ 3200]\n",
            "Epoch [4/60], Train Loss: 0.5783630193769932\n",
            "Test Error: \n",
            " Accuracy: 0.74%, Avg loss: 0.6422, F1 Score: 0.7276 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 0.476325  [    0/ 3200]\n",
            "loss: 0.412308  [ 1600/ 3200]\n",
            "Epoch [5/60], Train Loss: 0.5529410842806101\n",
            "Test Error: \n",
            " Accuracy: 0.50%, Avg loss: 1.3781, F1 Score: 0.4759 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 0.419628  [    0/ 3200]\n",
            "loss: 1.090796  [ 1600/ 3200]\n",
            "Epoch [6/60], Train Loss: 0.5052864727377891\n",
            "Test Error: \n",
            " Accuracy: 0.74%, Avg loss: 0.7257, F1 Score: 0.7380 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 0.439340  [    0/ 3200]\n",
            "loss: 0.528172  [ 1600/ 3200]\n",
            "Epoch [7/60], Train Loss: 0.45614235926419494\n",
            "Test Error: \n",
            " Accuracy: 0.73%, Avg loss: 0.6578, F1 Score: 0.7095 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 0.243168  [    0/ 3200]\n",
            "loss: 0.407708  [ 1600/ 3200]\n",
            "Epoch [8/60], Train Loss: 0.4281494948640466\n",
            "Test Error: \n",
            " Accuracy: 0.62%, Avg loss: 1.3239, F1 Score: 0.5671 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 0.284096  [    0/ 3200]\n",
            "loss: 0.231818  [ 1600/ 3200]\n",
            "Epoch [9/60], Train Loss: 0.3648775612935424\n",
            "Test Error: \n",
            " Accuracy: 0.64%, Avg loss: 1.0052, F1 Score: 0.6008 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 0.260193  [    0/ 3200]\n",
            "loss: 0.141425  [ 1600/ 3200]\n",
            "Epoch [10/60], Train Loss: 0.36240896008908746\n",
            "Test Error: \n",
            " Accuracy: 0.68%, Avg loss: 0.9763, F1 Score: 0.6341 \n",
            "\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "loss: 0.193882  [    0/ 3200]\n",
            "loss: 0.426706  [ 1600/ 3200]\n",
            "Epoch [11/60], Train Loss: 0.31059094563126566\n",
            "Test Error: \n",
            " Accuracy: 0.72%, Avg loss: 0.7773, F1 Score: 0.6915 \n",
            "\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "loss: 0.096846  [    0/ 3200]\n",
            "loss: 0.171161  [ 1600/ 3200]\n",
            "Epoch [12/60], Train Loss: 0.276400255728513\n",
            "Test Error: \n",
            " Accuracy: 0.70%, Avg loss: 0.8395, F1 Score: 0.6932 \n",
            "\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "loss: 0.174247  [    0/ 3200]\n",
            "loss: 0.239445  [ 1600/ 3200]\n",
            "Epoch [13/60], Train Loss: 0.23098472125828265\n",
            "Test Error: \n",
            " Accuracy: 0.74%, Avg loss: 0.7262, F1 Score: 0.7209 \n",
            "\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "loss: 0.038283  [    0/ 3200]\n",
            "loss: 0.064237  [ 1600/ 3200]\n",
            "Epoch [14/60], Train Loss: 0.22978790545836092\n",
            "Test Error: \n",
            " Accuracy: 0.53%, Avg loss: 1.4054, F1 Score: 0.4963 \n",
            "\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "loss: 0.456197  [    0/ 3200]\n",
            "loss: 0.261255  [ 1600/ 3200]\n",
            "Epoch [15/60], Train Loss: 0.18601100414991378\n",
            "Test Error: \n",
            " Accuracy: 0.71%, Avg loss: 0.8847, F1 Score: 0.6916 \n",
            "\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "loss: 0.090072  [    0/ 3200]\n",
            "loss: 0.056689  [ 1600/ 3200]\n",
            "Epoch [16/60], Train Loss: 0.1791508519463241\n",
            "Test Error: \n",
            " Accuracy: 0.65%, Avg loss: 0.9997, F1 Score: 0.6456 \n",
            "\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "loss: 0.367633  [    0/ 3200]\n",
            "loss: 0.058886  [ 1600/ 3200]\n",
            "Epoch [17/60], Train Loss: 0.16473395030014218\n",
            "Test Error: \n",
            " Accuracy: 0.65%, Avg loss: 1.3630, F1 Score: 0.6249 \n",
            "\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "loss: 0.058927  [    0/ 3200]\n",
            "loss: 0.417285  [ 1600/ 3200]\n",
            "Epoch [18/60], Train Loss: 0.14682609274052083\n",
            "Test Error: \n",
            " Accuracy: 0.75%, Avg loss: 0.9294, F1 Score: 0.7163 \n",
            "\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "loss: 0.445896  [    0/ 3200]\n",
            "loss: 0.032270  [ 1600/ 3200]\n",
            "Epoch [19/60], Train Loss: 0.14551988875959068\n",
            "Test Error: \n",
            " Accuracy: 0.75%, Avg loss: 0.8662, F1 Score: 0.7368 \n",
            "\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "loss: 0.138465  [    0/ 3200]\n",
            "loss: 1.021441  [ 1600/ 3200]\n",
            "Epoch [20/60], Train Loss: 0.12486557858530432\n",
            "Test Error: \n",
            " Accuracy: 0.71%, Avg loss: 0.9994, F1 Score: 0.7119 \n",
            "\n",
            "Epoch 21\n",
            "-------------------------------\n",
            "loss: 0.027738  [    0/ 3200]\n",
            "loss: 0.023083  [ 1600/ 3200]\n",
            "Epoch [21/60], Train Loss: 0.1043084733048454\n",
            "Test Error: \n",
            " Accuracy: 0.77%, Avg loss: 0.7687, F1 Score: 0.7630 \n",
            "\n",
            "New best model with F1 score: 0.7630426609002544\n",
            "Epoch 22\n",
            "-------------------------------\n",
            "loss: 0.048490  [    0/ 3200]\n",
            "loss: 0.031522  [ 1600/ 3200]\n",
            "Epoch [22/60], Train Loss: 0.11251518671168015\n",
            "Test Error: \n",
            " Accuracy: 0.73%, Avg loss: 0.9264, F1 Score: 0.7180 \n",
            "\n",
            "Epoch 23\n",
            "-------------------------------\n",
            "loss: 0.178623  [    0/ 3200]\n",
            "loss: 0.014341  [ 1600/ 3200]\n",
            "Epoch [23/60], Train Loss: 0.10018747153924779\n",
            "Test Error: \n",
            " Accuracy: 0.73%, Avg loss: 0.8242, F1 Score: 0.7367 \n",
            "\n",
            "Epoch 24\n",
            "-------------------------------\n",
            "loss: 0.028420  [    0/ 3200]\n",
            "loss: 0.006884  [ 1600/ 3200]\n",
            "Epoch [24/60], Train Loss: 0.08635050513315946\n",
            "Test Error: \n",
            " Accuracy: 0.78%, Avg loss: 0.8167, F1 Score: 0.7674 \n",
            "\n",
            "New best model with F1 score: 0.7674139500573274\n",
            "Epoch 25\n",
            "-------------------------------\n",
            "loss: 0.019766  [    0/ 3200]\n",
            "loss: 0.582728  [ 1600/ 3200]\n",
            "Epoch [25/60], Train Loss: 0.07839843534864485\n",
            "Test Error: \n",
            " Accuracy: 0.77%, Avg loss: 0.7791, F1 Score: 0.7731 \n",
            "\n",
            "New best model with F1 score: 0.7731274316871457\n",
            "Epoch 26\n",
            "-------------------------------\n",
            "loss: 0.007517  [    0/ 3200]\n",
            "loss: 0.069031  [ 1600/ 3200]\n",
            "Epoch [26/60], Train Loss: 0.0926158408424817\n",
            "Test Error: \n",
            " Accuracy: 0.70%, Avg loss: 1.0665, F1 Score: 0.7029 \n",
            "\n",
            "Epoch 27\n",
            "-------------------------------\n",
            "loss: 0.575941  [    0/ 3200]\n",
            "loss: 0.024307  [ 1600/ 3200]\n",
            "Epoch [27/60], Train Loss: 0.06962918834295123\n",
            "Test Error: \n",
            " Accuracy: 0.78%, Avg loss: 0.8126, F1 Score: 0.7762 \n",
            "\n",
            "New best model with F1 score: 0.7762112164524598\n",
            "Epoch 28\n",
            "-------------------------------\n",
            "loss: 0.055066  [    0/ 3200]\n",
            "loss: 0.715612  [ 1600/ 3200]\n",
            "Epoch [28/60], Train Loss: 0.09242170899640768\n",
            "Test Error: \n",
            " Accuracy: 0.75%, Avg loss: 0.9127, F1 Score: 0.7467 \n",
            "\n",
            "Epoch 29\n",
            "-------------------------------\n",
            "loss: 0.012323  [    0/ 3200]\n",
            "loss: 0.022421  [ 1600/ 3200]\n",
            "Epoch [29/60], Train Loss: 0.05775403279112652\n",
            "Test Error: \n",
            " Accuracy: 0.75%, Avg loss: 0.8875, F1 Score: 0.7407 \n",
            "\n",
            "Epoch 30\n",
            "-------------------------------\n",
            "loss: 0.064754  [    0/ 3200]\n",
            "loss: 0.009179  [ 1600/ 3200]\n",
            "Epoch [30/60], Train Loss: 0.05067161747720093\n",
            "Test Error: \n",
            " Accuracy: 0.78%, Avg loss: 0.8141, F1 Score: 0.7790 \n",
            "\n",
            "New best model with F1 score: 0.7790375279335143\n",
            "Epoch 31\n",
            "-------------------------------\n",
            "loss: 0.016931  [    0/ 3200]\n",
            "loss: 0.037120  [ 1600/ 3200]\n",
            "Epoch [31/60], Train Loss: 0.049834435947705064\n",
            "Test Error: \n",
            " Accuracy: 0.77%, Avg loss: 0.8529, F1 Score: 0.7553 \n",
            "\n",
            "Epoch 32\n",
            "-------------------------------\n",
            "loss: 0.007202  [    0/ 3200]\n",
            "loss: 0.094670  [ 1600/ 3200]\n",
            "Epoch [32/60], Train Loss: 0.053235415552044286\n",
            "Test Error: \n",
            " Accuracy: 0.77%, Avg loss: 0.8427, F1 Score: 0.7595 \n",
            "\n",
            "Epoch 33\n",
            "-------------------------------\n",
            "loss: 0.013005  [    0/ 3200]\n",
            "loss: 0.017107  [ 1600/ 3200]\n",
            "Epoch [33/60], Train Loss: 0.03855363291222602\n",
            "Test Error: \n",
            " Accuracy: 0.77%, Avg loss: 0.8693, F1 Score: 0.7628 \n",
            "\n",
            "Epoch 34\n",
            "-------------------------------\n",
            "loss: 0.007915  [    0/ 3200]\n",
            "loss: 0.006957  [ 1600/ 3200]\n",
            "Epoch [34/60], Train Loss: 0.030729373014182784\n",
            "Test Error: \n",
            " Accuracy: 0.74%, Avg loss: 0.9207, F1 Score: 0.7449 \n",
            "\n",
            "Epoch 35\n",
            "-------------------------------\n",
            "loss: 0.028020  [    0/ 3200]\n",
            "loss: 0.013352  [ 1600/ 3200]\n",
            "Epoch [35/60], Train Loss: 0.023030023394385354\n",
            "Test Error: \n",
            " Accuracy: 0.78%, Avg loss: 0.8726, F1 Score: 0.7757 \n",
            "\n",
            "Epoch 36\n",
            "-------------------------------\n",
            "loss: 0.009078  [    0/ 3200]\n",
            "loss: 0.037414  [ 1600/ 3200]\n",
            "Epoch [36/60], Train Loss: 0.022856392134563065\n",
            "Test Error: \n",
            " Accuracy: 0.77%, Avg loss: 0.8472, F1 Score: 0.7664 \n",
            "\n",
            "Epoch 37\n",
            "-------------------------------\n",
            "loss: 0.012629  [    0/ 3200]\n",
            "loss: 0.034796  [ 1600/ 3200]\n",
            "Epoch [37/60], Train Loss: 0.02724485278245993\n",
            "Test Error: \n",
            " Accuracy: 0.79%, Avg loss: 0.7766, F1 Score: 0.7846 \n",
            "\n",
            "New best model with F1 score: 0.7846204040504204\n",
            "Epoch 38\n",
            "-------------------------------\n",
            "loss: 0.007218  [    0/ 3200]\n",
            "loss: 0.036458  [ 1600/ 3200]\n",
            "Epoch [38/60], Train Loss: 0.034328745431266725\n",
            "Test Error: \n",
            " Accuracy: 0.75%, Avg loss: 1.0140, F1 Score: 0.7391 \n",
            "\n",
            "Epoch 39\n",
            "-------------------------------\n",
            "loss: 0.010524  [    0/ 3200]\n",
            "loss: 0.004261  [ 1600/ 3200]\n",
            "Epoch [39/60], Train Loss: 0.03650985084590502\n",
            "Test Error: \n",
            " Accuracy: 0.74%, Avg loss: 0.9524, F1 Score: 0.7233 \n",
            "\n",
            "Epoch 40\n",
            "-------------------------------\n",
            "loss: 0.006945  [    0/ 3200]\n",
            "loss: 0.008608  [ 1600/ 3200]\n",
            "Epoch [40/60], Train Loss: 0.01688426162698306\n",
            "Test Error: \n",
            " Accuracy: 0.79%, Avg loss: 0.7943, F1 Score: 0.7884 \n",
            "\n",
            "New best model with F1 score: 0.7884037763892243\n",
            "Epoch 41\n",
            "-------------------------------\n",
            "loss: 0.005226  [    0/ 3200]\n",
            "loss: 0.018203  [ 1600/ 3200]\n",
            "Epoch [41/60], Train Loss: 0.013711361314053648\n",
            "Test Error: \n",
            " Accuracy: 0.77%, Avg loss: 0.8217, F1 Score: 0.7666 \n",
            "\n",
            "Epoch 42\n",
            "-------------------------------\n",
            "loss: 0.018409  [    0/ 3200]\n",
            "loss: 0.002962  [ 1600/ 3200]\n",
            "Epoch [42/60], Train Loss: 0.026236796276061795\n",
            "Test Error: \n",
            " Accuracy: 0.79%, Avg loss: 0.8315, F1 Score: 0.7833 \n",
            "\n",
            "Epoch 43\n",
            "-------------------------------\n",
            "loss: 0.004576  [    0/ 3200]\n",
            "loss: 0.003263  [ 1600/ 3200]\n",
            "Epoch [43/60], Train Loss: 0.015950581309734844\n",
            "Test Error: \n",
            " Accuracy: 0.79%, Avg loss: 0.7639, F1 Score: 0.7829 \n",
            "\n",
            "Epoch 44\n",
            "-------------------------------\n",
            "loss: 0.004128  [    0/ 3200]\n",
            "loss: 0.005156  [ 1600/ 3200]\n",
            "Epoch [44/60], Train Loss: 0.016292164624028372\n",
            "Test Error: \n",
            " Accuracy: 0.79%, Avg loss: 0.8097, F1 Score: 0.7828 \n",
            "\n",
            "Epoch 45\n",
            "-------------------------------\n",
            "loss: 0.003935  [    0/ 3200]\n",
            "loss: 0.004297  [ 1600/ 3200]\n",
            "Epoch [45/60], Train Loss: 0.008895364219788461\n",
            "Test Error: \n",
            " Accuracy: 0.80%, Avg loss: 0.8114, F1 Score: 0.7894 \n",
            "\n",
            "New best model with F1 score: 0.7894118487312636\n",
            "Epoch 46\n",
            "-------------------------------\n",
            "loss: 0.003761  [    0/ 3200]\n",
            "loss: 0.004133  [ 1600/ 3200]\n",
            "Epoch [46/60], Train Loss: 0.0174541669950122\n",
            "Test Error: \n",
            " Accuracy: 0.78%, Avg loss: 0.8047, F1 Score: 0.7690 \n",
            "\n",
            "Epoch 47\n",
            "-------------------------------\n",
            "loss: 0.012963  [    0/ 3200]\n",
            "loss: 0.003847  [ 1600/ 3200]\n",
            "Epoch [47/60], Train Loss: 0.014653918159019667\n",
            "Test Error: \n",
            " Accuracy: 0.80%, Avg loss: 0.7700, F1 Score: 0.7933 \n",
            "\n",
            "New best model with F1 score: 0.7932808705240718\n",
            "Epoch 48\n",
            "-------------------------------\n",
            "loss: 0.004599  [    0/ 3200]\n",
            "loss: 0.006059  [ 1600/ 3200]\n",
            "Epoch [48/60], Train Loss: 0.009420930419000797\n",
            "Test Error: \n",
            " Accuracy: 0.79%, Avg loss: 0.7985, F1 Score: 0.7839 \n",
            "\n",
            "Epoch 49\n",
            "-------------------------------\n",
            "loss: 0.005981  [    0/ 3200]\n",
            "loss: 0.002856  [ 1600/ 3200]\n",
            "Epoch [49/60], Train Loss: 0.007863412438309752\n",
            "Test Error: \n",
            " Accuracy: 0.78%, Avg loss: 0.8243, F1 Score: 0.7658 \n",
            "\n",
            "Epoch 50\n",
            "-------------------------------\n",
            "loss: 0.002003  [    0/ 3200]\n",
            "loss: 0.004353  [ 1600/ 3200]\n",
            "Epoch [50/60], Train Loss: 0.011152616026811302\n",
            "Test Error: \n",
            " Accuracy: 0.78%, Avg loss: 0.8288, F1 Score: 0.7741 \n",
            "\n",
            "Epoch 51\n",
            "-------------------------------\n",
            "loss: 0.005142  [    0/ 3200]\n",
            "loss: 0.002841  [ 1600/ 3200]\n",
            "Epoch [51/60], Train Loss: 0.010372074357001111\n",
            "Test Error: \n",
            " Accuracy: 0.80%, Avg loss: 0.7805, F1 Score: 0.7911 \n",
            "\n",
            "Epoch 52\n",
            "-------------------------------\n",
            "loss: 0.014121  [    0/ 3200]\n",
            "loss: 0.004676  [ 1600/ 3200]\n",
            "Epoch [52/60], Train Loss: 0.007059670216985978\n",
            "Test Error: \n",
            " Accuracy: 0.79%, Avg loss: 0.8600, F1 Score: 0.7708 \n",
            "\n",
            "Epoch 53\n",
            "-------------------------------\n",
            "loss: 0.003714  [    0/ 3200]\n",
            "loss: 0.008406  [ 1600/ 3200]\n",
            "Epoch [53/60], Train Loss: 0.009664550426532515\n",
            "Test Error: \n",
            " Accuracy: 0.79%, Avg loss: 0.8162, F1 Score: 0.7803 \n",
            "\n",
            "Epoch 54\n",
            "-------------------------------\n",
            "loss: 0.001018  [    0/ 3200]\n",
            "loss: 0.003602  [ 1600/ 3200]\n",
            "Epoch [54/60], Train Loss: 0.01201033191609895\n",
            "Test Error: \n",
            " Accuracy: 0.79%, Avg loss: 0.7969, F1 Score: 0.7798 \n",
            "\n",
            "Epoch 55\n",
            "-------------------------------\n",
            "loss: 0.004580  [    0/ 3200]\n",
            "loss: 0.001290  [ 1600/ 3200]\n",
            "Epoch [55/60], Train Loss: 0.010663074499752838\n",
            "Test Error: \n",
            " Accuracy: 0.79%, Avg loss: 0.7812, F1 Score: 0.7806 \n",
            "\n",
            "Epoch 56\n",
            "-------------------------------\n",
            "loss: 0.002478  [    0/ 3200]\n",
            "loss: 0.003040  [ 1600/ 3200]\n",
            "Epoch [56/60], Train Loss: 0.008659260577114764\n",
            "Test Error: \n",
            " Accuracy: 0.79%, Avg loss: 0.8010, F1 Score: 0.7796 \n",
            "\n",
            "Epoch 57\n",
            "-------------------------------\n",
            "loss: 0.001153  [    0/ 3200]\n",
            "loss: 0.003034  [ 1600/ 3200]\n",
            "Epoch [57/60], Train Loss: 0.006687217441794928\n",
            "Test Error: \n",
            " Accuracy: 0.79%, Avg loss: 0.7888, F1 Score: 0.7844 \n",
            "\n",
            "Epoch 58\n",
            "-------------------------------\n",
            "loss: 0.014570  [    0/ 3200]\n",
            "loss: 0.004357  [ 1600/ 3200]\n",
            "Epoch [58/60], Train Loss: 0.013343059502367395\n",
            "Test Error: \n",
            " Accuracy: 0.79%, Avg loss: 0.8052, F1 Score: 0.7834 \n",
            "\n",
            "Epoch 59\n",
            "-------------------------------\n",
            "loss: 0.001420  [    0/ 3200]\n",
            "loss: 0.002657  [ 1600/ 3200]\n",
            "Epoch [59/60], Train Loss: 0.006661761044233572\n",
            "Test Error: \n",
            " Accuracy: 0.80%, Avg loss: 0.7397, F1 Score: 0.7938 \n",
            "\n",
            "New best model with F1 score: 0.7937514232775051\n",
            "Epoch 60\n",
            "-------------------------------\n",
            "loss: 0.000988  [    0/ 3200]\n",
            "loss: 0.009208  [ 1600/ 3200]\n",
            "Epoch [60/60], Train Loss: 0.00610756911453791\n",
            "Test Error: \n",
            " Accuracy: 0.78%, Avg loss: 0.8466, F1 Score: 0.7676 \n",
            "\n",
            "Loading the best model and evaluating on the test set.\n",
            "Linear output: 1024\n",
            "Test Error: \n",
            " Accuracy: 0.75%, Avg loss: 1.0119, F1 Score: 0.7447 \n",
            "\n",
            "Final Test: Accuracy: 0.75, F1: 0.7447\n",
            "   Average Accuracy  Average F1 Score  Epochs  Weight Decay  Dropout\n",
            "4          0.782703          0.778843      60         0.000      0.0\n",
            "1          0.760901          0.762512      30         0.001      0.0\n",
            "0          0.755814          0.753612      30         0.000      0.0\n",
            "7          0.749273          0.744739      60         0.001      0.2\n",
            "2          0.742006          0.744173      30         0.000      0.2\n",
            "6          0.745640          0.744088      60         0.000      0.2\n",
            "3          0.745640          0.742287      30         0.001      0.2\n",
            "5          0.728198          0.721741      60         0.001      0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the best hyperparameters from the sorted results\n",
        "best_epochs = sorted_results.iloc[0]['Epochs'].astype(int)\n",
        "print(f\"Best number of epochs: {best_epochs}\")\n",
        "best_weight_decay = sorted_results.iloc[0]['Weight Decay'].astype(float)\n",
        "print(f\"Best weight decay: {best_weight_decay}\")\n",
        "best_dropout = sorted_results.iloc[0]['Dropout'].astype(float)\n",
        "print(f\"Best dropout: {best_dropout}\")\n",
        "best_lr = 1e-3\n",
        "\n",
        "def retrain_model(train_loader, val_loader, model_class, loss_fn, num_classes, num_epochs, lr, weight_decay, dropout):\n",
        "    model = model_class(num_classes, nn.ReLU(), dropout)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "\n",
        "    best_f1_score = 0.0\n",
        "    best_model_state = None\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
        "        train_loss = train_loop(train_loader, model, loss_fn, optimizer, device=True)\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss}')\n",
        "        _, _, val_f1_score, _ = test_loop(val_loader, model, loss_fn, device=True)\n",
        "\n",
        "        if val_f1_score > best_f1_score:\n",
        "            best_f1_score = val_f1_score\n",
        "            best_model_state = model.state_dict()\n",
        "            print(f\"New best model with F1 score: {best_f1_score}\")\n",
        "            torch.save(best_model_state, 'best_model_retrained.pth')\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "    model.load_state_dict(torch.load('best_model_retrained.pth'))\n",
        "    return model\n",
        "\n",
        "best_model = retrain_model(train_loader_melgrams, val_loader_melgrams,\n",
        "                           MusicGenreCNN_Dropout, loss_fn,\n",
        "                           num_classes, best_epochs, best_lr, best_weight_decay, best_dropout)\n",
        "\n",
        "torch.save(best_model.state_dict(), 'best_CNNmodel.pth')\n",
        "\n",
        "print(\"Last evaluation on test set:\")\n",
        "report = evaluate(test_loader_melgrams, best_model, loss_fn, device=True, return_dict=False)\n"
      ],
      "metadata": {
        "id": "s0Ey0OU2WL_m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "700cfefd-070d-4c39-eaa3-a7c07d15800f"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best number of epochs: 60\n",
            "Best weight decay: 0.0\n",
            "Best dropout: 0.0\n",
            "Linear output: 1024\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 1.488491  [    0/ 3200]\n",
            "loss: 0.594611  [ 1600/ 3200]\n",
            "Epoch [1/60], Train Loss: 0.7874744901061058\n",
            "Test Error: \n",
            " Accuracy: 0.63%, Avg loss: 0.8981, F1 Score: 0.5835 \n",
            "\n",
            "New best model with F1 score: 0.5835383835963474\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 0.490453  [    0/ 3200]\n",
            "loss: 0.834073  [ 1600/ 3200]\n",
            "Epoch [2/60], Train Loss: 0.6209416772425175\n",
            "Test Error: \n",
            " Accuracy: 0.69%, Avg loss: 0.7933, F1 Score: 0.6800 \n",
            "\n",
            "New best model with F1 score: 0.6799770014652318\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 0.906967  [    0/ 3200]\n",
            "loss: 1.179375  [ 1600/ 3200]\n",
            "Epoch [3/60], Train Loss: 0.565341810286045\n",
            "Test Error: \n",
            " Accuracy: 0.63%, Avg loss: 0.9236, F1 Score: 0.6190 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 0.406274  [    0/ 3200]\n",
            "loss: 0.214686  [ 1600/ 3200]\n",
            "Epoch [4/60], Train Loss: 0.5087975077331066\n",
            "Test Error: \n",
            " Accuracy: 0.64%, Avg loss: 0.9826, F1 Score: 0.6093 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 0.233811  [    0/ 3200]\n",
            "loss: 0.147347  [ 1600/ 3200]\n",
            "Epoch [5/60], Train Loss: 0.43906198456883433\n",
            "Test Error: \n",
            " Accuracy: 0.59%, Avg loss: 1.3486, F1 Score: 0.5535 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 0.318889  [    0/ 3200]\n",
            "loss: 0.475087  [ 1600/ 3200]\n",
            "Epoch [6/60], Train Loss: 0.36404000751674176\n",
            "Test Error: \n",
            " Accuracy: 0.70%, Avg loss: 0.7629, F1 Score: 0.6887 \n",
            "\n",
            "New best model with F1 score: 0.6886618475393033\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 0.358042  [    0/ 3200]\n",
            "loss: 0.252979  [ 1600/ 3200]\n",
            "Epoch [7/60], Train Loss: 0.3089713325165212\n",
            "Test Error: \n",
            " Accuracy: 0.76%, Avg loss: 0.6335, F1 Score: 0.7532 \n",
            "\n",
            "New best model with F1 score: 0.7531871528612507\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 0.144239  [    0/ 3200]\n",
            "loss: 0.138488  [ 1600/ 3200]\n",
            "Epoch [8/60], Train Loss: 0.2778386118263006\n",
            "Test Error: \n",
            " Accuracy: 0.73%, Avg loss: 0.7840, F1 Score: 0.6763 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 0.064301  [    0/ 3200]\n",
            "loss: 0.892151  [ 1600/ 3200]\n",
            "Epoch [9/60], Train Loss: 0.237181903924793\n",
            "Test Error: \n",
            " Accuracy: 0.68%, Avg loss: 0.9395, F1 Score: 0.6763 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 0.329045  [    0/ 3200]\n",
            "loss: 0.138153  [ 1600/ 3200]\n",
            "Epoch [10/60], Train Loss: 0.2082590201124549\n",
            "Test Error: \n",
            " Accuracy: 0.77%, Avg loss: 0.6711, F1 Score: 0.7679 \n",
            "\n",
            "New best model with F1 score: 0.7678992989462516\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "loss: 0.092235  [    0/ 3200]\n",
            "loss: 0.131329  [ 1600/ 3200]\n",
            "Epoch [11/60], Train Loss: 0.15366954680066555\n",
            "Test Error: \n",
            " Accuracy: 0.66%, Avg loss: 1.1850, F1 Score: 0.6605 \n",
            "\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "loss: 0.041893  [    0/ 3200]\n",
            "loss: 0.120296  [ 1600/ 3200]\n",
            "Epoch [12/60], Train Loss: 0.13420359869021922\n",
            "Test Error: \n",
            " Accuracy: 0.78%, Avg loss: 0.6821, F1 Score: 0.7766 \n",
            "\n",
            "New best model with F1 score: 0.7765771157338157\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "loss: 0.038769  [    0/ 3200]\n",
            "loss: 0.020785  [ 1600/ 3200]\n",
            "Epoch [13/60], Train Loss: 0.1334168991073966\n",
            "Test Error: \n",
            " Accuracy: 0.72%, Avg loss: 0.9639, F1 Score: 0.7211 \n",
            "\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "loss: 0.130090  [    0/ 3200]\n",
            "loss: 0.031337  [ 1600/ 3200]\n",
            "Epoch [14/60], Train Loss: 0.10593403542414308\n",
            "Test Error: \n",
            " Accuracy: 0.73%, Avg loss: 0.8054, F1 Score: 0.7309 \n",
            "\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "loss: 0.036249  [    0/ 3200]\n",
            "loss: 0.020960  [ 1600/ 3200]\n",
            "Epoch [15/60], Train Loss: 0.07444723126478493\n",
            "Test Error: \n",
            " Accuracy: 0.79%, Avg loss: 0.6603, F1 Score: 0.7867 \n",
            "\n",
            "New best model with F1 score: 0.7867133389356745\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "loss: 0.249787  [    0/ 3200]\n",
            "loss: 0.032979  [ 1600/ 3200]\n",
            "Epoch [16/60], Train Loss: 0.07895286123850383\n",
            "Test Error: \n",
            " Accuracy: 0.76%, Avg loss: 0.8360, F1 Score: 0.7360 \n",
            "\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "loss: 0.016338  [    0/ 3200]\n",
            "loss: 0.009722  [ 1600/ 3200]\n",
            "Epoch [17/60], Train Loss: 0.07464484623051248\n",
            "Test Error: \n",
            " Accuracy: 0.74%, Avg loss: 0.9070, F1 Score: 0.7264 \n",
            "\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "loss: 0.047065  [    0/ 3200]\n",
            "loss: 0.007671  [ 1600/ 3200]\n",
            "Epoch [18/60], Train Loss: 0.05204999272013083\n",
            "Test Error: \n",
            " Accuracy: 0.76%, Avg loss: 0.8740, F1 Score: 0.7533 \n",
            "\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "loss: 0.019840  [    0/ 3200]\n",
            "loss: 0.004019  [ 1600/ 3200]\n",
            "Epoch [19/60], Train Loss: 0.059114397993544114\n",
            "Test Error: \n",
            " Accuracy: 0.71%, Avg loss: 1.0557, F1 Score: 0.7227 \n",
            "\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "loss: 0.201144  [    0/ 3200]\n",
            "loss: 0.008678  [ 1600/ 3200]\n",
            "Epoch [20/60], Train Loss: 0.04827916620008182\n",
            "Test Error: \n",
            " Accuracy: 0.76%, Avg loss: 0.9071, F1 Score: 0.7568 \n",
            "\n",
            "Epoch 21\n",
            "-------------------------------\n",
            "loss: 0.019348  [    0/ 3200]\n",
            "loss: 0.058259  [ 1600/ 3200]\n",
            "Epoch [21/60], Train Loss: 0.03328581277164631\n",
            "Test Error: \n",
            " Accuracy: 0.78%, Avg loss: 0.7617, F1 Score: 0.7741 \n",
            "\n",
            "Epoch 22\n",
            "-------------------------------\n",
            "loss: 0.050370  [    0/ 3200]\n",
            "loss: 0.416774  [ 1600/ 3200]\n",
            "Epoch [22/60], Train Loss: 0.04759424740623217\n",
            "Test Error: \n",
            " Accuracy: 0.77%, Avg loss: 0.8176, F1 Score: 0.7615 \n",
            "\n",
            "Epoch 23\n",
            "-------------------------------\n",
            "loss: 0.013272  [    0/ 3200]\n",
            "loss: 0.017882  [ 1600/ 3200]\n",
            "Epoch [23/60], Train Loss: 0.03661824990413152\n",
            "Test Error: \n",
            " Accuracy: 0.73%, Avg loss: 1.1933, F1 Score: 0.7014 \n",
            "\n",
            "Epoch 24\n",
            "-------------------------------\n",
            "loss: 0.039302  [    0/ 3200]\n",
            "loss: 0.001698  [ 1600/ 3200]\n",
            "Epoch [24/60], Train Loss: 0.046867939816438595\n",
            "Test Error: \n",
            " Accuracy: 0.75%, Avg loss: 0.8866, F1 Score: 0.7538 \n",
            "\n",
            "Epoch 25\n",
            "-------------------------------\n",
            "loss: 0.006604  [    0/ 3200]\n",
            "loss: 0.015783  [ 1600/ 3200]\n",
            "Epoch [25/60], Train Loss: 0.022815287401899694\n",
            "Test Error: \n",
            " Accuracy: 0.78%, Avg loss: 0.8562, F1 Score: 0.7788 \n",
            "\n",
            "Epoch 26\n",
            "-------------------------------\n",
            "loss: 0.088999  [    0/ 3200]\n",
            "loss: 0.012235  [ 1600/ 3200]\n",
            "Epoch [26/60], Train Loss: 0.025011673512344713\n",
            "Test Error: \n",
            " Accuracy: 0.78%, Avg loss: 0.8583, F1 Score: 0.7793 \n",
            "\n",
            "Epoch 27\n",
            "-------------------------------\n",
            "loss: 0.047284  [    0/ 3200]\n",
            "loss: 0.045172  [ 1600/ 3200]\n",
            "Epoch [27/60], Train Loss: 0.02793282699160045\n",
            "Test Error: \n",
            " Accuracy: 0.79%, Avg loss: 0.8070, F1 Score: 0.7900 \n",
            "\n",
            "New best model with F1 score: 0.7899532651090396\n",
            "Epoch 28\n",
            "-------------------------------\n",
            "loss: 0.002907  [    0/ 3200]\n",
            "loss: 0.001542  [ 1600/ 3200]\n",
            "Epoch [28/60], Train Loss: 0.022343288847478108\n",
            "Test Error: \n",
            " Accuracy: 0.77%, Avg loss: 0.8659, F1 Score: 0.7706 \n",
            "\n",
            "Epoch 29\n",
            "-------------------------------\n",
            "loss: 0.003671  [    0/ 3200]\n",
            "loss: 0.004306  [ 1600/ 3200]\n",
            "Epoch [29/60], Train Loss: 0.01744377682771301\n",
            "Test Error: \n",
            " Accuracy: 0.62%, Avg loss: 1.6648, F1 Score: 0.6200 \n",
            "\n",
            "Epoch 30\n",
            "-------------------------------\n",
            "loss: 0.350660  [    0/ 3200]\n",
            "loss: 0.009742  [ 1600/ 3200]\n",
            "Epoch [30/60], Train Loss: 0.016541985400108386\n",
            "Test Error: \n",
            " Accuracy: 0.80%, Avg loss: 0.8622, F1 Score: 0.7860 \n",
            "\n",
            "Epoch 31\n",
            "-------------------------------\n",
            "loss: 0.000883  [    0/ 3200]\n",
            "loss: 0.004162  [ 1600/ 3200]\n",
            "Epoch [31/60], Train Loss: 0.005662705139402533\n",
            "Test Error: \n",
            " Accuracy: 0.80%, Avg loss: 0.8247, F1 Score: 0.7945 \n",
            "\n",
            "New best model with F1 score: 0.7944590623298546\n",
            "Epoch 32\n",
            "-------------------------------\n",
            "loss: 0.000256  [    0/ 3200]\n",
            "loss: 0.002933  [ 1600/ 3200]\n",
            "Epoch [32/60], Train Loss: 0.007976287616475019\n",
            "Test Error: \n",
            " Accuracy: 0.78%, Avg loss: 0.9624, F1 Score: 0.7691 \n",
            "\n",
            "Epoch 33\n",
            "-------------------------------\n",
            "loss: 0.003796  [    0/ 3200]\n",
            "loss: 0.000565  [ 1600/ 3200]\n",
            "Epoch [33/60], Train Loss: 0.011729010074705002\n",
            "Test Error: \n",
            " Accuracy: 0.77%, Avg loss: 0.9481, F1 Score: 0.7675 \n",
            "\n",
            "Epoch 34\n",
            "-------------------------------\n",
            "loss: 0.001433  [    0/ 3200]\n",
            "loss: 0.000855  [ 1600/ 3200]\n",
            "Epoch [34/60], Train Loss: 0.009452421926253009\n",
            "Test Error: \n",
            " Accuracy: 0.75%, Avg loss: 0.9807, F1 Score: 0.7485 \n",
            "\n",
            "Epoch 35\n",
            "-------------------------------\n",
            "loss: 0.001188  [    0/ 3200]\n",
            "loss: 0.000879  [ 1600/ 3200]\n",
            "Epoch [35/60], Train Loss: 0.007500385697640013\n",
            "Test Error: \n",
            " Accuracy: 0.79%, Avg loss: 0.9744, F1 Score: 0.7769 \n",
            "\n",
            "Epoch 36\n",
            "-------------------------------\n",
            "loss: 0.012508  [    0/ 3200]\n",
            "loss: 0.003802  [ 1600/ 3200]\n",
            "Epoch [36/60], Train Loss: 0.0048231621968443505\n",
            "Test Error: \n",
            " Accuracy: 0.77%, Avg loss: 1.0522, F1 Score: 0.7588 \n",
            "\n",
            "Epoch 37\n",
            "-------------------------------\n",
            "loss: 0.002456  [    0/ 3200]\n",
            "loss: 0.000399  [ 1600/ 3200]\n",
            "Epoch [37/60], Train Loss: 0.004680913530537509\n",
            "Test Error: \n",
            " Accuracy: 0.77%, Avg loss: 0.9395, F1 Score: 0.7678 \n",
            "\n",
            "Epoch 38\n",
            "-------------------------------\n",
            "loss: 0.000435  [    0/ 3200]\n",
            "loss: 0.000755  [ 1600/ 3200]\n",
            "Epoch [38/60], Train Loss: 0.003681643964519026\n",
            "Test Error: \n",
            " Accuracy: 0.77%, Avg loss: 0.9280, F1 Score: 0.7699 \n",
            "\n",
            "Epoch 39\n",
            "-------------------------------\n",
            "loss: 0.000776  [    0/ 3200]\n",
            "loss: 0.000554  [ 1600/ 3200]\n",
            "Epoch [39/60], Train Loss: 0.004989914574616705\n",
            "Test Error: \n",
            " Accuracy: 0.78%, Avg loss: 1.0000, F1 Score: 0.7694 \n",
            "\n",
            "Epoch 40\n",
            "-------------------------------\n",
            "loss: 0.000561  [    0/ 3200]\n",
            "loss: 0.000985  [ 1600/ 3200]\n",
            "Epoch [40/60], Train Loss: 0.004331032856498496\n",
            "Test Error: \n",
            " Accuracy: 0.76%, Avg loss: 0.9656, F1 Score: 0.7616 \n",
            "\n",
            "Epoch 41\n",
            "-------------------------------\n",
            "loss: 0.026130  [    0/ 3200]\n",
            "loss: 0.002947  [ 1600/ 3200]\n",
            "Epoch [41/60], Train Loss: 0.0031455657056721977\n",
            "Test Error: \n",
            " Accuracy: 0.79%, Avg loss: 0.9403, F1 Score: 0.7836 \n",
            "\n",
            "Epoch 42\n",
            "-------------------------------\n",
            "loss: 0.002599  [    0/ 3200]\n",
            "loss: 0.020764  [ 1600/ 3200]\n",
            "Epoch [42/60], Train Loss: 0.006827119942099671\n",
            "Test Error: \n",
            " Accuracy: 0.78%, Avg loss: 0.9760, F1 Score: 0.7711 \n",
            "\n",
            "Epoch 43\n",
            "-------------------------------\n",
            "loss: 0.001175  [    0/ 3200]\n",
            "loss: 0.000365  [ 1600/ 3200]\n",
            "Epoch [43/60], Train Loss: 0.0038270846700470427\n",
            "Test Error: \n",
            " Accuracy: 0.79%, Avg loss: 0.9423, F1 Score: 0.7839 \n",
            "\n",
            "Epoch 44\n",
            "-------------------------------\n",
            "loss: 0.000733  [    0/ 3200]\n",
            "loss: 0.000346  [ 1600/ 3200]\n",
            "Epoch [44/60], Train Loss: 0.003927480326674413\n",
            "Test Error: \n",
            " Accuracy: 0.78%, Avg loss: 1.0538, F1 Score: 0.7698 \n",
            "\n",
            "Epoch 45\n",
            "-------------------------------\n",
            "loss: 0.001420  [    0/ 3200]\n",
            "loss: 0.002156  [ 1600/ 3200]\n",
            "Epoch [45/60], Train Loss: 0.004400974456912081\n",
            "Test Error: \n",
            " Accuracy: 0.76%, Avg loss: 1.0228, F1 Score: 0.7592 \n",
            "\n",
            "Epoch 46\n",
            "-------------------------------\n",
            "loss: 0.000566  [    0/ 3200]\n",
            "loss: 0.000628  [ 1600/ 3200]\n",
            "Epoch [46/60], Train Loss: 0.004867357953189639\n",
            "Test Error: \n",
            " Accuracy: 0.78%, Avg loss: 0.9760, F1 Score: 0.7719 \n",
            "\n",
            "Epoch 47\n",
            "-------------------------------\n",
            "loss: 0.001092  [    0/ 3200]\n",
            "loss: 0.004427  [ 1600/ 3200]\n",
            "Epoch [47/60], Train Loss: 0.004782203737959207\n",
            "Test Error: \n",
            " Accuracy: 0.78%, Avg loss: 0.9514, F1 Score: 0.7784 \n",
            "\n",
            "Epoch 48\n",
            "-------------------------------\n",
            "loss: 0.000293  [    0/ 3200]\n",
            "loss: 0.000288  [ 1600/ 3200]\n",
            "Epoch [48/60], Train Loss: 0.004240110199534683\n",
            "Test Error: \n",
            " Accuracy: 0.78%, Avg loss: 0.9405, F1 Score: 0.7786 \n",
            "\n",
            "Epoch 49\n",
            "-------------------------------\n",
            "loss: 0.000241  [    0/ 3200]\n",
            "loss: 0.000186  [ 1600/ 3200]\n",
            "Epoch [49/60], Train Loss: 0.003954395198452403\n",
            "Test Error: \n",
            " Accuracy: 0.78%, Avg loss: 0.9580, F1 Score: 0.7697 \n",
            "\n",
            "Epoch 50\n",
            "-------------------------------\n",
            "loss: 0.000268  [    0/ 3200]\n",
            "loss: 0.000434  [ 1600/ 3200]\n",
            "Epoch [50/60], Train Loss: 0.001238988176228304\n",
            "Test Error: \n",
            " Accuracy: 0.77%, Avg loss: 0.9715, F1 Score: 0.7684 \n",
            "\n",
            "Epoch 51\n",
            "-------------------------------\n",
            "loss: 0.000238  [    0/ 3200]\n",
            "loss: 0.000199  [ 1600/ 3200]\n",
            "Epoch [51/60], Train Loss: 0.002186853769235313\n",
            "Test Error: \n",
            " Accuracy: 0.77%, Avg loss: 0.9764, F1 Score: 0.7603 \n",
            "\n",
            "Epoch 52\n",
            "-------------------------------\n",
            "loss: 0.000561  [    0/ 3200]\n",
            "loss: 0.000646  [ 1600/ 3200]\n",
            "Epoch [52/60], Train Loss: 0.0007114034357073251\n",
            "Test Error: \n",
            " Accuracy: 0.77%, Avg loss: 0.9801, F1 Score: 0.7653 \n",
            "\n",
            "Epoch 53\n",
            "-------------------------------\n",
            "loss: 0.000790  [    0/ 3200]\n",
            "loss: 0.000100  [ 1600/ 3200]\n",
            "Epoch [53/60], Train Loss: 0.0013342058829948656\n",
            "Test Error: \n",
            " Accuracy: 0.78%, Avg loss: 0.9328, F1 Score: 0.7730 \n",
            "\n",
            "Epoch 54\n",
            "-------------------------------\n",
            "loss: 0.000321  [    0/ 3200]\n",
            "loss: 0.001024  [ 1600/ 3200]\n",
            "Epoch [54/60], Train Loss: 0.001315843537049659\n",
            "Test Error: \n",
            " Accuracy: 0.78%, Avg loss: 0.9460, F1 Score: 0.7723 \n",
            "\n",
            "Epoch 55\n",
            "-------------------------------\n",
            "loss: 0.000744  [    0/ 3200]\n",
            "loss: 0.000194  [ 1600/ 3200]\n",
            "Epoch [55/60], Train Loss: 0.00212337519365974\n",
            "Test Error: \n",
            " Accuracy: 0.77%, Avg loss: 0.9471, F1 Score: 0.7673 \n",
            "\n",
            "Epoch 56\n",
            "-------------------------------\n",
            "loss: 0.001199  [    0/ 3200]\n",
            "loss: 0.013148  [ 1600/ 3200]\n",
            "Epoch [56/60], Train Loss: 0.0008643977414612891\n",
            "Test Error: \n",
            " Accuracy: 0.78%, Avg loss: 0.9605, F1 Score: 0.7745 \n",
            "\n",
            "Epoch 57\n",
            "-------------------------------\n",
            "loss: 0.081898  [    0/ 3200]\n",
            "loss: 0.000203  [ 1600/ 3200]\n",
            "Epoch [57/60], Train Loss: 0.004139454783035035\n",
            "Test Error: \n",
            " Accuracy: 0.77%, Avg loss: 0.9562, F1 Score: 0.7653 \n",
            "\n",
            "Epoch 58\n",
            "-------------------------------\n",
            "loss: 0.000335  [    0/ 3200]\n",
            "loss: 0.000442  [ 1600/ 3200]\n",
            "Epoch [58/60], Train Loss: 0.0010146472022825037\n",
            "Test Error: \n",
            " Accuracy: 0.77%, Avg loss: 0.9353, F1 Score: 0.7658 \n",
            "\n",
            "Epoch 59\n",
            "-------------------------------\n",
            "loss: 0.000201  [    0/ 3200]\n",
            "loss: 0.000532  [ 1600/ 3200]\n",
            "Epoch [59/60], Train Loss: 0.0007648055683966959\n",
            "Test Error: \n",
            " Accuracy: 0.78%, Avg loss: 0.9440, F1 Score: 0.7710 \n",
            "\n",
            "Epoch 60\n",
            "-------------------------------\n",
            "loss: 0.000156  [    0/ 3200]\n",
            "loss: 0.000547  [ 1600/ 3200]\n",
            "Epoch [60/60], Train Loss: 0.000818887087116309\n",
            "Test Error: \n",
            " Accuracy: 0.78%, Avg loss: 0.9404, F1 Score: 0.7714 \n",
            "\n",
            "Last evaluation on test set:\n",
            "Test Error: \n",
            " Accuracy: 74.1%, Avg loss: 0.075122 \n",
            "\n",
            "                     precision    recall  f1-score   support\n",
            "\n",
            "              blues       0.61      0.49      0.54       324\n",
            "          classical       0.86      0.91      0.88       297\n",
            "             hiphop       0.80      0.86      0.83       356\n",
            "rock_metal_hardrock       0.69      0.71      0.70       399\n",
            "\n",
            "           accuracy                           0.74      1376\n",
            "          macro avg       0.74      0.74      0.74      1376\n",
            "       weighted avg       0.73      0.74      0.74      1376\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model doesn't seem to generalize well. I took the initiative to print out a detailed classification report which includes the precision, recall and f1-score of each class independently. A quick observation is that classifying 'blues' is it's biggest weakness. I would say that more data could improve these differentiations, as CNN work best with as many (correctly handled and labeled) data as possible. The average accuracy and f1-score of the model on the test set rests at around 75%"
      ],
      "metadata": {
        "id": "4oy8st0CWNeA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Query 4"
      ],
      "metadata": {
        "id": "XGqky6Z3WOEA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MusicGenreCNN(nn.Module):\n",
        "    def __init__(self, out_dim, activation_fn, dropout=0.0):\n",
        "        super(MusicGenreCNN, self).__init__()\n",
        "        self.activation_fn = activation_fn\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            nn.Conv2d(1, 16, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm2d(16),\n",
        "            activation_fn,\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(16, 32, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm2d(32),\n",
        "            activation_fn,\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(32, 64, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm2d(64),\n",
        "            activation_fn,\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(64, 128, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm2d(128),\n",
        "            activation_fn,\n",
        "            nn.MaxPool2d(2, 2)\n",
        "        )\n",
        "\n",
        "        self._to_linear = None\n",
        "        self.calculate_linear_output((1, 21, 128))\n",
        "        print(f'Linear output: {self._to_linear}')\n",
        "\n",
        "        self.fc_layers = nn.Sequential(\n",
        "            nn.Linear(self._to_linear, 1024),\n",
        "            nn.BatchNorm1d(1024),\n",
        "            nn.Dropout(dropout),\n",
        "            activation_fn,\n",
        "            nn.Linear(1024, 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.Dropout(dropout),\n",
        "            activation_fn,\n",
        "            nn.Linear(256, 32),\n",
        "            nn.BatchNorm1d(32),\n",
        "            nn.Dropout(dropout),\n",
        "            activation_fn,\n",
        "            nn.Linear(32, out_dim)\n",
        "        )\n",
        "\n",
        "    def calculate_linear_output(self, shape):\n",
        "        output = torch.ones(1, *shape)\n",
        "        output = self.conv_layers(output)\n",
        "        self._to_linear = output.numel()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_layers(x)\n",
        "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
        "        x = self.fc_layers(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "num_classes = 4\n",
        "\n",
        "model = MusicGenreCNN(num_classes, nn.ReLU())\n",
        "model.load_state_dict(torch.load('best_CNNmodel.pth'))\n",
        "model.eval()\n",
        "\n",
        "def evaluate(dataloader, model):\n",
        "    predictions = []\n",
        "    with torch.no_grad():\n",
        "        for inputs in dataloader:\n",
        "            inputs = inputs.float().unsqueeze(1) # Add channel dimension\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            predictions.extend(predicted.cpu().numpy())\n",
        "    return predictions\n",
        "\n",
        "\n",
        "classical = np.load('lacrimosa_classical.npy')\n",
        "# https://www.youtube.com/watch?v=k1-TrAvp_xs\n",
        "blues = np.load('bbking_blues.npy')\n",
        "# https://www.youtube.com/watch?v=L8hOvsg_AiY\n",
        "rock_metal_hardrock = np.load('judaspriest_metal.npy')\n",
        "# https://www.youtube.com/watch?v=nM__lPTWThU\n",
        "hiphop = np.load('eminem_hiphop.npy')\n",
        "# https://www.youtube.com/watch?v=xFYQQPAOz7Y\n",
        "\n",
        "print(f\"Classical shape: {classical.shape}\")\n",
        "print(f\"Number of Samples (seconds): {classical.shape[0]}, Time (frame window): {classical.shape[1]}, Frequency: {classical.shape[2]}\")\n",
        "print(f\"\\nBlues shape: {blues.shape}\")\n",
        "print(f\"Number of Samples (seconds): {blues.shape[0]}, Time (frame window): {blues.shape[1]}, Frequency: {blues.shape[2]}\")\n",
        "print(f\"\\nMetal shape: {rock_metal_hardrock.shape}\")\n",
        "print(f\"Number of Samples (seconds): {rock_metal_hardrock.shape[0]}, Time (frame window): {rock_metal_hardrock.shape[1]}, Frequency: {rock_metal_hardrock.shape[2]}\")\n",
        "print(f\"\\nHiphop shape: {hiphop.shape}\")\n",
        "print(f\"Number of Samples (seconds): {hiphop.shape[0]}, Time (frame window): {hiphop.shape[1]}, Frequency: {hiphop.shape[2]}\")\n",
        "\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "classical_dataloader = DataLoader(classical, batch_size=BATCH_SIZE, shuffle=False)\n",
        "blues_dataloader = DataLoader(blues, batch_size=BATCH_SIZE, shuffle=False)\n",
        "rock_metal_hardrock_dataloader = DataLoader(rock_metal_hardrock, batch_size=BATCH_SIZE, shuffle=False)\n",
        "hiphop_dataloader = DataLoader(hiphop, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "predictions = evaluate(classical_dataloader, model)\n",
        "\n",
        "genres_dict = {0: 'blues', 1: 'classical', 2: 'hiphop', 3: 'rock_metal_hardrock'}\n",
        "\n",
        "predictions_dict = {\n",
        "    'blues': [],\n",
        "    'classical': [],\n",
        "    'hiphop': [],\n",
        "    'rock_metal_hardrock': []\n",
        "}\n",
        "\n",
        "predictions_dict['blues'] = evaluate(blues_dataloader, model)\n",
        "predictions_dict['classical'] = evaluate(classical_dataloader, model)\n",
        "predictions_dict['hiphop'] = evaluate(hiphop_dataloader, model)\n",
        "predictions_dict['rock_metal_hardrock'] = evaluate(rock_metal_hardrock_dataloader, model)\n",
        "\n",
        "print(predictions_dict)"
      ],
      "metadata": {
        "id": "BgEnUKhHWRh-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5dfd42f-2a2a-449b-8b0a-ee4e753e14b7"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear output: 1024\n",
            "Classical shape: (201, 20, 128)\n",
            "Number of Samples (seconds): 201, Time (frame window): 20, Frequency: 128\n",
            "\n",
            "Blues shape: (617, 20, 128)\n",
            "Number of Samples (seconds): 617, Time (frame window): 20, Frequency: 128\n",
            "\n",
            "Metal shape: (369, 20, 128)\n",
            "Number of Samples (seconds): 369, Time (frame window): 20, Frequency: 128\n",
            "\n",
            "Hiphop shape: (328, 20, 128)\n",
            "Number of Samples (seconds): 328, Time (frame window): 20, Frequency: 128\n",
            "{'blues': [0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 2, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 2, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 3, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 2], 'classical': [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'hiphop': [1, 1, 2, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 2, 2, 1, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 0, 0, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 2, 0, 2, 0, 0, 0, 0, 2, 2, 0, 2, 1, 1, 0, 2, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 1, 2, 2, 2, 2, 2, 0, 2, 2, 3, 2, 2, 2, 0, 2, 0, 1, 0, 2, 2, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 0, 2, 1, 2, 2, 2, 2, 0, 0, 2, 2, 0, 0, 1, 2, 2, 2, 0, 0, 2, 2, 2, 0, 2, 0, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 0, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 0, 2, 0, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 1, 1, 0], 'rock_metal_hardrock': [2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 2, 1, 2, 2, 1, 1, 1]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap, BoundaryNorm\n",
        "\n",
        "# padding predictions to have the same length\n",
        "max_samples = max(len(predictions_dict['blues']), len(predictions_dict['classical']), len(predictions_dict['hiphop']), len(predictions_dict['rock_metal_hardrock']))\n",
        "\n",
        "def pad_predictions(preds, max_samples):\n",
        "    return preds + [None] * (max_samples - len(preds))\n",
        "\n",
        "padded_predictions = {\n",
        "    'blues': pad_predictions(predictions_dict['blues'], max_samples),\n",
        "    'classical': pad_predictions(predictions_dict['classical'], max_samples),\n",
        "    'hiphop': pad_predictions(predictions_dict['hiphop'], max_samples),\n",
        "    'rock_metal_hardrock': pad_predictions(predictions_dict['rock_metal_hardrock'], max_samples)\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(padded_predictions)\n",
        "df = df.fillna(-1) # -1 values is the absence of a prediction (padding)\n",
        "\n",
        "# Define a custom color map using a smoother palette\n",
        "colors = plt.cm.tab20(np.linspace(0, 1, len(genres_dict) + 1))\n",
        "cmap = ListedColormap(colors)\n",
        "norm = BoundaryNorm(np.arange(-1.5, len(genres_dict) + 0.5), cmap.N)\n",
        "\n",
        "# Plotting the heatmap\n",
        "plt.figure(figsize=(14, 8))\n",
        "heatmap = sns.heatmap(df.T, cmap=cmap, norm=norm, cbar_kws={'label': 'Genre Prediction'}, annot=False)\n",
        "\n",
        "# Setting up the labels and title\n",
        "plt.ylabel('Music Classes')\n",
        "plt.xlabel('Timestamps (seconds)')\n",
        "plt.title('Music Genre Predictions Over Time')\n",
        "\n",
        "# Customizing the y-ticks to show music genres\n",
        "plt.yticks(np.arange(len(genres_dict)) + 0.5, list(genres_dict.values()), rotation=0)\n",
        "\n",
        "# Creating a custom color bar with genre labels\n",
        "colorbar = heatmap.collections[0].colorbar\n",
        "colorbar.set_ticks(np.arange(-1, len(genres_dict)))\n",
        "colorbar.set_ticklabels(['No Prediction'] + list(genres_dict.values()))\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uL2XHuEoWTCI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        },
        "outputId": "516358dc-400c-49a3-8fe0-85f916c9c748"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1400x800 with 2 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABSoAAALKCAYAAAA1T2+yAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAACleElEQVR4nOzdeXiM9/7/8dckZJJIYiexJkgItcRWuyitNVrUEk41OKilqD1VS2qLrWo7ulBbi6JotdaGqMauYo211GnL0doiliCZ3x/9ma+RRTLCHTwf1zXXNfO5P/f7fs+dO6nzOvdislgsFgEAAAAAAACAgRyMbgAAAAAAAAAACCoBAAAAAAAAGI6gEgAAAAAAAIDhCCoBAAAAAAAAGI6gEgAAAAAAAIDhCCoBAAAAAAAAGI6gEgAAAAAAAIDhCCoBAAAAAAAAGI6gEgAAAAAAAIDhCCoBAHgBBAYGKjAw0Og2kAYhISHy9va2GTOZTBo1alSGbYPjwRijRo2SyWQyug0AAIBMi6ASAIAnaP78+TKZTDKZTPr555+TLLdYLCpcuLBMJpOaNWtmQIf2W7NmjYKCgpQ/f345OTkpV65cqlOnjqZMmaLY2Fij27PL2bNnrT8vk8kkR0dHFSlSRC1atFB0dLTR7aXL0aNHNWrUKJ09e9boVuwSFRWlFi1aKH/+/DKbzfL29lb37t117tw5o1uz4e3tbXPMpPSaP3++0a0CAABkelmMbgAAgBeBs7OzFi9erFq1atmMb926Vb///rvMZvMT3f7GjRszrFZiYqK6dOmi+fPnq2zZsurZs6cKFy6s69eva8eOHfrggw+0du1aRUREZNg2n7bg4GA1adJECQkJiomJ0ezZs7Vu3Trt3LlTFSpUeOr93Lp1S1mypO+fbUePHlVYWJgCAwOTnKGZkcfDkzBjxgz17dtXxYoV07vvvisvLy/FxMRozpw5+vrrr7V27VrVqFHD6DYlSR9//LHi4uKsn9euXaslS5Zo6tSpypMnj3W8Ro0a+te//qWhQ4ca0SYAAMAzgaASAICnoEmTJlq+fLmmT59uEzgtXrxYlSpV0t9///1Et+/k5JRhtSZOnKj58+frvffe05QpU2wuZe3bt6/Onz+vhQsXZtj20ur27dtycnKSg8PjXzBSsWJF/etf/7J+rlmzppo3b67Zs2fr008/TXadGzduKFu2bI+97eQ4OztnaL2MPB4yWlRUlPr166datWpp/fr1cnV1tS7r0aOHatasqTfffFNHjhxRzpw5n1pfKf1833jjDZvPFy5c0JIlS/TGG28kCYglpTtwBgAAeJFw6TcAAE9BcHCwLl26pE2bNlnH7ty5oxUrVqh9+/ZJ5kdGRspkMikyMtJm/P6lyQ9eRnrhwgV16tRJhQoVktlslpeXl15//XWbS36Tuyfh7du3NWrUKPn5+cnZ2VleXl5q2bKlTp8+neL3uHnzpiZMmKAyZcpo0qRJyd5vz8vLS0OGDEky/uWXX6pSpUpycXFRrly51K5dO/33v/+1mRMYGKiXXnpJR48eVb169eTq6qqCBQtq4sSJye6fpUuX6oMPPlDBggXl6upqveR8165datSokbJnzy5XV1fVrVtXUVFRKX6vR3nllVckSWfOnJH0f5f0b926VT179lS+fPlUqFAh6/x169apdu3aypYtm9zd3dW0aVMdOXIkSd3Vq1frpZdekrOzs1566SWtWrUq2e0nd4/KP/74Q126dFGBAgVkNpvl4+OjHj166M6dO5o/f75at24tSapXr5718uP7x1Nyx8PFixfVpUsX5c+fX87OzipfvrwWLFhgM+f+8Td58mR99tlnKl68uMxms6pUqaI9e/bYzE3LcZmc0aNHy2QyacGCBTYhpSQVL15cEydO1Pnz562B8eTJk2UymfTbb78lqRUaGionJydduXLFOpaWY+P+vSSPHj2q9u3bK2fOnEnOhrZHcveoNJlM6t27t5YvX67SpUvLxcVF1atX16FDhyRJn376qUqUKCFnZ2cFBgYmu/8y+ngHAAAwCkElAABPgbe3t6pXr64lS5ZYx9atW6dr166pXbt2j1W7VatWWrVqlTp16qT//Oc/6tOnj65fv57qvfwSEhLUrFkzhYWFqVKlSpoyZYr69u2ra9eu6fDhwymu9/PPP+vq1asKDg6Wo6NjmnscO3asOnbsKF9fX3300Ufq16+fIiIiVKdOHV29etVm7pUrV9SoUSOVL19eU6ZMUalSpTRkyBCtW7cuSd3Ro0frhx9+0MCBAzVu3Dg5OTlp8+bNqlOnjmJjYzVy5EiNGzdOV69e1SuvvKLdu3enuecH3Q9vc+fObTPes2dPHT16VCNGjLBe0rto0SI1bdpUbm5umjBhgoYPH66jR4+qVq1aNiHTxo0b1apVK5lMJo0fP15vvPGGOnXqpL179z6ynz///FNVq1bV0qVL1bZtW02fPl1vvfWWtm7dqps3b6pOnTrq06ePJOn999/XokWLtGjRIvn7+ydb79atWwoMDNSiRYvUoUMHTZo0SdmzZ1dISIimTZuWZP7ixYs1adIkde/eXWPGjNHZs2fVsmVL3b171zrHnuPy5s2bioiIUO3ateXj45PsnLZt28psNuv777+XJLVp00Ymk0nLli1LMnfZsmV67bXXrGdepvfYaN26tW7evKlx48apa9euKfb9uLZt26YBAwbo7bff1qhRoxQTE6NmzZpp1qxZmj59unr27KlBgwZpx44d6ty5s826T+J4BwAAMIwFAAA8MfPmzbNIsuzZs8cyc+ZMi7u7u+XmzZsWi8Viad26taVevXoWi8ViKVq0qKVp06bW9bZs2WKRZNmyZYtNvTNnzlgkWebNm2exWCyWK1euWCRZJk2alGofdevWtdStW9f6+YsvvrBIsnz00UdJ5iYmJqZYZ9q0aRZJltWrV9uM37t3z/LXX3/ZvO7XOXv2rMXR0dEyduxYm3UOHTpkyZIli8143bp1LZIsCxcutI7Fx8dbPD09La1atbKO3d8/xYoVs+7P+737+vpaGjZsaPM9bt68afHx8bG8+uqrKX43i+X/9m9YWJjlr7/+sly4cMESGRlpCQgIsEiyfPPNNxaL5f9+rrVq1bLcu3fPuv7169ctOXLksHTt2tWm7oULFyzZs2e3Ga9QoYLFy8vLcvXqVevYxo0bLZIsRYsWtVlfkmXkyJHWzx07drQ4ODhY9uzZk+Q73P/ey5cvT/YYsliSHg8ff/yxRZLlyy+/tI7duXPHUr16dYubm5slNjbWZv/kzp3bcvnyZevcb7/91iLJsmbNGovFkvbj8mHR0dEWSZa+ffumOq9cuXKWXLlyWT9Xr17dUqlSJZs5u3fvtjmW0nNsjBw50iLJEhwcnK7+LRaLZdKkSRZJljNnziRZdr/ugyRZzGazzfxPP/3UIsni6elp3fcWi8USGhpqU/txj3cAAIDMhjMqAQB4Stq0aaNbt27p+++/1/Xr1/X9998ne9l3eri4uMjJyUmRkZE2l7c+yjfffKM8efLo3XffTbIsucu577t/abWbm5vN+KFDh5Q3b16b16VLlyRJK1euVGJiotq0aaO///7b+vL09JSvr6+2bNliU8vNzc3m/pBOTk6qWrWqfv311yT9vP3223JxcbF+jo6O1smTJ9W+fXtdunTJuq0bN26ofv36+umnn5SYmPjI/TNy5EjlzZtXnp6eCgwM1OnTpzVhwgS1bNnSZl7Xrl1tzizdtGmT9YzTB7+ro6OjXn75Zet3PX/+vKKjo/X2228re/bs1vVfffVVlS5dOtXeEhMTtXr1agUFBaly5cpJlqf280vJ2rVr5enpqeDgYOtY1qxZ1adPH8XFxWnr1q0289u2bWtzf8jatWtLkvVnZO9xef36dUmSu7t7qvPc3d1tnizftm1b7du3z+a2BV9//bXMZrNef/11SfYdG++8806ae38c9evXt7mf5csvvyzpn7NSH9wX98fv7+eMOt4BAAAyC+7mDQDAU5I3b141aNBAixcv1s2bN5WQkKA333zzsWqazWZNmDBBAwYMUP78+VWtWjU1a9ZMHTt2lKenZ4rrnT59WiVLlkz3gz3uhyYPPuVYkkqUKGG9/+bChQu1aNEi67KTJ0/KYrHI19c32ZpZs2a1+VyoUKEkYVvOnDl18ODBJOs+fHnwyZMnJf0TYKbk2rVrj3wIS7du3dS6dWs5ODgoR44cKlOmTLJPZk9p+/fvafkwDw8PSbLeTzG5fVKyZEn98ssvKfb2119/KTY2Vi+99FKq3yE9fvvtN/n6+iZ5ENH9S8Ufvv9jkSJFbD7f35/3Q0l7j8v7x9f9wDIl169ftwnwWrdurf79++vrr7/W+++/L4vFouXLl6tx48bWfW7PsZHS5ecZ7eH9eT+8Lly4cLLj9/dzRh3vAAAAmQVBJQAAT1H79u3VtWtXXbhwQY0bN1aOHDmSnZfSWXEJCQlJxvr166egoCCtXr1aGzZs0PDhwzV+/Hht3rxZAQEBGdm+SpUqJUk6fPiw9Uw16Z+zIBs0aCDpn/tYPigxMVEmk0nr1q1L9r6WD5+dmdK9Ly0WS5KxB8+mvL8tSZo0aZIqVKiQbJ2Ht5ccX19f6/dJTUrbX7RoUbKB3PPyxOe0/IzsOS5LlCihLFmyJBtK3xcfH6/jx4/bnE1aoEAB1a5dW8uWLdP777+vnTt36ty5c5owYYJ1jj3HxsM/3yclpf35qP2cUcc7AABAZvF8/GsZAIBnRIsWLdS9e3ft3LlTX3/9dYrz7p8B9fCDZpJ7srH0z9OQBwwYoAEDBujkyZOqUKGCpkyZoi+//DLF+bt27dLdu3eTnNGYmtq1ayt79uxaunSpQkNDk5yBl9K2LBaLfHx85Ofnl+Zt2aN48eKS/jlzMS1B45Pafr58+VLdftGiRSX93xlxDzp+/Hiq28ibN688PDxSfeiRlL5LwIsWLaqDBw8qMTHR5md67Ngxm37TK73HZbZs2VSvXj1t3rxZv/32W7LbXbZsmeLj49WsWTOb8bZt26pnz546fvy4vv76a7m6uiooKMimF8m4Y+NJeB6/EwAAeLFxj0oAAJ4iNzc3zZ49W6NGjbIJUR5WtGhROTo66qeffrIZ/89//mPz+ebNm7p9+7bNWPHixeXu7q74+PgU67dq1Up///23Zs6cmWRZcmcu3ufq6qrBgwfr8OHDGjp0aLJzHx5r2bKlHB0dFRYWlmSZxWKx3ssyI1SqVEnFixfX5MmTk1yeLv1z2fST1LBhQ3l4eGjcuHE2T8B+ePteXl6qUKGCFixYoGvXrlmXb9q0SUePHk11Gw4ODnrjjTe0Zs2aZJ8Qfn8fZ8uWTVLSsDs5TZo00YULF2zC83v37mnGjBlyc3NT3bp1H1njQfYel5L0wQcfyGKxKCQkRLdu3bJZdubMGQ0ePFheXl7q3r27zbJWrVrJ0dFRS5Ys0fLly9WsWTPrPpCMPzaehOfxOwEAgBcbZ1QCAPCUpXY/ufuyZ8+u1q1ba8aMGTKZTCpevLi+//57Xbx40WbeiRMnVL9+fbVp00alS5dWlixZtGrVKv3vf/9Tu3btUqzfsWNHLVy4UP3799fu3btVu3Zt3bhxQz/++KN69uxpc1n3w4YOHaqYmBhNmjRJGzduVKtWrVSoUCFduXJFv/zyi5YvX658+fLJ2dlZ0j8B1ZgxYxQaGqqzZ8/qjTfekLu7u86cOaNVq1apW7duGjhwYBr3XuocHBw0Z84cNW7cWGXKlFGnTp1UsGBB/fHHH9qyZYs8PDy0Zs2aDNlWcjw8PDR79my99dZbqlixotq1a6e8efPq3Llz+uGHH1SzZk1rODx+/Hg1bdpUtWrVUufOnXX58mXNmDFDZcqUSTZ0etC4ceO0ceNG1a1bV926dZO/v7/Onz+v5cuX6+eff1aOHDlUoUIFOTo6asKECbp27ZrMZrNeeeUV5cuXL0m9bt266dNPP1VISIj27dsnb29vrVixQlFRUfr4448f+XCbh9l7XEpSnTp1NHnyZPXv31/lypVTSEiIvLy8dOzYMX3++edKTEzU2rVrk9x3MV++fKpXr54++ugjXb9+XW3btrVZbvSx8SQ8j98JAAC82AgqAQDIpGbMmKG7d+/qk08+kdlsVps2bTRp0iSbh6gULlxYwcHBioiI0KJFi5QlSxaVKlVKy5YtU6tWrVKs7ejoqLVr12rs2LFavHixvvnmG+XOnVu1atVS2bJlU+3LwcFBixYtUqtWrfT5559rxowZunLlitzc3PTSSy9p7Nix6tq1q8298YYOHSo/Pz9NnTpVYWFh1t5fe+01NW/e/DH3lK3AwEDt2LFDo0eP1syZMxUXFydPT0+9/PLLSc7CexLat2+vAgUKKDw8XJMmTVJ8fLwKFiyo2rVrq1OnTtZ5jRo10vLly/XBBx8oNDRUxYsX17x58/Ttt98qMjIy1W0ULFhQu3bt0vDhw/XVV18pNjZWBQsWVOPGjeXq6ipJ8vT01CeffKLx48erS5cuSkhI0JYtW5INKl1cXBQZGamhQ4dqwYIFio2NVcmSJTVv3jyFhISkex/Ye1ze995776ly5cqaMmWKPv74Y127dk1eXl5q3bq1hg0bluKl6G3bttWPP/4od3d3NWnSJMlyo4+NJ+F5/E4AAODFZbKkdn0XAAAAAAAAADwF3KMSAAAAAAAAgOEIKgEAAAAAAAAYjqASAAAAAAAAgOEIKgEAAAAAAAAYjqASAAAAAAAAgOEIKgEAAAAAAAAYjqASAAAAAAAAgOGyGN0A8KR9enaSzee4IVut790m1FXckK1ym1A3yXoPj6c0LyUPb8eede5/fvh9WtZPae7Dcx6unxb366ZnnYyumdzPJrV1H6z/8LaS2/bDx0ZK+y2l9R5XSj/z5Hq+/778hdo64Lktydzkek6uVlqkdoyldsynZ1+mVPNR81I73lNaT/pnv0nSAc9tdv2epSS5fZDSnPR870fVSOnvRlp/dqnVT6mXR/2+pPZ78fCytPSV1t4frp/c71Vy207tb0pa/64mVzO5nh7V86OWpfbzedR3TK3f1Oaktdajvkty20vpb9TD3yel39G0/q1OTy+P8zc9Lf/tSc2jftYp/Y6n9rc5PX8vH14vue+W0vdLab201Eiut7Tuy9T2SWr/zrJHeo7LtMxLbnla/nan9rftUf+GeLBuen9HUqqVlvXTKrn9UfO9DyTJ+m+elL5reo6vXGM+kyTdyfpVmvdlar93afleKa33qL/Z9vxtT0la/5ubnt/jB93/WVWrVi1dfT0r5p3807Bt39y4SrNmzZIk9erVS7169TKsFyCjEVQCAAAAAAA8Iwgn8Tzj0m8AAAAAAAAAhiOoBAAAAAAAAGA4gkoAAAAAAAAAhiOoBAAAAAAAAGA4gkoAAAAAAAAAhiOoBAAAAAAAAGA4gkoAAAAAAAAAhiOoBAAAAAAAAGA4gkoAAAAAAAAAhiOoBAAAAAAAAGA4gkoAAAAAAAAAhiOoBAAAAAAAAGA4gkoAAAAAAAAAhiOoBAAAAAAAAGA4gkoAAAAAAAAAhiOoBAAAAAAAAGA4gkoAAAAAAAAAhiOoBAAAAAAAAGA4gkoAAAAAAAAAhiOoBAAAAAAAAGA4gkoAAAAAAAAAhiOoBAAAAAAAAGA4gkoAAAAAAAAAhiOoBAAAAAAAAGA4gkoAAAAAAAAAhiOoBAAAAAAAAGA4gkoAAAAAAAAAhiOoBAAAAAAAAGA4gkoAAAAAAAAAhiOoBAAAAAAAAGA4gkoAAAAAAAAAhiOoBAAAAAAAAGA4gkoAAAAAAAAAhiOoBAAAAAAAAGA4gkoAAAAAAAAAhiOoBAAAAAAAAGA4gkoAAAAAAAAAhiOoBAAAAAAAAGA4gkoAAAAAAAAAhiOoBAAAAAAAAGA4gkoAAAAAAAAAhiOoBAAAAAAAAGA4gkoAAAAAAAAAhiOoBAAAAAAAAGA4gkoAAAAAAAAAhiOoBAAAAAAAAGA4gkoAAAAAAAAAhiOoBAAAAAAAAGA4gkoAAAAAAAAAhiOoBAAAAAAAAGA4gkoAAAAAAAAAhiOoBAAAAAAAAGA4gkoAAAAAAAAAhiOoBAAAAAAAAGA4gkoAAAAAAAAAhiOoBAAAAAAAwCOdPXtWJpNJ0dHRRreSIby9vfXxxx+naa7JZNLq1aufaD/3Pa39HBISojfeeOOJbiO9CCoBAAAAAADwzHqaISKeLIJKAAAAAACAF8CdO3eMbgEPsPfnYbFYdO/evQzuJnMgqAQAAAAAAHgOBQYGqnfv3urXr5/y5Mmjhg0bauvWrapatarMZrO8vLw0dOhQm9ArMTFREydOVIkSJWQ2m1WkSBGNHTs22foJCQnq3LmzSpUqpXPnzj2yH5PJpE8//VTNmjWTq6ur/P39tWPHDp06dUqBgYHKli2batSoodOnT9us9+2336pixYpydnZWsWLFFBYWZu3Z29tbktSiRQuZTCbr59OnT+v1119X/vz55ebmpipVqujHH3+0Yy/+n7///lstWrSQq6urfH199d1339nsiy5dusjHx0cuLi4qWbKkpk2bZrP+/Uutx44dqwIFCqhkyZKSpN27dysgIEDOzs6qXLmy9u/fb7NeZGSkTCaT1q1bp0qVKslsNuvnn39WfHy8+vTpo3z58snZ2Vm1atXSnj17bNY9cuSImjVrJg8PD7m7u6t27dpJ9u99e/bsUd68eTVhwoTH2k+Pg6ASAAAAAADgGREfH6/Y2FibV3x8fIrzFyxYICcnJ0VFRWnUqFFq0qSJqlSpogMHDmj27NmaO3euxowZY50fGhqq8PBwDR8+XEePHtXixYuVP3/+ZPto3bq1oqOjtW3bNhUpUiRN/Y8ePVodO3ZUdHS0SpUqpfbt26t79+4KDQ3V3r17ZbFY1Lt3b+v8bdu2qWPHjurbt6+OHj2qTz/9VPPnz7eGp/eDuXnz5un8+fPWz3FxcWrSpIkiIiK0f/9+NWrUSEFBQWkKVFMSFhamNm3a6ODBg2rSpIk6dOigy5cvS/on4C1UqJCWL1+uo0ePasSIEXr//fe1bNkymxoRERE6fvy4Nm3apO+//15xcXFq1qyZSpcurX379mnUqFEaOHBgstsfOnSowsPDFRMTo3Llymnw4MH65ptvtGDBAv3yyy8qUaKEGjZsaO3pjz/+UJ06dWQ2m7V582bt27dPnTt3TvZszM2bN+vVV1/V2LFjNWTIELv30ePKYtiWAQAAAAAAkC7jx49XWFiYzdjIkSM1atSoZOf7+vpq4sSJkqSFCxeqcOHCmjlzpkwmk0qVKqU///xTQ4YM0YgRI3Tjxg1NmzZNM2fO1Ntvvy1JKl68uGrVqmVTMy4uTk2bNlV8fLy2bNmi7Nmzp7n/Tp06qU2bNpKkIUOGqHr16ho+fLgaNmwoSerbt686depknR8WFqahQ4da+ylWrJhGjx6twYMHa+TIkcqbN68kKUeOHPL09LSuV758eZUvX976efTo0Vq1apW+++47myA0PUJCQhQcHCxJGjdunKZPn67du3erUaNGypo1q83PxcfHRzt27NCyZcus31eSsmXLpjlz5sjJyUmS9NlnnykxMVFz586Vs7OzypQpo99//109evRIsv0PP/xQr776qiTpxo0bmj17tubPn6/GjRtLkj7//HNt2rRJc+fO1aBBgzRr1ixlz55dS5cuVdasWSVJfn5+SequWrVKHTt21Jw5c9S2bVu79k1GIagEAAAAAAB4RoSGhqp///42Y2azOcX5lSpVsr6PiYlR9erVZTKZrGM1a9ZUXFycfv/9d124cEHx8fGqX79+qj0EBwerUKFC2rx5s1xcXNLVf7ly5azv75+pWbZsWZux27dvKzY2Vh4eHjpw4ICioqJsLj9PSEjQ7du3dfPmTbm6uia7nbi4OI0aNUo//PCDzp8/r3v37unWrVuPdUblg71ny5ZNHh4eunjxonVs1qxZ+uKLL3Tu3DndunVLd+7cUYUKFWxqlC1b1hpSSrKeHens7Gwdq169erLbr1y5svX96dOndffuXdWsWdM6ljVrVlWtWlUxMTGSpOjoaNWuXdsaUiZn165d+v7777VixYpM8QRwgkoAAAAAAIBnhNlsTjWYfFi2bNnSPDetoWOTJk305ZdfaseOHXrllVfSXF+STWh2PzBNbiwxMVHSP4FjWFiYWrZsmaTWg+HewwYOHKhNmzZp8uTJKlGihFxcXPTmm28+1gOFHg78TCaTtc+lS5dq4MCBmjJliqpXry53d3dNmjRJu3btslknPT+Ph6V33bT8PIsXL67cuXPriy++UNOmTVMNNZ8G7lEJAAAAAADwArj/8BqLxWIdi4qKkru7uwoVKiRfX1+5uLgoIiIi1To9evRQeHi4mjdvrq1btz7RnitWrKjjx4+rRIkSSV4ODv/EWlmzZlVCQoLNelFRUQoJCVGLFi1UtmxZeXp66uzZs0+sz6ioKNWoUUM9e/ZUQECASpQokeJDax7k7++vgwcP6vbt29axnTt3PnK94sWLW+89et/du3e1Z88elS5dWtI/Z4Bu27ZNd+/eTbFOnjx5tHnzZp06dUpt2rRJde7TQFAJAAAAAADwAujZs6f++9//6t1339WxY8f07bffauTIkerfv78cHBzk7OysIUOGaPDgwVq4cKFOnz6tnTt3au7cuUlqvfvuuxozZoyaNWumn3/++Yn1PGLECC1cuFBhYWE6cuSIYmJitHTpUn3wwQfWOd7e3oqIiNCFCxd05coVSf/cm3PlypWKjo7WgQMH1L59e+vZj0+Cr6+v9u7dqw0bNujEiRMaPnx4kidwJ6d9+/YymUzq2rWrjh49qrVr12ry5MmPXC9btmzq0aOHBg0apPXr1+vo0aPq2rWrbt68qS5dukiSevfurdjYWLVr10579+7VyZMntWjRIh0/ftymVr58+bR582YdO3ZMwcHByT5s52khqAQAAAAAAHgBFCxYUGvXrtXu3btVvnx5vfPOO+rSpYtN6Dd8+HANGDBAI0aMkL+/v9q2bWtzH8YH9evXT2FhYWrSpIm2b9/+RHpu2LChvv/+e23cuFFVqlRRtWrVNHXqVBUtWtQ6Z8qUKdq0aZMKFy6sgIAASdJHH32knDlzqkaNGgoKClLDhg1VsWLFJ9KjJHXv3l0tW7ZU27Zt9fLLL+vSpUvq2bPnI9dzc3PTmjVrdOjQIQUEBGjYsGGaMGFCmrYZHh6uVq1a6a233lLFihV16tQpbdiwQTlz5pQk5c6dW5s3b1ZcXJzq1q2rSpUq6fPPP0/28m5PT09t3rxZhw4dUocOHZKcofq0cI9KAAAAAACA51BkZGSSsbp162r37t0pruPg4KBhw4Zp2LBhSZZ5e3vbXDYuSf3790/ycJ+UPLxucvUCAwOTjDVs2ND6VPDkBAUFKSgoKEntzZs324z16tXL5nN6LgV/uCdJunr1qvW92WzWvHnzNG/ePJs548ePt76fP39+srWrVaum6OjoFLeX3D6R/rlH5/Tp0zV9+vQU+y5Xrpw2bNiQ7LKH+/Hy8kpytuXTxhmVAAAAAAAAAAxHUIk0CwwMVL9+/VJc7u3trY8//vip9QMAAAAAADKHr776Sm5ubsm+ypQpY3R7qXqWe3/ecOk3AAAAAAAAHkvz5s318ssvJ7ssuXsiZibPcu/PG4JKAAAAAAAAPBZ3d3e5u7sb3YZdnuXenzdc+o10uXfvnnr37q3s2bMrT548Gj58eLI3dD179qxMJpPNzWCvXr0qk8lkczPfw4cPq3HjxnJzc1P+/Pn11ltv6e+//7YuX7FihcqWLSsXFxflzp1bDRo00I0bN57kVwQAAAAAAIABCCqRLgsWLFCWLFm0e/duTZs2TR999JHmzJljV62rV6/qlVdeUUBAgPbu3av169frf//7n9q0aSNJOn/+vIKDg9W5c2fFxMQoMjJSLVu2TDYYBQAAAAAAwLONS7+RLoULF9bUqVNlMplUsmRJHTp0SFOnTlXXrl3TXWvmzJkKCAjQuHHjrGNffPGFChcurBMnTiguLk737t1Ty5YtVbRoUUlS2bJlM+y7AAAAAAAAIPPgjEqkS7Vq1WQymayfq1evrpMnTyohISHdtQ4cOKAtW7bYPE2rVKlSkqTTp0+rfPnyql+/vsqWLavWrVvr888/15UrV1KtGR8fr9jYWJvX3fh76e4NAAAAAAAATxdBJZ4IB4d/Dq0HL9O+e/euzZy4uDgFBQUpOjra5nXy5EnVqVNHjo6O2rRpk9atW6fSpUtrxowZKlmypM6cOZPidsePH6/s2bPbvNbP3vxkviQAAAAAAAAyDEEl0mXXrl02n3fu3ClfX185OjrajOfNm1fSP/eZvO/BB+tIUsWKFXXkyBF5e3urRIkSNq9s2bJJkkwmk2rWrKmwsDDt379fTk5OWrVqVYr9hYaG6tq1azavRj1eeZyvDAAAAAAAgKeAoBLpcu7cOfXv31/Hjx/XkiVLNGPGDPXt2zfJPBcXF1WrVk3h4eGKiYnR1q1b9cEHH9jM6dWrly5fvqzg4GDt2bNHp0+f1oYNG9SpUyclJCRo165dGjdunPbu3atz585p5cqV+uuvv+Tv759if2azWR4eHjavrGZuxQoAAAAAAJDZkeAgXTp27Khbt26patWqcnR0VN++fdWtW7dk537xxRfq0qWLKlWqpJIlS2rixIl67bXXrMsLFCigqKgoDRkyRK+99pri4+NVtGhRNWrUSA4ODvLw8NBPP/2kjz/+WLGxsSpatKimTJmixo0bP62vCwAAAAAAgKeEoBJpFhkZaX0/e/bsJMvPnj1r89nf31/bt2+3GXvwnpWS5Ovrq5UrVya7PX9/f61fv96+ZgEAAAAAAPBM4dJvAAAAAAAAAIYjqAQAAAAAAABgOIJKAAAAAAAAAIYjqAQAAAAAAABgOIJKAAAAAAAAAIYjqAQAAAAAAABgOIJKAAAAAAAAAIYjqAQAAAAAAABgOIJKAAAAAAAAAIYjqAQAAAAAAABgOIJKAAAAAAAAAIYjqAQAAAAAAABgOIJKAAAAAAAAAIYjqAQAAAAAAABgOIJKAAAAAAAAAIYjqAQAAAAAAABgOIJKAAAAAAAAAIYjqAQAAAAAAABgOIJKAAAAAAAAAIYjqAQAAAAAAABgOIJKAAAAAAAAAIYjqAQAAAAAAABgOIJKAAAAAAAAAIYjqAQAAAAAAABgOIJKAAAAAAAAAIYjqAQAAAAAAABgOIJKAAAAAAAAAIYjqAQAAAAAAABgOIJKAAAAAAAAAIYjqAQAAAAAAABgOIJKAAAAAAAAAIYjqAQAAAAAAABgOIJKAAAAAAAAAIYjqAQAAAAAAABgOIJKAAAAAAAAAIYjqAQAAAAAAABgOIJKAAAAAAAAAIYjqAQAAAAAAABgOIJKAAAAAAAAAIYjqAQAAAAAAABgOIJKAAAAAAAAAIbLYnQDAAAAAAAAzxL/S+cM2/asjas0a9YsSVKvXr3Uq1cvw3oBMhpBJQAAAAAAwDOCcBLPMy79BgAAAAAAAGA4gkoAAAAAAAAAhiOoBAAAAAAAAGA4gkoAAAAAAAAAhiOoBAAAAAAAAGA4gkoAAAAAAAAAhiOoBAAAAAAAAGA4gkoAAAAAAAAAhiOoBAAAAAAAAGA4gkoAAAAAAAAAhiOoBAAAAAAAAGA4gkoAAAAAAAAAhiOoBAAAAAAAAGA4gkoAAAAAAAAAhiOoBAAAAAAAAGA4gkoAAAAAAAAAhiOoBAAAAAAAAGA4gkoAAAAAAAAAhiOoBAAAAAAAAGA4gkoAAAAAAAAAhiOoBAAAAAAAAGA4gkoAAAAAAAAAhiOoBAAAAAAAAGA4gkoAAAAAAAAAhiOoBAAAAAAAAGA4gkoAAAAAAAAAhiOoBAAAAAAAAGA4gkoAAAAAAAAAhiOoBAAAAAAAAGA4k8VisRjdBPAkTWnbLF3z3SbUlSTFDdma5PPDyx5e50EPr//w+MPLHqyf3DbS239aaqTW29OW3D68LyP7cptQN131kvuZ36+R2s89vdtO7VhIS93k1kvtmHpULykdvw/WedTP7FHfKbmaD/eR0rKMUvO9DxQ1dcxj15nh0yMDugEAAHj+nA1vanQLT8TOnTsN23a1atUM2zbwpHFGJQAAAAAAAADDEVQCAAAAAAAAMBxBJQAAAAAAAADDEVQCAAAAAAAAMBxBJQAAAAAAAADDEVQCAAAAAAAAMBxBJQAAAAAAAADDEVQCAAAAAAAAMBxBJQAAAAAAAADDEVQCAAAAAAAAMBxBJQAAAAAAAADDEVQCAAAAAAAAMBxBJQAAAAAAAADDEVQCAAAAAAAAMBxBJQAAAAAAAADDEVQCAAAAAAAAMBxBJQAAAAAAAADDEVQCAAAAAAAAMBxBJQAAAAAAAADDEVQCAAAAAAAAMBxBJQAAAAAAAADDEVQCAAAAAAAAMBxBJQAAAAAAAADDEVQCAAAAAAAAMBxBJQAAAAAAAADDEVQCAAAAAAA8hwIDA9WvX78Ul5tMJq1evTrN9SIjI2UymXT16tXH7g1IThajGwAAAAAAAMDTd/78eeXMmdPoNgArgkoAAAAAAIAXkKenp9EtADa49BsAAAAAAOA5lZiYqMGDBytXrlzy9PTUqFGjrMsevPT77NmzMplMWrp0qWrUqCFnZ2e99NJL2rp1a5Ka+/btU+XKleXq6qoaNWro+PHjNstnz56t4sWLy8nJSSVLltSiRYtslptMJs2ePVuNGzeWi4uLihUrphUrVmT4d8ezh6ASAAAAAADgGREfH6/Y2FibV3x8fIrzFyxYoGzZsmnXrl2aOHGiPvzwQ23atCnF+YMGDdKAAQO0f/9+Va9eXUFBQbp06ZLNnGHDhmnKlCnau3evsmTJos6dO1uXrVq1Sn379tWAAQN0+PBhde/eXZ06ddKWLVtsagwfPlytWrXSgQMH1KFDB7Vr104xMTF27hU8LwgqAQAAAAAAnhHjx49X9uzZbV7jx49PcX65cuU0cuRI+fr6qmPHjqpcubIiIiJSnN+7d2+1atVK/v7+mj17trJnz665c+fazBk7dqzq1q2r0qVLa+jQodq+fbtu374tSZo8ebJCQkLUs2dP+fn5qX///mrZsqUmT55sU6N169b697//LT8/P40ePVqVK1fWjBkzHmPP4HlAUAkAAAAAAPCMCA0N1bVr12xeoaGhKc4vV66czWcvLy9dvHgxxfnVq1e3vs+SJYsqV66c5EzHB2t6eXlJkrVmTEyMatasaTO/Zs2aSWo8uJ37nzmjEjxMBwAAAAAA4BlhNptlNpvTPD9r1qw2n00mkxITEx+rhwdrmkwmSXrsmoDEGZUAAAAAAAD4/3bu3Gl9f+/ePe3bt0/+/v5pXt/f319RUVE2Y1FRUSpdunSK27n/OT3bwfOJMyoBAAAAAAAgSZo1a5Z8fX3l7++vqVOn6sqVKzYPy3mUQYMGqU2bNgoICFCDBg20Zs0arVy5Uj/++KPNvOXLl6ty5cqqVauWvvrqK+3evTvJvTDx4iGoBAAAAAAAgCQpPDxc4eHhio6OVokSJfTdd98pT548aV7/jTfe0LRp0zR58mT17dtXPj4+mjdvngIDA23mhYWFaenSperZs6e8vLy0ZMmSJGdd4sVDUAkAAAAAAPAcioyMTDK2evVq63uLxZJkub+/v3bt2pVsvcDAwCTrVKhQIclYjx491KNHj1R7K1CggDZu3JjqHLx4uEclAAAAAAAAAMMRVAIAAAAAAAAwHJd+AwAAAAAAvOC8vb2TvRT8SXha28GzhzMqAQAAAAAAABiOoBIAAAAAAACA4QgqAQAAAAAAABiOoBIAAAAAAACA4QgqAQAAAAAAABiOoBIAAAAAAACA4QgqAQAAAAAAABiOoBIAAAAAAACA4QgqAQAAAAAAABiOoBIAAAAAAACA4QgqAQAAAAAAABiOoBIAAAAAAACA4QgqAQAAAAAAABiOoBIAAAAAAACA4QgqAQAAAAAAABiOoBIAAAAAAACA4QgqAQAAAAAAABiOoBIAAAAAAACA4QgqAQAAAAAAABiOoBIAAAAAAACA4QgqAQAAAAAAABiOoBIAAAAAAACA4QgqAQAAAAAAABiOoBIAAAAAAACA4QgqAQAAAAAAABiOoBIAAAAAAACA4QgqAQAAAAAAABiOoBIAAAAAAACA4QgqAQAAAAAAABiOoPIFc/bsWZlMJkVHRz/xbc2fP185cuTIsHqRkZEymUy6evVqhtUEAAAAAABA5kBQiSembdu2OnHihNFtAAAAAAAA4BmQxegG8PxycXGRi4uL0W0AAAAAAADgGcAZlc+pxMRETZw4USVKlJDZbFaRIkU0duzYJPMSEhLUpUsX+fj4yMXFRSVLltS0adNs5kRGRqpq1arKli2bcuTIoZo1a+q3336TJB04cED16tWTu7u7PDw8VKlSJe3du1dS8pd+r1mzRlWqVJGzs7Py5MmjFi1aWJctWrRIlStXlru7uzw9PdW+fXtdvHgxg/cMAAAAAAAAMiPOqHxOhYaG6vPPP9fUqVNVq1YtnT9/XseOHUsyLzExUYUKFdLy5cuVO3dubd++Xd26dZOXl5fatGmje/fu6Y033lDXrl21ZMkS3blzR7t375bJZJIkdejQQQEBAZo9e7YcHR0VHR2trFmzJtvTDz/8oBYtWmjYsGFauHCh7ty5o7Vr11qX3717V6NHj1bJkiV18eJF9e/fXyEhITZzAAAAAAAA8HwiqHwOXb9+XdOmTdPMmTP19ttvS5KKFy+uWrVq6ezZszZzs2bNqrCwMOtnHx8f7dixQ8uWLVObNm0UGxura9euqVmzZipevLgkyd/f3zr/3LlzGjRokEqVKiVJ8vX1TbGvsWPHql27djbbK1++vPV9586dre+LFSum6dOnq0qVKoqLi5Obm5sdewIAAAAAAADPCi79fg7FxMQoPj5e9evXT9P8WbNmqVKlSsqbN6/c3Nz02Wef6dy5c5KkXLlyKSQkRA0bNlRQUJCmTZum8+fPW9ft37+//v3vf6tBgwYKDw/X6dOnU9xOdHR0qj3t27dPQUFBKlKkiNzd3VW3bl1JsvaSFvHx8YqNjbV53UtISPP6AAAAAAAAMAZB5XMoPQ+wWbp0qQYOHKguXbpo48aNio6OVqdOnXTnzh3rnHnz5mnHjh2qUaOGvv76a/n5+Wnnzp2SpFGjRunIkSNq2rSpNm/erNKlS2vVqlXp7uvGjRtq2LChPDw89NVXX2nPnj3WOg/28ijjx49X9uzZbV4RMSmHpwAAAAAAAMgcCCqfQ76+vnJxcVFERMQj50ZFRalGjRrq2bOnAgICVKJEiWTPigwICFBoaKi2b9+ul156SYsXL7Yu8/Pz03vvvaeNGzeqZcuWmjdvXrLbKleuXIo9HTt2TJcuXVJ4eLhq166tUqVK2fUgndDQUF27ds3mVd+/eLrrAAAAAAAA4OniHpXPIWdnZw0ZMkSDBw+Wk5OTatasqb/++ktHjhxJcum1r6+vFi5cqA0bNsjHx0eLFi3Snj175OPjI0k6c+aMPvvsMzVv3lwFChTQ8ePHdfLkSXXs2FG3bt3SoEGD9Oabb8rHx0e///679uzZo1atWiXb18iRI1W/fn0VL15c7dq1071797R27VoNGTJERYoUkZOTk2bMmKF33nlHhw8f1ujRo9P93c1ms8xms81YFkfHdNcBAAAAAADA00VQ+ZwaPny4smTJohEjRujPP/+Ul5eX3nnnnSTzunfvrv3796tt27YymUwKDg5Wz549tW7dOkmSq6urjh07pgULFujSpUvy8vJSr1691L17d927d0+XLl1Sx44d9b///U958uRRy5YtbR6W86DAwEAtX75co0ePVnh4uDw8PFSnTh1JUt68eTV//ny9//77mj59uipWrKjJkyerefPmT24nAQAAAAAAINMgqHxOOTg4aNiwYRo2bFiSZRaLxfrebDZr3rx5SS7XHj9+vCQpf/78Kd5z0snJSUuWLEmxh5CQEIWEhNiMtWzZUi1btkx2fnBwsIKDg1PsNTAw0OYzAAAAAAAAnh/coxIAAAAAAACA4QgqAQAAAAAAABiOoBIAAAAAAACA4QgqAQAAAAAAABiOoBIAAAAAAACA4QgqAQAAAAAAABiOoBIAAAAAAACA4QgqAQAAAAAAABiOoBIAAAAAAACA4QgqAQAAAAAAABiOoBIAAAAAAACA4bIY3QAAAAAAAMCzJGrqGMO2va9OY82aNUuS1KtXL/Xq1cuwXoCMRlAJAAAAAADwjCCcxPOMS78BAAAAAAAAGI6gEgAAAAAAAIDhCCoBAAAAAAAAGI57VAIAAAAAAABIl6tXr2r37t26ePGiEhMTbZZ17NjRrpoElQAAAAAAAADSbM2aNerQoYPi4uLk4eEhk8lkXWYymewOKrn0GwAAAAAAAECaDRgwQJ07d1ZcXJyuXr2qK1euWF+XL1+2uy5BJQAAAAAAAIA0++OPP9SnTx+5urpmaF2CSgAAAAAAAABp1rBhQ+3duzfD63KPSgAAAAAAAABp1rRpUw0aNEhHjx5V2bJllTVrVpvlzZs3t6suQSUAAAAAAACANOvatask6cMPP0yyzGQyKSEhwa66BJUAAAAAAAAA0iwxMfGJ1OUelQAAAAAAAAAM99hBZUJCgqKjo3XlypWM6AcAAAAAAABAJrd161YFBQWpRIkSKlGihJo3b65t27Y9Vs10B5X9+vXT3LlzJf0TUtatW1cVK1ZU4cKFFRkZ+VjNAAAAAAAAAMjcvvzySzVo0ECurq7q06eP+vTpIxcXF9WvX1+LFy+2u26671G5YsUK/etf/5IkrVmzRmfOnNGxY8e0aNEiDRs2TFFRUXY3AwAAAAAAACBzGzt2rCZOnKj33nvPOtanTx999NFHGj16tNq3b29X3XSfUfn333/L09NTkrR27Vq1bt1afn5+6ty5sw4dOmRXEwAAAAAAAACeDb/++quCgoKSjDdv3lxnzpyxu266g8r8+fPr6NGjSkhI0Pr16/Xqq69Kkm7evClHR0e7GwEAAAAAAACQ+RUuXFgRERFJxn/88UcVLlzY7rrpvvS7U6dOatOmjby8vGQymdSgQQNJ0q5du1SqVCm7GwEAAAAAAACQ+Q0YMEB9+vRRdHS0atSoIUmKiorS/PnzNW3aNLvrpjuoHDVqlF566SX997//VevWrWU2myVJjo6OGjp0qN2NAAAAAAAAAMj8evToIU9PT02ZMkXLli2TJPn7++vrr7/W66+/bnfddAeVkvTmm29Kkm7fvm0de/vtt+1uAgAAAAAAAMCzo0WLFmrRokWG1kz3PSoTEhI0evRoFSxYUG5ubvr1118lScOHD9fcuXMztDkAAAAAAAAAL4Z0B5Vjx47V/PnzNXHiRDk5OVnHX3rpJc2ZMydDmwMAAAAAAABgvFy5cunvv/+WJOXMmVO5cuVK8WWvdF/6vXDhQn322WeqX7++3nnnHet4+fLldezYMbsbAQAAAAAAAJA5TZ06Ve7u7tb3JpMpw7eR7qDyjz/+UIkSJZKMJyYm6u7duxnSFAAAAAAAAIDM48Hn04SEhDyRbaT70u/SpUtr27ZtScZXrFihgICADGkKAAAAAAAAQObk6OioixcvJhm/dOmSHB0d7a6b7jMqR4wYobffflt//PGHEhMTtXLlSh0/flwLFy7U999/b3cjAAAAAAAAADI/i8WS7Hh8fLzNM23SK91B5euvv641a9boww8/VLZs2TRixAhVrFhRa9as0auvvmp3IwAAAAAAAAAyr+nTp0uSTCaT5syZIzc3N+uyhIQE/fTTTypVqpTd9dMdVEpS7dq1tWnTJrs3CgAAAAAAAODZMnXqVEn/nFH5ySef2Fzm7eTkJG9vb33yySd21093UPnf//5XJpNJhQoVkiTt3r1bixcvVunSpdWtWze7GwEAAAAAAACQeZ05c0aSVK9ePa1cuVI5c+bM0PrpfphO+/bttWXLFknShQsX1KBBA+3evVvDhg3Thx9+mKHNAQAAAAAAAMhctmzZkuEhpWRHUHn48GFVrVpVkrRs2TKVLVtW27dv11dffaX58+dndH8AAAAAAAAAMpFWrVppwoQJScYnTpyo1q1b21033UHl3bt3ZTabJUk//vijmjdvLkkqVaqUzp8/b3cjAAAAAAAAADK/n376SU2aNEky3rhxY/3000921013UFmmTBl98skn2rZtmzZt2qRGjRpJkv7880/lzp3b7kYAAAAAAAAAZH5xcXFycnJKMp41a1bFxsbaXTfdQeWECRP06aefKjAwUMHBwSpfvrwk6bvvvrNeEg4AAAAAAADg+VS2bFl9/fXXScaXLl2q0qVL21033U/9DgwM1N9//63Y2Fibm2Z269ZNrq6udjcCAAAAAAAAIPMbPny4WrZsqdOnT+uVV16RJEVERGjJkiVavny53XXTHVTeunVLFovFGlL+9ttvWrVqlfz9/dWwYUO7GwEAAAAAAACQ+QUFBWn16tUaN26cVqxYIRcXF5UrV04//vij6tata3fddAeVr7/+ulq2bKl33nlHV69e1csvv6ysWbPq77//1kcffaQePXrY3QwAAAAAAACAzK9p06Zq2rRphtZM9z0qf/nlF9WuXVuStGLFCuXPn1+//fabFi5cqOnTp2docwAAAAAAAABeDOk+o/LmzZtyd3eXJG3cuFEtW7aUg4ODqlWrpt9++y3DGwQAAAAAAABgrFy5cunEiRPKkyePcubMKZPJlOLcy5cv27WNdAeVJUqU0OrVq9WiRQtt2LBB7733niTp4sWL8vDwsKsJAAAAAAAAAJnX1KlTrScvfvzxx09kG+kOKkeMGKH27dvrvffeU/369VW9enVJ/5xdGRAQkOENAgAAAAAAADDW22+/nez7jJTuoPLNN99UrVq1dP78eZUvX946Xr9+fbVo0SJDmwMAAAAAAABgvNjY2DTPtfeq63QHlZLk6ekpT09Pm7GqVava1QAAAAAAAACAzC1Hjhyp3pfyQQkJCXZtw66gcu/evVq2bJnOnTunO3fu2CxbuXKlXY0AAAAAAAAAyJy2bNlifX/27FkNHTpUISEh1ttC7tixQwsWLND48ePt3ka6g8qlS5eqY8eOatiwoTZu3KjXXntNJ06c0P/+9z8u/QYAAAAAAACeQ3Xr1rW+//DDD/XRRx8pODjYOta8eXOVLVtWn332md33sHRI7wrjxo3T1KlTtWbNGjk5OWnatGk6duyY2rRpoyJFitjVBAAAAAAAAIBnw44dO1S5cuUk45UrV9bu3bvtrpvuoPL06dNq2rSpJMnJyUk3btyQyWTSe++9p88++8zuRgAAAAAAAABkfoULF9bnn3+eZHzOnDkqXLiw3XXTfel3zpw5df36dUlSwYIFdfjwYZUtW1ZXr17VzZs37W4EAAAAAAAAQOY3depUtWrVSuvWrdPLL78sSdq9e7dOnjypb775xu666T6jsk6dOtq0aZMkqXXr1urbt6+6du2q4OBg1a9f3+5GAAAAAAAAAGR+TZo00YkTJxQUFKTLly/r8uXLCgoK0okTJ9SkSRO766b7jMqZM2fq9u3bkqRhw4Ypa9as2r59u1q1aqUPPvjA7kYAAAAAAAAAPBsKFy6scePGZWjNdAeVuXLlsr53cHDQ0KFDM7QhAAAAAAAAAJnbtm3b9Omnn+rXX3/V8uXLVbBgQS1atEg+Pj6qVauWXTXTdOl3bGxsml8AAAAAAAAAnl/ffPONGjZsKBcXF/3yyy+Kj4+XJF27du2xzrJM0xmVOXLkkMlkSnWOxWKRyWRSQkKC3c0AAAAAAAAAyNzGjBmjTz75RB07dtTSpUut4zVr1tSYMWPsrpumoHLLli12bwAAAAAAAADA8+P48eOqU6dOkvHs2bPr6tWrdtdNU1BZt25duzcAAAAAAACAzOPs2bPy8fHR/v37VaFChSe6rfnz56tfv36PFV49KDIyUvXq1dOVK1eUI0eODKmJ9PP09NSpU6fk7e1tM/7zzz+rWLFidtdN0z0qJenkyZMKDg5O9j6U165dU/v27fXrr7/a3QgAAAAAAACeL23bttWJEyeMbgMZrGvXrurbt6927dolk8mkP//8U1999ZUGDhyoHj162F03zU/9njRpkgoXLiwPD48ky7Jnz67ChQtr0qRJmj17tt3NAAAAAAAA4Pnh4uIiFxcXo9tABhs6dKgSExNVv3593bx5U3Xq1JHZbNbAgQP17rvv2l03zWdUbt26Va1bt05xeZs2bbR582a7GwEAAAAAAEDGSUxM1MSJE1WiRAmZzWYVKVJEY8eOTTIvISFBXbp0kY+Pj1xcXFSyZElNmzbNZk5kZKSqVq2qbNmyKUeOHKpZs6Z+++03SdKBAwdUr149ubu7y8PDQ5UqVdLevXsl/XPp98OXaK9Zs0ZVqlSRs7Oz8uTJoxYtWliXLVq0SJUrV5a7u7s8PT3Vvn17Xbx4MYP3DB5HQkKCtm3bpl69euny5cs6fPiwdu7cqb/++kujR49+rNppPqPy3LlzypcvX4rL8+TJo//+97+P1QwAAAAAAABSFh8fr/j4eJsxs9kss9mcZG5oaKg+//xzTZ06VbVq1dL58+d17NixJPMSExNVqFAhLV++XLlz59b27dvVrVs3eXl5qU2bNrp3757eeOMNde3aVUuWLNGdO3e0e/dumUwmSVKHDh0UEBCg2bNny9HRUdHR0cqaNWuy/f/www9q0aKFhg0bpoULF+rOnTtau3atdfndu3c1evRolSxZUhcvXlT//v0VEhJiMwfGcnR01GuvvaaYmBjlyJFDpUuXzrDaJovFYknLRE9PTy1evFivvPJKsssjIiLUoUMHXbhwIcOaAzLClLbNJEk13/tAkhQ1dYzcJiT/gKi4IVuty+KGbJUkm7n3xx4eT075C7Wt76OmjknTOsmtK0kHPLelOvfh5ffXf3D8/rzkliW33fvLy1+obe3//ndIqf6z6v73iBuy1XqcpDbv/neNyV1EkuR/6VySeQ/uowePpeT28YM1n3UP76O0zIuaOibF/Z7SPnpwPLmfW2r79sFtP/izSk//GeHhbT+pdR5VI601Y3IXSXKsp3UbyS2XHr2fU/odk/7v7+qjfmcf/BuZ3Pae5s88M3nw7959bhPqavwnGfePTAAAJOlseFOjW3gi7v/vTCNc96+ssLAwm7GRI0dq1KhRtvOuX1fevHk1c+ZM/fvf/7ZZlpaH6fTu3VsXLlzQihUrdPnyZeXOnVuRkZHJPnDZw8NDM2bM0Ntvv51k2cMP06lRo4aKFSumL7/8Mk3fd+/evapSpYquX78uNzc3HqaTSVSuXFkTJkxQ/fr1M7Rumi/9rlOnjmbMmJHi8unTp6t27RfrH/kAAAAAAABPU2hoqK5du2bzCg0NTTIvJiZG8fHxaQ6SZs2apUqVKilv3rxyc3PTZ599pnPn/vk/jHPlyqWQkBA1bNhQQUFBmjZtms6fP29dt3///vr3v/+tBg0aKDw8XKdPn05xO9HR0an2tG/fPgUFBalIkSJyd3e3BqP3e0HmMGbMGA0cOFDff/+9zp8/r9jYWJuXvdIcVIaGhmrdunV68803tXv3busvw65du9SqVStt2LAh2V8MAAAAAAAAZAyz2SwPDw+bV3KXfafnATZLly7VwIED1aVLF23cuFHR0dHq1KmT7ty5Y50zb9487dixQzVq1NDXX38tPz8/7dy5U5I0atQoHTlyRE2bNtXmzZtVunRprVq1KtltpdbXjRs31LBhQ3l4eOirr77Snj17rHUe7AXGa9KkiQ4cOKDmzZurUKFCypkzp3LmzKkcOXIoZ86cdtdN8z0qAwICtGLFCnXu3DnJwZY7d24tW7ZMFStWtLsRAAAAAAAAZAxfX1+5uLgoIiIiyaXfD4uKilKNGjXUs2dP61hyZ0UGBAQoICBAoaGhql69uhYvXqxq1apJkvz8/OTn56f33ntPwcHBmjdvns1Dcu4rV66cIiIi1KlTpyTLjh07pkuXLik8PFyFCxeWJOtDeZC5bNmy5YnUTXNQKUnNmjXTb7/9pvXr1+vUqVOyWCzy8/PTa6+9JldX1yfSIAAAAAAAANLH2dlZQ4YM0eDBg+Xk5KSaNWvqr7/+0pEjR5Jceu3r66uFCxdqw4YN8vHx0aJFi7Rnzx75+PhIks6cOaPPPvtMzZs3V4ECBXT8+HGdPHlSHTt21K1btzRo0CC9+eab8vHx0e+//649e/aoVatWyfY1cuRI1a9fX8WLF1e7du107949rV27VkOGDFGRIkXk5OSkGTNm6J133tHhw4cf+ynSyHgWi0UFChTQnTt3VLJkSWXJkq54MVXpruTi4pJsIg4AAAAAAIDMY/jw4cqSJYtGjBihP//8U15eXnrnnXeSzOvevbv279+vtm3bymQyKTg4WD179tS6deskSa6urjp27JgWLFigS5cuycvLS7169VL37t117949Xbp0SR07dtT//vc/5cmTRy1btkzywJ/7AgMDtXz5co0ePVrh4eHy8PBQnTp1JEl58+bV/Pnz9f7772v69OmqWLGiJk+erObNmz+5nYR0OXPmjJo3b66jR49KkgoVKqRvvvlGlStXzpD6GRd5AgAAAAAAINNwcHDQsGHDNGzYsCTLLBaL9b3ZbNa8efM0b948mznjx4+XJOXPnz/Fe046OTlpyZIlKfYQEhKikJAQm7GWLVuqZcuWyc4PDg5WcHBwir0GBgbafMbTNWjQIN27d09ffvmlnJ2dNXnyZHXv3l379u3LkPoElQAAAAAAAAAe6eeff9aKFStUq1YtSVK1atVUqFAh3bhxQ9myZXvs+ml+6jcAAAAAAACAF9fFixfl6+tr/ezl5SUXFxddvHgxQ+pzRiUAAAAAAACARzKZTIqLi5OLi4t1zMHBQdevX1dsbKx1zMPDw6766Q4q165dK0dHRzVs2NBmfMOGDUpMTFTjxo3tagQAAAAAAABA5mWxWOTn55dkLCAgwPreZDIpISHBrvrpDiqHDh2q8PDwZBsdOnQoQSUAAAAAAADwHNqyZcsTrZ/uoPLkyZMqXbp0kvFSpUrp1KlTGdIUAAAAAAAAgMylbt26T7R+uh+mkz17dv36669Jxk+dOpUhT/cBAAAAAAAA8OJJd1D5+uuvq1+/fjp9+rR17NSpUxowYICaN2+eoc0BAAAAAAAAeDGkO6icOHGismXLplKlSsnHx0c+Pj7y9/dX7ty5NXny5CfRIwAAAAAAAIDnXLrvUZk9e3Zt375dmzZt0oEDB+Ti4qJy5cqpTp06T6I/AAAAAAAAAC+AdAeVkmQymfTaa6/ptddey+h+AAAAAAAAADwDTp06pdOnT6tOnTpycXGRxWKRyWSyu16agsrp06erW7ducnZ21vTp01Od26dPH7ubAQAAAAAAAJC5Xbp0SW3bttXmzZtlMpl08uRJFStWTF26dFHOnDk1ZcoUu+qmKaicOnWqOnToIGdnZ02dOjXFeSaTiaASAAAAAAAAeI699957ypIli86dOyd/f3/reNu2bdW/f/8nG1SeOXMm2fcAAAAAAAAAXiwbN27Uhg0bVKhQIZtxX19f/fbbb3bXTfdTvx+WkJCg6OhoXbly5XFLAQAAAAAAAMjkbty4IVdX1yTjly9fltlstrtuuoPKfv36ae7cuZL+CSnr1KmjihUrqnDhwoqMjLS7EQAAAAAAAACZX+3atbVw4ULrZ5PJpMTERE2cOFH16tWzu266n/q9YsUK/etf/5IkrVmzRmfPntWxY8e0aNEiDRs2TFFRUXY3AwAAAAAAACBzmzhxourXr6+9e/fqzp07Gjx4sI4cOaLLly8/VjaY7jMq//77b3l6ekqS1q5dq9atW8vPz0+dO3fWoUOH7G4EAAAAAAAAQOb30ksv6cSJE6pVq5Zef/113bhxQy1bttT+/ftVvHhxu+um+4zK/Pnz6+jRo/Ly8tL69es1e/ZsSdLNmzfl6OhodyMAAAAAAAAAMre7d++qUaNG+uSTTzRs2LAMrZ3uoLJTp05q06aNvLy8ZDKZ1KBBA0nSrl27VKpUqQxtDgAAAAAAAEDmkTVrVh08ePCJ1E73pd+jRo3SnDlz1K1bN0VFRVmf5OPo6KihQ4dmeIMAAAAAAAAAMo9//etf1odtZ6R0n1EpSW+++WaSsbfffvuxmwEAAAAAAACQud27d09ffPGFfvzxR1WqVEnZsmWzWf7RRx/ZVTfdQeWHH36Y6vIRI0bY1QgAAAAAAACAzO/w4cOqWLGiJOnEiRM2y0wmk9110x1Urlq1yubz3bt3debMGWXJkkXFixcnqAQAAAAAAACeY1u2bHkiddMdVO7fvz/JWGxsrEJCQtSiRYsMaQoAAAAAAADAi8Wue1Q+zMPDQ2FhYQoKCtJbb72VESUBAAAAAAAAZEI3btxQeHi4IiIidPHiRSUmJtos//XXX+2qmyFBpSRdu3ZN165dy6hyAAAAAAAAADKhf//739q6daveeusteXl5PdZ9KR+U7qBy+vTpNp8tFovOnz+vRYsWqXHjxhnSFAAAAAAAAIDMad26dfrhhx9Us2bNDK2b7qBy6tSpNp8dHByUN29evf322woNDc2wxgAAAAAAAABkPjlz5lSuXLkyvG66g8ozZ85keBMAAAAAAAAAng2jR4/WiBEjtGDBArm6umZY3Qy7RyUAAAAAAACA59+UKVN0+vRp5c+fX97e3sqaNavN8l9++cWuumkOKjt37pymeV988YVdjQAAAAAAAADI/N54440nUjfNQeX8+fNVtGhRBQQEyGKxPJFmAAAAAAAAAGRuI0eOfCJ10xxU9ujRQ0uWLNGZM2fUqVMn/etf/3oiN80EAAAAAAAAkLldvXpVK1as0OnTpzVo0CDlypVLv/zyi/Lnz6+CBQvaVdMhrRNnzZql8+fPa/DgwVqzZo0KFy6sNm3aaMOGDZxhCQAAAAAAALwgDh48KD8/P02YMEGTJ0/W1atXJUkrV65UaGio3XXTHFRKktlsVnBwsDZt2qSjR4+qTJky6tmzp7y9vRUXF2d3EwAAAAAAAACeDf3791dISIhOnjwpZ2dn63iTJk30008/2V03XUGlzYoODjKZTLJYLEpISLC7AQAAAAAAAADPjj179qh79+5JxgsWLKgLFy7YXTddQWV8fLyWLFmiV199VX5+fjp06JBmzpypc+fOyc3Nze4mAAAAAAAAADwbzGazYmNjk4yfOHFCefPmtbtumh+m07NnTy1dulSFCxdW586dtWTJEuXJk8fuDQMAAAAAAAB49jRv3lwffvihli1bJkkymUw6d+6chgwZolatWtldN81B5SeffKIiRYqoWLFi2rp1q7Zu3ZrsvJUrV9rdDAAAAAAAAIDMbcqUKXrzzTeVL18+3bp1S3Xr1tWFCxdUvXp1jR071u66aQ4qO3bsKJPJZPeGAAAAAAAAADz7smfPrk2bNunnn3/WwYMHFRcXp4oVK6pBgwaPVTfNQeX8+fMfa0MAAAAAAAAAnh+1atVSrVq1MqxemoNKAAAAAAAAAC+uW7duKSIiQs2aNZMkhYaGKj4+3rrc0dFRo0ePlrOzs131CSoBAAAAAAAAPNKCBQv0ww8/WIPKmTNnqkyZMnJxcZEkHTt2TAUKFNB7771nV32HDOsUAAAAAAAAwHPrq6++Urdu3WzGFi9erC1btmjLli2aNGmS9Ung9iCoBAAAAAAAAPBIp06dUtmyZa2fnZ2d5eDwf/Fi1apVdfToUbvrc+k3AAAAAAAAgEe6evWqzT0p//rrL5vliYmJNsvTizMqAQAAAAAAADxSoUKFdPjw4RSXHzx4UIUKFbK7PkElAAAAAAAAgEdq0qSJRowYodu3bydZduvWLYWFhalp06Z21+fSbwAAAAAAAACP9P7772vZsmUqWbKkevfuLT8/P0nS8ePHNXPmTN27d0/vv/++3fUJKgEAAAAAAAA8Uv78+bV9+3b16NFDQ4cOlcVikSSZTCa9+uqr+s9//qP8+fPbXZ+gEgAAAAAAAECa+Pj4aP369bp8+bJOnTolSSpRooRy5cr12LUJKgEAAAAAAACkS65cuVS1atUMrcnDdAAAAAAAAAAYjqASAAAAAAAAgOG49BsAAAAAACAd3CbUNWzbs2bN0qxZsyRJvXr1Uq9evQzrBchoBJUAAAAAAADPCMJJPM+49BsAAAAAAACA4QgqAQAAAAAAABiOoBIAAAAAAACA4QgqAQAAAAAAABiOoBIAAAAAAACA4QgqAQAAAAAAABiOoPIFFxgYqH79+qW43GQyafXq1WmuFxkZKZPJpKtXrz52bwAAAAAAAHhxZDG6AWRu58+fV86cOY1uAwAAAAAAAM85gkqkytPT0+gWAAAAAAAA8ALg0m8oMTFRgwcPVq5cueTp6alRo0ZZlz146ffZs2dlMpm0dOlS1ahRQ87OznrppZe0devWJDX37dunypUry9XVVTVq1NDx48dtls+ePVvFixeXk5OTSpYsqUWLFtksN5lMmj17tho3biwXFxcVK1ZMK1asyPDvDgAAAAAAgMyBoBJasGCBsmXLpl27dmnixIn68MMPtWnTphTnDxo0SAMGDND+/ftVvXp1BQUF6dKlSzZzhg0bpilTpmjv3r3KkiWLOnfubF22atUq9e3bVwMGDNDhw4fVvXt3derUSVu2bLGpMXz4cLVq1UoHDhxQhw4d1K5dO8XExGTslwcAAAAAAECmQFAJlStXTiNHjpSvr686duyoypUrKyIiIsX5vXv3VqtWreTv76/Zs2cre/bsmjt3rs2csWPHqm7duipdurSGDh2q7du36/bt25KkyZMnKyQkRD179pSfn5/69++vli1bavLkyTY1WrdurX//+9/y8/PT6NGjVblyZc2YMSPjdwAAAAAAAAAMR1AJlStXzuazl5eXLl68mOL86tWrW99nyZJFlStXTnKm44M1vby8JMlaMyYmRjVr1rSZX7NmzSQ1HtzO/c+POqMyPj5esbGxNq97CQmprgMAAAAAAADjEVRCWbNmtflsMpmUmJiYYTVNJpMkPXbNtBg/fryyZ89u84qIOf3EtwsAAAAAAIDHQ1CJdNu5c6f1/b1797Rv3z75+/uneX1/f39FRUXZjEVFRal06dIpbuf+50dtJzQ0VNeuXbN51fcvnubeAAAAAAAAYIwsRjeAZ8+sWbPk6+srf39/TZ06VVeuXLF5WM6jDBo0SG3atFFAQIAaNGigNWvWaOXKlfrxxx9t5i1fvlyVK1dWrVq19NVXX2n37t1J7oX5MLPZLLPZbDOWxdEx7V8OAAAAAAAAhiCoRLqFh4crPDxc0dHRKlGihL777jvlyZMnzeu/8cYbmjZtmiZPnqy+ffvKx8dH8+bNU2BgoM28sLAwLV26VD179pSXl5eWLFmS5KxLAAAAAAAAPB8IKl9wkZGRScZWr15tfW+xWJIs9/f3165du5KtFxgYmGSdChUqJBnr0aOHevTokWpvBQoU0MaNG1OdAwAAAAAAgOcD96gEAAAAAAAAYDiCSgAAAAAAAACG49JvpJm3t3eyl4I/CU9rOwAAAAAAAMgcOKMSAAAAAAAAgOEIKgEAAAAAAAAYjqASAAAAAAAAgOEIKgEAAAAAAAAYjqASAAAAAAAAgOEIKgEAAAAAAAAYjqASAAAAAAAAgOEIKgEAAAAAAAAYjqASAAAAAAAAgOEIKgEAAAAAAAAYjqASAAAAAAAAgOEIKgEAAAAAAAAYjqASAAAAAAAAgOEIKgEAAAAAAAAYjqASAAAAAAAAgOEIKgEAAAAAAAAYjqASAAAAAAAAgOEIKgEAAAAAAAAYjqASAAAAAAAAgOEIKgEAAAAAAAAYjqASAAAAAAAAgOEIKgEAAAAAAAAYjqASAAAAAAAAgOEIKgEAAAAAAAAYjqASAAAAAAAAgOEIKgEAAAAAAAAYjqASAAAAAAAAgOEIKgEAAAAAAAAYjqASAAAAAAAAgOEIKgEAAAAAAAAYjqASAAAAAAAAgOEIKgEAAAAAAAAYjqASAAAAAAAAgOEIKgEAAAAAAJ5DgYGB6tevX4rLvb299fHHHz+1foBHIagEAAAAAAAAYDiCSgAAAAAAAACGI6gEAAAAAAB4Tt27d0+9e/dW9uzZlSdPHg0fPlwWiyXJvLNnz8pkMik6Oto6dvXqVZlMJkVGRlrHDh8+rMaNG8vNzU358+fXW2+9pb///tu6fMWKFSpbtqxcXFyUO3duNWjQQDdu3HiSXxHPEYJKAAAAAACAZ0R8fLxiY2NtXvHx8SnOX7BggbJkyaLdu3dr2rRp+uijjzRnzhy7tn316lW98sorCggI0N69e7V+/Xr973//U5s2bSRJ58+fV3BwsDp37qyYmBhFRkaqZcuWyQajQHKyGN0AAAAAAAAA0mb8+PEKCwuzGRs5cqRGjRqV7PzChQtr6tSpMplMKlmypA4dOqSpU6eqa9eu6d72zJkzFRAQoHHjxlnHvvjiCxUuXFgnTpxQXFyc7t27p5YtW6po0aKSpLJly6Z7O3hxcUYlAAAAAADAMyI0NFTXrl2zeYWGhqY4v1q1ajKZTNbP1atX18mTJ5WQkJDubR84cEBbtmyRm5ub9VWqVClJ0unTp1W+fHnVr19fZcuWVevWrfX555/rypUr6f+SeGFxRiUAAAAAAMAzwmw2y2w2Z3hdB4d/zmV78DLtu3fv2syJi4tTUFCQJkyYkGR9Ly8vOTo6atOmTdq+fbs2btyoGTNmaNiwYdq1a5d8fHwyvGc8fzijEgAAAAAA4Dm1a9cum887d+6Ur6+vHB0dbcbz5s0r6Z/7TN734IN1JKlixYo6cuSIvL29VaJECZtXtmzZJEkmk0k1a9ZUWFiY9u/fLycnJ61ateoJfDM8jwgqAQAAAAAAnlPnzp1T//79dfz4cS1ZskQzZsxQ3759k8xzcXFRtWrVFB4erpiYGG3dulUffPCBzZxevXrp8uXLCg4O1p49e3T69Glt2LBBnTp1UkJCgnbt2qVx48Zp7969OnfunFauXKm//vpL/v7+T+vr4hnHpd8AAAAAAADPqY4dO+rWrVuqWrWqHB0d1bdvX3Xr1i3ZuV988YW6dOmiSpUqqWTJkpo4caJee+016/ICBQooKipKQ4YM0Wuvvab4+HgVLVpUjRo1koODgzw8PPTTTz/p448/VmxsrIoWLaopU6aocePGT+vr4hlHUAkAAAAAAPAcioyMtL6fPXt2kuVnz561+ezv76/t27fbjD14z0pJ8vX11cqVK5Pdnr+/v9avX29fs4C49BsAAAAAAABAJkBQCQAAAAAAAMBwBJUAAAAAAAAADEdQCQAAAAAAAMBwBJUAAAAAAAAADEdQCQAAAAAAAMBwBJUAAAAAAAAADEdQCQAAAAAAAMBwBJUAAAAAAAAADEdQCQAAAAAAAMBwBJUAAAAAAAAADEdQCQAAAAAAAMBwBJUAAAAAAAAADEdQCQAAAAAAAMBwBJUAAAAAAAAADEdQCQAAAAAAAMBwBJUAAAAAAAAADEdQCQAAAAAAAMBwBJUAAAAAAAAADEdQCQAAAAAAAMBwBJUAAAAAAAAADEdQCQAAAAAAAMBwBJUAAAAAAAAADEdQCQAAAAAAAMBwJovFYjG6CeBJ2rlzp6KmjknT3JrvfZDmuUBm4DahrvV93JCtqc5Lbbk920xvvQd7Tc3DdVP6jin1kdo+SW4/pNRXRu0vGCvXmM90+YNukjL298BI9v4OPu42M/O+u79Pyl+obf3veM33PpCkZP+7PsOnx9NrDgBecGfDmxrdwhPx6dlJhm27u/cgw7YNPGmcUQkAAAAAAADAcASVAAAAAAAAAAxHUAkAAAAAAADAcASVAAAAAAAAAAxHUAkAAAAAAADAcASVAAAAAAAAAAxHUAkAAAAAAADAcASVAAAAAAAAAAxHUAkAAAAAAADAcASVAAAAAAAAAAxHUAkAAAAAAADAcASVAAAAAAAAAAxHUAkAAAAAAADAcASVAAAAAAAAAAxHUAkAAAAAAADAcASVAAAAAAAAAAxHUAkAAAAAAADAcASVAAAAAAAAAAxHUAkAAAAAAADAcASVAAAAAAAAAAxHUAkAAAAAAADAcASVAAAAAAAAAAxHUAkAAAAAAADAcASVAAAAAAAAAAxHUAkAAAAAAADAcASVAAAAAAAAAAxHUAkAAAAAAADAcASVAAAAAAAAAAxHUAkAAAAAAADAcASVAAAAAAAAAAyXxegGAAAAAAAAniXjPylt2LbvFZ6lWbNmSZJ69eqlXr16GdYLkNEIKgEAAAAAAJ4RhJN4nnHpNwAAAAAAAADDEVQCAAAAAAAAMBxBJQAAAAAAAADDEVQCAAAAAAAAMBxBJQAAAAAAAADDEVQCAAAAAAAAMBxBJQAAAAAAAADDEVQCAAAAAAAAMBxBJQAAAAAAAADDEVQCAAAAAAAAMBxBJQAAAAAAAADDEVQCAAAAAAAAMBxBJQAAAAAAAADDEVQCAAAAAAAAMBxBJQAAAAAAAADDEVQCAAAAAAAAMBxBJQAAAAAAAADDEVQCAAAAAAAAMBxBJQAAAAAAAADDEVQCAAAAAAAAMBxBJQAAAAAAAADDEVQCAAAAAAAAMBxBJQAAAAAAAADDEVQCAAAAAAAAMBxBJQAAAAAAAADDEVQCAAAAAAAAMBxBJQAAAAAAAADDEVQCAAAAAAAAMBxBJQAAAAAAAADDEVQCAAAAAAAAMBxBJQAAAAAAAADDEVQCAAAAAAAAMBxBJQAAAAAAAADDEVQCAAAAAAAAMBxBJQAAAAAAAADDEVQCAAAAAAAAMBxBJQAAAAAAAADDPVdB5dmzZ2UymRQdHW10KxnC29tbH3/8cZrmmkwmrV69+on2c9/T2s8hISF64403nug2AAAAAAAAkDk8V0FlZvU0Q0QAAAAAAADgWZSpgso7d+4Y3QIeYO/Pw2Kx6N69exncDQAAAAAAAJ5nhgaVgYGB6t27t/r166c8efKoYcOG2rp1q6pWrSqz2SwvLy8NHTrUJvRKTEzUxIkTVaJECZnNZhUpUkRjx45Ntn5CQoI6d+6sUqVK6dy5c4/sx2Qy6dNPP1WzZs3k6uoqf39/7dixQ6dOnVJgYKCyZcumGjVq6PTp0zbrffvtt6pYsaKcnZ1VrFgxhYWFWXv29vaWJLVo0UImk8n6+fTp03r99deVP39+ubm5qUqVKvrxxx/t2Iv/5++//1aLFi3k6uoqX19ffffddzb7okuXLvLx8ZGLi4tKliypadOm2ax//1LrsWPHqkCBAipZsqQkaffu3QoICJCzs7MqV66s/fv326wXGRkpk8mkdevWqVKlSjKbzfr5558VHx+vPn36KF++fHJ2dlatWrW0Z88em3WPHDmiZs2aycPDQ+7u7qpdu3aS/Xvfnj17lDdvXk2YMOGx9hMAAAAAAAAyH8PPqFywYIGcnJwUFRWlUaNGqUmTJqpSpYoOHDig2bNna+7cuRozZox1fmhoqMLDwzV8+HAdPXpUixcvVv78+ZPUjY+PV+vWrRUdHa1t27apSJEiaepn9OjR6tixo6Kjo1WqVCm1b99e3bt3V2hoqPbu3SuLxaLevXtb52/btk0dO3ZU3759dfToUX366aeaP3++NTy9H8zNmzdP58+ft36Oi4tTkyZNFBERof3796tRo0YKCgpKU6CakrCwMLVp00YHDx5UkyZN1KFDB12+fFnSPwFvoUKFtHz5ch09elQjRozQ+++/r2XLltnUiIiI0PHjx7Vp0yZ9//33iouLU7NmzVS6dGnt27dPo0aN0sCBA5Pd/tChQxUeHq6YmBiVK1dOgwcP1jfffKMFCxbol19+UYkSJdSwYUNrT3/88Yfq1Kkjs9mszZs3a9++fercuXOyZ2Nu3rxZr776qsaOHashQ4bYvY8AAAAAAACQOWUxugFfX19NnDhRkrRw4UIVLlxYM2fOlMlkUqlSpfTnn39qyJAhGjFihG7cuKFp06Zp5syZevvttyVJxYsXV61atWxqxsXFqWnTpoqPj9eWLVuUPXv2NPfTqVMntWnTRpI0ZMgQVa9eXcOHD1fDhg0lSX379lWnTp2s88PCwjR06FBrP8WKFdPo0aM1ePBgjRw5Unnz5pUk5ciRQ56entb1ypcvr/Lly1s/jx49WqtWrdJ3331nE4SmR0hIiIKDgyVJ48aN0/Tp07V79241atRIWbNmVVhYmHWuj4+PduzYoWXLllm/ryRly5ZNc+bMkZOTkyTps88+U2JioubOnStnZ2eVKVNGv//+u3r06JFk+x9++KFeffVVSdKNGzc0e/ZszZ8/X40bN5Ykff7559q0aZPmzp2rQYMGadasWcqePbuWLl2qrFmzSpL8/PyS1F21apU6duyoOXPmqG3btnbtGwAAAAAAAGRuhgeVlSpVsr6PiYlR9erVZTKZrGM1a9ZUXFycfv/9d124cEHx8fGqX79+qjWDg4NVqFAhbd68WS4uLunqp1y5ctb398/ULFu2rM3Y7du3FRsbKw8PDx04cEBRUVE2l58nJCTo9u3bunnzplxdXZPdTlxcnEaNGqUffvhB58+f171793Tr1q3HOqPywd6zZcsmDw8PXbx40To2a9YsffHFFzp37pxu3bqlO3fuqEKFCjY1ypYtaw0pJVnPjnR2draOVa9ePdntV65c2fr+9OnTunv3rmrWrGkdy5o1q6pWraqYmBhJUnR0tGrXrm0NKZOza9cuff/991qxYkWangAeHx+v+Ph4mzHufQoAAAAAAJD5GX7pd7Zs2dI8N62hY5MmTXTw4EHt2LEj3f08GJrdD0yTG0tMTJT0T+AYFham6Oho6+vQoUM6efKkTbj3sIEDB2rVqlUaN26ctm3bpujoaJUtW/axQrWHAz+TyWTtc+nSpRo4cKC6dOmijRs3Kjo6Wp06dUqyvfT8PB6W3nXT8vMsXry4SpUqpS+++EJ379595Pzx48cre/bsNq+FCxemqy8AAAAAAAA8fYYHlQ+6//Aai8ViHYuKipK7u7sKFSokX19fubi4KCIiItU6PXr0UHh4uJo3b66tW7c+0Z4rVqyo48ePq0SJEkleDg7/7N6sWbMqISHBZr2oqCiFhISoRYsWKlu2rDw9PXX27Nkn1mdUVJRq1Kihnj17KiAgQCVKlEjxoTUP8vf318GDB3X79m3r2M6dOx+5XvHixa33Hr3v7t272rNnj0qXLi3pnzNAt23blmoAmSdPHm3evFmnTp1SmzZtHhlWhoaG6tq1azavjh07PrJfAAAAAAAAGCtTBZU9e/bUf//7X7377rs6duyYvv32W40cOVL9+/eXg4ODnJ2dNWTIEA0ePFgLFy7U6dOntXPnTs2dOzdJrXfffVdjxoxRs2bN9PPPPz+xnkeMGKGFCxcqLCxMR44cUUxMjJYuXaoPPvjAOsfb21sRERG6cOGCrly5Iumfe3OuXLlS0dHROnDggNq3b289+/FJ8PX11d69e7VhwwadOHFCw4cPT/IE7uS0b99eJpNJXbt21dGjR7V27VpNnjz5ketly5ZNPXr00KBBg7R+/XodPXpUXbt21c2bN9WlSxdJUu/evRUbG6t27dpp7969OnnypBYtWqTjx4/b1MqXL582b96sY8eOKTg4ONmH7dxnNpvl4eFh83rwUnYAAAAAAABkTpkqqCxYsKDWrl2r3bt3q3z58nrnnXfUpUsXm9Bv+PDhGjBggEaMGCF/f3+1bdvW5j6MD+rXr5/CwsLUpEkTbd++/Yn03LBhQ33//ffauHGjqlSpomrVqmnq1KkqWrSodc6UKVO0adMmFS5cWAEBAZKkjz76SDlz5lSNGjUUFBSkhg0bqmLFik+kR0nq3r27WrZsqbZt2+rll1/WpUuX1LNnz0eu5+bmpjVr1ujQoUMKCAjQsGHDNGHChDRtMzw8XK1atdJbb72lihUr6tSpU9qwYYNy5swpScqdO7c2b96suLg41a1bV5UqVdLnn3+e7D0rPT09tXnzZh06dEgdOnRIcoYqAAAAAAAAnm2GPkwnMjIyyVjdunW1e/fuFNdxcHDQsGHDNGzYsCTLvL29bS4bl6T+/furf//+aern4XWTqxcYGJhkrGHDhtangicnKChIQUFBSWpv3rzZZqxXr142n9NzKfjDPUnS1atXre/NZrPmzZunefPm2cwZP3689f38+fOTrV2tWjVFR0enuL3k9okkOTs7a/r06Zo+fXqKfZcrV04bNmxIdtnD/Xh5eSU52xIAAAAAAADPh0x1RiUAAAAAAACAF9MLE1R+9dVXcnNzS/ZVpkwZo9tL1bPcOwAAAAAAAJAWhl76/TQ1b95cL7/8crLLkrsnYmbyLPcOAAAAAAAApMULE1S6u7vL3d3d6Dbs8iz3DgAAAAAAAKTFC3PpNwAAAAAAAIDMi6ASAAAAAAAAmdL8+fOVI0cO6+dRo0apQoUKj1UzI2rgySCoBAAAAAAAeM6EhITIZDIpPDzcZnz16tUymUyPVXv+/PkymUwymUxycHBQoUKF1KlTJ128ePGx6qbFwIEDFRERkeb5JpNJq1evfqwaeHoIKgEAAAAAAJ5Dzs7OmjBhgq5cuZLhtT08PHT+/Hn9/vvv+vzzz7Vu3Tq99dZbyc5NSEhQYmJihmzXzc1NuXPnNrwGngyCSgAAAAAAgOdQgwYN5OnpqfHjx6c675tvvlGZMmVkNpvl7e2tKVOmPLK2yWSSp6enChQooMaNG6tPnz768ccfdevWLevl2t99951Kly4ts9msc+fOKT4+XgMHDlTBggWVLVs2vfzyy4qMjLSpO3/+fBUpUkSurq5q0aKFLl26ZLM8ucu2v/jiC2v/Xl5e6t27tyTJ29tbktSiRQuZTCbr54drJCYm6sMPP1ShQoVkNptVoUIFrV+/3rr87NmzMplMWrlyperVqydXV1eVL19eO3bseOR+QvoQVAIAAAAAADyHHB0dNW7cOM2YMUO///57snP27dunNm3aqF27djp06JBGjRql4cOHa/78+enalouLixITE3Xv3j1J0s2bNzVhwgTNmTNHR44cUb58+dS7d2/t2LFDS5cu1cGDB9W6dWs1atRIJ0+elCTt2rVLXbp0Ue/evRUdHa169eppzJgxqW539uzZ6tWrl7p166ZDhw7pu+++U4kSJSRJe/bskSTNmzdP58+ft35+2LRp0zRlyhRNnjxZBw8eVMOGDdW8eXNrX/cNGzZMAwcOVHR0tPz8/BQcHGz9vsgYWYxuAAAAAAAAAGkTHx+v+Ph4mzGz2Syz2Zzs/BYtWqhChQoaOXKk5s6dm2T5Rx99pPr162v48OGSJD8/Px09elSTJk1SSEhImno6efKkPvnkE1WuXFnu7u6SpLt37+o///mPypcvL0k6d+6c5s2bp3PnzqlAgQKS/rlX5Pr16zVv3jyNGzdO06ZNU6NGjTR48GBrL9u3b7c5u/FhY8aM0YABA9S3b1/rWJUqVSRJefPmlSTlyJFDnp6eKdaYPHmyhgwZonbt2kmSJkyYoC1btujjjz/WrFmzrPMGDhyopk2bSpLCwsJUpkwZnTp1SqVKlUrTfsKjcUYlAAAAAADAM2L8+PHKnj27zetRl3ZPmDBBCxYsUExMTJJlMTExqlmzps1YzZo1dfLkSSUkJKRY89q1a3Jzc5Orq6tKliyp/Pnz66uvvrIud3JyUrly5ayfDx06pISEBPn5+cnNzc362rp1q06fPm3t5eWXX7bZTvXq1VPs4eLFi/rzzz9Vv379VL9/amJjY/Xnn38muw8e3l8Pfh8vLy9rD8g4nFEJAAAAAADwjAgNDVX//v1txlI6m/K+OnXqqGHDhgoNDU3zWZKP4u7url9++UUODg7y8vKSi4uLzXIXFxebp4vHxcXJ0dFR+/btk6Ojo81cNzc3u3p4eJtPWtasWa3v73+3jHpIEP5BUAkAAAAAAPCMSO0y79SEh4erQoUKKlmypM24v7+/oqKibMaioqLk5+eXJFB8kIODg/VekGkREBCghIQEXbx4UbVr1052jr+/v3bt2mUztnPnzhRruru7y9vbWxEREapXr16yc7JmzZrqmaEeHh4qUKCAoqKiVLduXet4VFSUqlatmtpXwhNAUAkAAAAAAPCcK1u2rDp06KDp06fbjA8YMEBVqlTR6NGj1bZtW+3YsUMzZ87Uf/7znwzdvp+fnzp06KCOHTtqypQpCggI0F9//aWIiAiVK1dOTZs2VZ8+fVSzZk1NnjxZr7/+ujZs2JDq/Smlf57g/c477yhfvnxq3Lixrl+/rqioKL377ruSZA0ya9asKbPZrJw5cyapMWjQII0cOVLFixdXhQoVNG/ePEVHR9tcyo6ng3tUAgAAAAAA/L/27jw8prP/H/hnJoskIotsokiCyFJURKQJlYjUWsKjlCIotdQWRUvRUK3Sp7WUPrT5onjUUkWphraW2kOC0BaNSERbsRRBQoJ5//7IL/MYcyaSSZox8X5d11xX5tzznvvMmTN3zvnMmXOeAu+9957eT5WbNm0q69atkzVr1kjDhg3l3Xfflffee6/cfiL+sGXLlklsbKyMGzdO/Pz8pGvXrnLkyBGpU6eOiIg8//zzkpCQIPPnz5fnnntOfvjhB5kyZUqxz9m/f3+ZN2+e/Oc//5Fnn31WXnrpJZ2rdX/yySfy448/Su3atSUoKEjxOUaPHi1vvvmmjBs3Tho1aiTbtm2TzZs3i6+vb/m9eCoRHlFJRERERERERFTJfPnll3rTvL299a4YLiLSvXt36d69e4mfe8CAAcUWMg21W1lZyfTp02X69OkGs6+99pq89tprOtPGjRun/XvatGkybdo0nfahQ4fK0KFDFZ+vc+fO0rlzZ51pjz6HWq2W+Ph4iY+PV3wOb29vAaAzzcnJSW8alR2PqCQiIiIiIiIiIiKTY6GSiIiIiIiIiIiITI6FSiIiIiIiIiIiIjI5FiqJiIiIiIiIiIjI5FioJCIiIiIiIiIiIpNjoZKIiIiIiIiIiIhMjoVKIiIiIiIiIiIiMjkWKomIiIiIiIiIiMjkWKgkIiIiIiIiIiIik2OhkoiIiIiIiIiIiEyOhUoiIiIiIiIiIiIyORYqiYiIiIiIiIiIyORYqCQiIiIiIiIiIiKTY6GSiIiIiIiIiIiITI6FSiIiIiIiIiIiIjI5FiqJiIiIiIiIiIjI5FioJCIiIiIiIiIiIpNjoZKIiIiIiIiIiIhMjoVKIiIiIiIiIiIiMjkWKomIiIiIiIiIiMjkWKgkIiIiIiIiIiIik2OhkoiIiIiIiIiIiEyOhUoiIiIiIiIiIiIyORYqiYiIiIiIiIiIyORYqCQiIiIiIiIiIiKTY6GSiIiIiIiIiIiITI6FSiIiIiIiIiIiIjI5FiqJiIiIiIiIiIjI5FioJCIiIiIiIiIiIpNjoZKIiIiIiIiIiIhMjoVKIiIiIiIiIiIiMjkWKomIiIiIiIiIiMjkWKgkIiIiIiIiIiIik2OhkoiIiIiIiIiIiEyOhUoiIiIiIiIiIiIyORYqiYiIiIiIiIiIyORYqCQiIiIiIiIiIiKTY6GSiIiIiIiIiIiITI6FSiIiIiIiIiIiIjI5FiqJiIiIiIiIiIjI5FioJCIiIiIiIiIiIpNjoZKIiIiIiIiIiIhMjoVKIiIiIiIiIiIiMjkWKomIiIiIiIiIiMjkWKgkIiIiIiIiIiIik2OhkoiIiIiIiIiIiEyOhUoiIiIiIiIiIiIyORYqiYiIiIiIiIiIyORYqCQiIiIiIiIiIiKTY6GSiIiIiIiIiIiITI6FSiIiIiIiIiIiIjI5FiqJiIiIiIiIiIjI5FioJCIiIiIiIiIiIpNjoZKIiIiIiIiIiIhMjoVKIiIiIiIiIiIiMj0QVWJ3795FfHw87t69WylzpuiTOeYqY84UfTLHXGXMmaJP5pirjDlT9MkccxWRo/KxcOFCBAQEICAgAAsXLjT17BCVKxYqqVLLycmBiCAnJ6dS5kzRJ3PMVcacKfpkjrnKmDNFn8wxVxlzpuiTOeYqIkdE9Dj86TcRERERERERERGZHAuVREREREREREREZHIsVBIREREREREREZHJsVBJlVqVKlUkPj5eqlSpUilzpuiTOeYqY84UfTLHXGXMmaJP5pirjDlT9MkccxWRIyJ6HBUAmHomiIiIiIiIiIiI6OnGIyqJiIiIiIiIiIjI5FioJCIiIiIiIiIiIpNjoZKIiIiIiIiIiIhMjoVKIiIiIiIiIiIiMjkWKomIiIiIiIiIiMjkLE09A0Tl6erVq7J06VI5ePCgZGdni4hIjRo1JDw8XAYMGCBubm4mnkMiqgzu378vv/76q844ExgYKFZWVsXmsrOzJSkpSScXGhoqNWrU+Ef6M5dcRS8XY/szNkdUWubyWTKXHMcYIl1c14joSaYCAFPPBFF5OHLkiLRr107s7OwkOjpaPDw8RETk0qVLsmPHDsnLy5Pt27dLs2bNyrXfw4cP6xVGw8LCpHnz5uXaj6n6KygokE2bNikWf2NiYsTa2vqJ6K8s82mKjbWK3okxl52mJ33DWaPRyLvvviufffaZ5OTk6LQ5OjrKyJEjZfr06aJW6/5gITc3V4YOHSpr1qwRlUol1atXFxGRa9euCQDp3bu3fP7552JnZ1cu/ZlLrqKXi7H9GZsrYi7FlbJkWXgqn5y5fJbMJfe0jDEi5vNZMpdcZR1jyrquVfR+CBE9pUBUSYSGhmLIkCHQaDR6bRqNBkOGDMHzzz9vMJ+fn4+1a9ciLi4OvXr1Qq9evRAXF4d169YhPz9f7/GXLl1Cy5YtoVKp4OXlhebNm6N58+bw8vKCSqVCy5YtcenSJYP9JSUlYd68eZg4cSImTpyIefPmISkpyeDjy9qfMX2mpaWhbt26sLGxQUREBHr27ImePXsiIiICNjY2qF+/PtLS0kzen7G527dvo0+fPrCwsIClpSXc3d3h7u4OS0tLWFhYoG/fvsjNzS3XZWpsn+aSq+jlYmx/xuYmTJgANzc3LF68GBkZGcjLy0NeXh4yMjLw+eefw93dHW+99ZZebtCgQfD19cW2bdtw//597fT79+9j+/btaNCgAQYPHlxu/ZlLrqKXi7H9GZt78OABJk+eDCcnJ6hUKp2bk5MTpkyZggcPHujljP08GNtfWbIVnavoZVPRY6i5fJbMJVfZxxjAfD5L5pKr7GOMsetaeeyHEBGVFAuVVGnY2Njg1KlTBttPnToFGxsbxTZjCl3du3dHWFgYTp8+rfd8p0+fRnh4OF5++WW9NmP/0RvbX1n6jI6ORkxMDHJycvTacnJyEBMTg7Zt25q8P2NzZdkxMPY1VvROTEXnKnq5GNufsTkPDw9s27ZNb3qRbdu2wd3dXW+6k5MT9u/fbzC3b98+ODk5lVt/5pKr6OVibH/G5syluFKWLAtP5TuGmstnyVxylX2MAczns2Quuco+xhi7rpVlP4SIqLRYqKRKw9vbG8uXLzfYvnz5cnh5eSm2GVPosre3x9GjRw32l5ycDHt7e73pxv6jN7a/svRpa2uLkydPGuzzxIkTsLW1NXl/xubKsmNg7Gus6J2Yis5V9HIxtj9jc3Z2djhx4oTB+UxNTUXVqlX1pjs4OODIkSMGc4cPH4aDg0O59WcuuYpeLsb2Z2zOXIorZcmy8FS+Y6i5fJbMJVfZxxjAfD5L5pKr7GOMsetaWfZDiIhKi1f9pkpj/PjxMmTIEBkzZoxs3rxZkpKSJCkpSTZv3ixjxoyRYcOGyVtvvaWY3b9/v7z//vvi4OCg1+bg4CAzZsyQvXv36kyvUqWK3Lx50+D83Lp1S6pUqaI3ffv27fLZZ5+Jn5+fXpufn598+umnsm3bNr02Y/srS59OTk6SmZlpsM/MzExxcnIyeX/G5jQaTbHnrrS2thaNRqPYZuxrNLZPc8lV9HIxtj9jc5GRkTJ+/Hi5evWqXtvVq1fl7bfflsjISL22l156SYYMGSLHjh3Tazt27JgMHz5cOnfuXG79mUuuopeLsf0Zm7t165bUrFlTb3oRT09Pyc3N1Ztu7OfB2P7Kkq3oXEUvm4oeQ83ls2Quuco+xoiYz2fJXHKVfYwxdl0ry34IEVGpmbpSSlSe1qxZg9DQUFhaWmrPC2NpaYnQ0FCsXbvWYM7T0xNbtmwx2L5582Z4enrqTHvjjTfg5eWFDRs26ByJmZOTgw0bNsDb2xsjR47Uey4XFxfs3r3bYF+7du2Ci4uL3nRj+ytLn1OnToWzszPmzJmD1NRUZGdnIzs7G6mpqZgzZw6qV6+O+Ph4k/dnbO7VV19FUFCQ4jfER48eRXBwMPr06aP4Gox9jcb2aS65il4uxvZnbC4rKwsNGzaEpaUlgoKC0L59e7Rv3x5BQUGwtLRE48aNkZWVpZe7du0a2rdvD5VKherVq8Pf3x/+/v6oXr061Go1OnTogOvXr5dbf+aSq+jlYmx/xuY6duyItm3b4sqVK3ptV65cQfv27dGpUye9NmM/D8b2V5ZsRecqetlU9BhqLp8lc8lV9jEGMJ/PkrnkKvsYY+y6Vpb9ECKi0uJVv6lSunfvnvZbcFdX18deae/dd9+VhQsXytSpU6VNmzZ6Vwx///33ZdSoUTJt2jRtJj8/X+Li4mTp0qVy//597beaBQUFYmlpKYMGDZK5c+fqfbs4YsQI2bp1q8ydO1fatGmjPYrz5s2bsmPHDnnzzTflpZdekgULFujkDPWXn58vVlZWBvsrS58iIrNnz5b58+dLdna2qFQqEREBIDVq1JC4uDjFo1Qruj9jc9evX5dXX31Vtm/fLs7OzuLu7i4iIpcvX5YbN25Iu3bt5KuvvlI8GtPY12hsn+aSq+jlYmx/ZVlHNRqNbN++XQ4dOqR31cu2bdvqXQX2YadPn1a8Wqa/v7/BjLH9mUuuopeLiMipU6cUc8X1Z0zuwoUL0rFjRzl9+rQ0atRI53/LyZMnJTAwUL777jupXbu2Ts7Yz4Ox/ZUlW9G5il42FT2GipjPZ8lcchW9XEQqbowRMZ/PkrnknoYxRqT065qx+z1ERMZgoZLo/zO2QHbz5k1JSUnR+UcfHBys+DNykbL/o79586YkJyfLpUuXRETEw8NDmjVrZrC/8uhTRCQjI0PnNfr4+Dwx/ZU1Z8yOQVlfY0XuxIgYt5NmTK6sy6Wi+uMGN1UUcyqusPD0ZBSeiErLXD5L5pKr6OUiYj5jTGn3e4iIjMFCJdEjjC2QlVZ5/aO3traW1NRUCQgIqLA+S+rRouo/2d/Fixdl0aJFsm/fPrl48aKo1WqpW7eudO3aVQYMGCAWFhbl3qcIN9gMMcW6Zkx/xq6jhw8f1tuBCQ8Pl5CQEKPm//r167JlyxaJjY1VbNdoNIo7OBqNRv744w+pU6eOXhsAyczMlNq1a4ulpaUUFBTIxo0bJT8/Xzp27Ciurq4lnr+oqChZtmyZeHl5lTiTkZEhZ8+eFU9PT2nYsKHiY/Lz80WtVmuPek9PT5elS5dKVlaWeHl5yaBBgxTH4G+++UY6dOggdnZ2JZ6fIqmpqZKSkiKRkZFSt25d+fXXX+Wzzz4TjUYj3bp1k3bt2hnM7ty5U2+M6dKli/j6+pZ6PoiKwzHm8TjGEBmnoKBANm3apDjGxMTEFHvuSyKiCmGaX5wTmZesrCwMHDhQb3peXh727t2LX3/9Va/tzp07Bq9C/ttvv2Hp0qU4deoUAODUqVMYNmwYBg4ciB07dihmxo4dq3hTq9WIjY3V3i+J27dvY+nSpXjnnXewcOFCXL16VfFxKSkpOHfunPb+ihUrEB4ejlq1aqFFixZYvXq1Ym7kyJHYs2dPieblUQsWLEC/fv20z71ixQoEBATAz88PkyZNwr179/QyR44cgaOjI4KDg9GyZUtYWFigX79+eOWVV+Dk5ITw8HDcvHlTsb/8/HysXbsWcXFx6NWrF3r16oW4uDisW7cO+fn5Rr2Gkrhw4QJu3bqlN72goAA///yzYubq1avYuXMn/v77bwCF5zyaNWsWpk+fjt9++61U/fv4+OD3338v8eM1Gg127tyJL774Alu2bEFBQUGp+nucjz/+GJmZmeX6nP+ES5cuoWXLllCpVPDy8kLz5s3RvHlzeHl5QaVSoWXLlrh06VKpn/f48eNQq9V603NyctCjRw/Y2NjA3d0dU6dOxf3797Xt2dnZirnTp0/Dy8sLarUa9evXx7lz5xAcHIyqVavCzs4Orq6uiu//t99+q3izsLDAwoULtfcfNXz4cO36nJeXh+7du2vPE6xWq9G6dWvF9T0iIgJff/01gMIrlFapUgWNGzfGK6+8gqCgINjZ2eHAgQN6OZVKBQcHB7z++us4dOhQMUtW1zfffAMLCwu4uLjA3t4eP/74I5ycnBAdHY127drBwsICq1at0stdunQJzZs3h1qthqWlJdRqNYKDg1GjRg1YWFhgwoQJxfablJSEefPmYeLEiZg4cSLmzZuHw4cPl3i+H3Xt2jWD/18A4MGDBwannz9/3mBOo9Hg3Llz2nE2Pz8fa9aswfLlyxXPu1ac1q1bl/ozfe7cOfzwww84efKkwcfcvXtXZ/w5e/Ys3nnnHfTt2xeTJ0/W+Z/1sPXr1yM3N7dU81Pk+PHjWLJkCdLT0wEAv/zyC4YPH46hQ4cWe5VfANixYwemT5+OYcOG4Y033sDHH39c7NjLMYZjjDFjDGAe4wzHGGUVOcakpaWhbt26sLGxQUREBHr27ImePXsiIiICNjY2qF+/PtLS0kr9GrKzszF9+vRS54iIlLBQSVQCShv4Z86c0e44qNVqtGrVCn/++ae23dDGfWJiIqytrVG9enXY2NggMTERbm5uiI6ORlRUFCwsLBSLlSqVCk2aNEFkZKTOTaVSISQkBJGRkWjdurXi/AcEBGiLW1lZWfD29oajoyNCQkJQvXp1uLu7K254NW7cGD/++CMAICEhAba2thg9ejQWLVqEuLg42NvbY8mSJYrzqlar4evri1mzZuHixYvFLN3/mTFjBqpVq4bu3bujRo0amDVrFlxcXPD+++9j5syZcHNzw7vvvquXa9GiBaZNm6a9v3LlSoSGhgIo3NBu0qQJRo8erZf7pzbWAMMbbH/99RdCQkKgVqu1BdWHd64MrTdJSUlwdHSESqWCs7MzkpOT4ePjA19fX9SrVw+2trZISUnRy82fP1/xZmFhgUmTJmnvP6pDhw64ceMGAODvv/9GaGgoVCoV3NzcoFar4e/vj8uXL+vlLly4oLOzsWfPHrz66qto2bIl+vTpo7hDCBSuMxYWFoiOjsaaNWtKVSTesmULpk6din379gEo3Gjv0KED2rVrh88//9xgLi8vD0uWLMHAgQPRvn17dOzYESNHjsRPP/1kMNO9e3eEhYXh9OnTem2nT59GeHg4Xn75Zb22nJycYm979+5VfN9Hjx6NBg0a4Ouvv0ZCQgK8vLzQqVMn7fLJzs6GSqXSy8XExKBLly44ceIE4uLiEBAQgJiYGBQUFODu3bvo3Lkz+vbtq5cr+uwWFQCUbkrzqVartcWTSZMmoVatWti5cydyc3Oxb98+1KtXDxMnTtTLOTg4aHeoIiIi9L5smTJlClq0aKE4n++99x6CgoKgUqnw7LPPYu7cuQa/dCnStGlTvP/++wCA1atXw8nJCe+99562/eOPP0aTJk30cq+88gq6du2KnJwc3L17FyNHjkRsbCyAwvXNxcUF8+bN08uZS9EJYOHJkIouPHGM4RhTmjEGMJ9xhmOMsooeY6KjoxETE6NzQZwiOTk5iImJQdu2bUs8/0UMrS9ERMZgoZIIhjeCim5z587V++fbtWtXdOrUCVeuXEFaWho6deoEHx8f7bfIhjbUwsLCMHnyZACFG7HOzs545513tO0TJ07Eiy++qJf78MMP4ePjo1fEtLS0VDyi82EqlUq7kdqnTx+Eh4dri1C3bt1CdHQ0evfurZeztbXVflsdFBSEL774Qqd91apVCAwMVOzvp59+wpgxY+Dq6gorKyt06dIFW7ZsMfgNPADUq1cP33zzDYDCDR4LCwv897//1bZv2LAB9evXV5zPom+hgcJv862srJCdnQ0A+OGHH1CzZk293D+1sVY0/0rvf2xsLEJDQ3HkyBH8+OOPCA4ORrNmzXDt2jUAhncKo6OjMXjwYNy8eRP//ve/UatWLQwePFjbPnDgQHTt2lUvp1KpUKtWLXh7e+vcVCoVnnnmGXh7e8PHx0cxV7TODB8+HIGBgdpi9oULFxAcHIxhw4bp5Zo3b44tW7YAADZt2gS1Wo0uXbrg7bffRrdu3WBlZaVtf7S/ZcuWISYmBlZWVnBxccGYMWOKPeoBABYvXgxLS0sEBwfDwcEBK1euRLVq1TB48GAMHToUtra2ijt3aWlp8PLygru7O2rXrg2VSoVOnTohNDQUFhYW6NGjh+LRu/b29opX2CySnJwMe3t7xdenVqsN3gztnNepUwe7du3S3r9y5QqaN2+Otm3b4u7duwbHGTc3Nxw7dgxA4RHUKpUKe/fu1bbv378fderU0csVXZX00Z3ax40zD68vDRs2xFdffaXT/u2336JBgwZ6uapVq2qPLPfw8MDx48d12s+ePWtweRb1l5ycjOHDh8PJyQlVqlRBjx498MMPPyjOZ9WqVZGRkQGg8MgeKysrnDhxQtuenp6u2J+DgwN++eUX7f3bt2/DyspKO3asXLkSfn5+ejlzKToBLDwZUtGFJ44xyjjGKI8xgPmMMxxjlFX0GGNra1vsttWJEydga2urNz01NbXY29q1a1moJKJyw0IlEYzbCHJ3d9fZ+NRoNBg2bBjq1KmD9PR0gxv3Dg4O2qP0Hjx4AEtLS52dkpMnT8LDw0NxPg8fPowGDRpg3Lhx2p+llLZQWbduXb0N7P3796N27dp6ORcXFyQnJ2tfr9IGvtLGzMP9FRQUYO3atdpvhWvWrIl33nlH8UhFW1tbnZ8LWVlZ6Wy4Z2Zmws7OTi/n5eWlPZoOKDxqUaVSIS8vDwCQkZEBGxsbxf6M2VgDjN9gq1mzJpKSkrT3izbOmzRpgr///tvgeuPs7Kz9eXdBQQHUarXO86SkpOCZZ57Ryw0dOhRNmjTR+2l4aXYK/fz89I5W+OmnnxQLnFWrVtUWNENDQzFr1iyd9gULFiAoKKjY/i5duoTZs2fD398farUaISEh+OKLLxR/vh8YGKgtoO/cuRM2Njb47LPPtO3Lli1DQECAXq5Dhw4YOnQoNBoNAGDWrFno0KEDAOD333+Ht7c34uPj9XIuLi7YvXu33vQiu3btgouLi950BwcHzJ49G7t371a8JSQkKL7vtra2ekc737x5E2FhYYiKisK5c+cM5h7+LNnb2+Ps2bPa+1lZWahSpYria5gzZw5q166tU1AuyfpSdIStq6urzucWKPzsKn2WoqKi8NFHHwEAwsPD9X5muH79esVix8PrS5E7d+5gxYoViIyMhFqthre3t16uRo0a2jHt2rVrUKlUOkWaw4cPo0aNGno5Nzc3ndefl5cHtVqtPVI9PT1dcXmaS9Gp6DWy8KSvogtPHGOUcYxRHmMA8xlnOMY8GWOMp6en4hfGRTZv3gxPT0/F12doX6m49YWIyBgsVBKhsHi0adMmg+3Hjh3T++dbrVo1xfMCjhgxArVq1cKePXsMFiof3pi3t7fXORowMzNTsahW5NatW4iNjUXjxo1x8uRJWFlZlahQWbSBX7NmTb3inKE++/bti0GDBgEAevTogSlTpui0z5w5E40aNVLsT+lnRufPn0d8fLz2pz+P8vHxQWJiIoDCYpFarca6deu07Vu3blXcMRgzZgwaNmyIxMRE7Ny5E61bt0ZkZKS2fdu2bahXr55eztiNtaLXaMwGW9WqVfV+2nTv3j107doVjRs3xokTJwzmijZkAf315vz58wbXmw0bNqB27dpYsGCBdlppdgrd3d0VdwqVdpocHR2RmpqqzRX9XeTs2bOKxWZD68yePXvQv39/VK1aFVWrVtVrVypuP7x+Z2RkKPZnZ2en8z7k5+fDyspKe+TDpk2bFNe1N954A15eXtiwYYPOkbg5OTnYsGEDvL29MXLkSL1cZGQkZs+erTe9yPHjxxWPQPHz88PWrVv1pt+6dQthYWF47rnnFNeXevXq6ewA/uc//9Ep9KakpCjuLBc5duwYAgMDMWTIEOTm5pZofRk6dCjGjh0Ld3d3vR2ylJQUuLq66uUOHDgAR0dHxMfHY8GCBXB1dcWUKVOwatUqvPvuu3ByclJcbg8fKaMkLS1N50j1In379kVoaCj++9//onPnzmjXrh2ef/55nDp1CqdPn0ZERITikUfdunVD9+7dcfv2bRQUFCAuLk7n6O5Dhw4pLk9zKToVZVl4Mn3hiWOMMo4xymMMYD7jDMeYJ2OMmTp1KpydnTFnzhykpqYiOzsb2dnZSE1NxZw5c1C9enWDX9QuWbIEmZmZiretW7eyUElE5YaFSiIAnTt3xtSpUw22K23gh4SEYMWKFYqPHzFiBJycnBT/YTdu3FhbjAMKj6B8+Ceme/bsUTxS7VGrV6+Gh4cH1Gp1iQqVjRo1QlBQEOzt7bF+/Xqd9p9//lnxaLw///wT3t7eaNWqFd58803Y2tqiZcuWeP3119GqVStYW1sr7uAYKjoV0Wg0it8sT5kyBW5ubhg8eDB8fHwwceJE1KlTB4sWLcLixYtRu3ZtxQsG3bp1Cz179oSlpSVUKhXCw8N1NqK3b9+uU/AsYuzGGmD8BlujRo30lj/wv2JlnTp1FHP+/v46P/v/7rvvtEeMAoU7MbVq1VKcVwD4448/EBUVhfbt2+PixYsl2sDv2LEjunXrBmdnZ72C7qFDhxSP/O3SpYv2p1Tt2rXTO/9lQkICfH199XKP2ynMycnRO/UAAO2XAkDh+qpSqXTWyd27dysul5o1a+qc0/P69etQqVTaHe1z584pbuDfvXsXw4YNg7W1NdRqNWxsbGBjYwO1Wg1ra2sMHz4cd+/e1ct98cUXiucCLZKdna1zntUio0aNUtypBQp3DENDQxXXl6FDhyIhIcFgfx9++CE6duxosB0o3OkZOnQofH19YWFhUez6EhERoXPu3Ef7njFjBiIiIhSzBw4cwPPPP69X8H/mmWcMnpPtcWOMIdnZ2XjxxRdhb2+Pdu3a4caNGxg5cqT2iwVfX1+dHeci6enpqFevHiwtLWFlZQUnJyft+XuBwiN3lX5CaC5FJ4CFpyel8GRojFGpVBxjOMYo9mku4wzHmCdjjAEKf0Xi6empc1StSqWCp6enwXWibdu2mDFjhsHXZ2h9ISIyBguVRCgsDj5cPHzU7du39b6tnjlzpvanokqGDx+u+A970aJF+O677wzmJk2apD2K8XEuXLiATZs24fbt28U+btq0aTq3R68gOH78ePTq1Usxe/36dbz99tsIDAyEjY0NrK2t4eXlhVdffRVHjhxRzHh7ez/2nDxKHjx4gA8++AAvvfQSZs6cCY1Gg9WrV6N27dpwcXHBgAEDin2td+7cUTwxenGM2VgDjN9ge+uttwye9/LevXvo0qWLYm7atGkGr7IOAO+88w7+9a9/GWwHCgvEM2fO1J5ovbgN/AEDBujc1q5dq9M+YcIEtGvXTi/322+/wcXFBbGxsZgxYwbs7e3Rt29ffPDBB4iNjUWVKlWwbNkyvZyxO4UjRoyAr68v3n//fTRv3hz9+/eHv78/EhMTsW3bNjRq1AivvfaaXq5///6IiIjAqVOncO7cOe1J8ovs3r1b8XQIRXJycrBz50589dVX+Oqrr7Bz507Fc52W1bVr1/SOAnnYzZs3iz2SxpBz587hr7/+KtFjv/32W8TFxRn1/hRJT0/HhQsXin3M5cuXcejQIRw4cEDn6GElmZmZ2p/tl4f09HS9L40elZubi+3bt2PLli0lvjptWQrbhgooQPkXnQAWngwprvCkUqn+kcITUDjG7NixQzvG7Nixw6gx5nGfE0NjTFGutGNMUa60Y8zo0aONen+K+ivtGGPoys1FMjMzFc+pbey4Y+wYU5L++AWassowxpS2uP1wMfZxYwxQ+Dk9cOBAiT4TGzZswMqVKw22X7t2DV9++eVjXi0RUcmoAECIiJ5yGRkZkp2dLSIiNWrUEB8fn2Ifv3HjRsnNzZW+ffsqtl+/fl02b94s/fv315l+//59ycvLEwcHB8Xc/fv35c8//xQvL69SzX9eXp5YWFhIlSpVHvvYlJQU2bdvn8TGxoqzs3Op+imSm5srFhYWYmNjo9eWnp4uU6ZMka1bt8rt27dFRMTS0lJCQkJkwoQJ0rVrV6P6NDQfY8eOlYMHD0p4eLgsWLBAPv30U5k8ebLcu3dPIiIiZO3ateLu7q6Tu3z5ssTExEhSUpKoVCqpXbu2bNy4UYKCgkREZP369XLx4kUZNWpUuc0rPb1u3rwpKSkpOmNMcHCwwXHAWNevX5e//vpLnn32WcX2W7duydGjRyUiIqLUz52RkSE2Njbi6en52Mdu3rxZdu3aJZMmTdL77JXUuXPnxNraWmrVqmXwMVeuXJFz586JRqMRT09P8fb2NvjY8+fPS506dUSlUhk1P0rzl5eXJ/7+/mJpaan4mLy8PNm/f7/k5+fL888/L66urkb3Z21tLampqRIQEMAcc4rMfZwpzRizZcsW2blzJ8eYchxjiIieJCxUEhEZcOHCBYmPj5elS5c+8X0+iTkAcvnyZdFoNOLq6ipWVlal6qMs83n37l25d++eVKtWrdjHpaWlSX5+frE7Ao+6c+eOpKSkSPXq1SUwMFCv33Xr1klsbCxzzImIyKlTp+TQoUMSFhYm/v7+cvr0aZk/f77k5+dL3759JSoqSi9jilx59zlv3jwpKCgoUS48PFz8/PxK3Z+xOWNfX0X09+abbyo+1/z586Vv377i4uIiIiJz5sxhjjlFubm5sm7dOjl79qzUrFlTevXqpc2XNOfp6Sm9e/dmTky7PP+J/o4ePSrOzs7aL+RXrlwpixcvlqysLPHy8pKRI0dKr169HtsfEdE/yqTHcxIRPcGOHz9u9InBs7KyMHDgwArr01xyFb1cjO3PUO7MmTPw8vLS/iSrVatW+PPPP7Xthq52qpR7+GeRzFXOXGJiIqytrVG9enXY2NggMTERbm5uiI6ORlRUFCwsLHTOPWuqnDnNa2XPqVQqNGnSROcnq5GRkVCpVAgJCUFkZCRat27NHHNaAQEB2guoZGVlwdvbG46OjggJCUH16tXh7u6u+LNec815eXlVaK6yvb7GjRtrT0ORkJAAW1tbjB49GosWLUJcXBzs7e2xZMkSvVxKSorO861YsQLh4eGoVasWWrRoUezpiYiISouFSiJ6an377bfF3ubOnWt0odJQYc3YPs0lV9HLxdj+jM117doVnTp1wpUrV5CWloZOnTrBx8dHeyVTQwUr5p7OXFhYGCZPngyg8AJozs7OOhdTmDhxIl588UWT58xpXit77sMPP4SPj49eEfNxFw5h7unMAbrnRuzTpw/Cw8Nx48YNAIUXuImOjkbv3r2ZYw5A4dXXMzMzAQBBQUF6FypctWoVAgMD9XLGFjiJiIzBQiURPbWKjo569GTpD98MFbmMLawZ26e55Cp6uVR0Idbd3R0nTpzQ3tdoNBg2bBjq1KmD9PR0gwUr5p7OnIODA9LS0gAUXizM0tISR48e1bafPHkSHh4eJs+Z07xW9hwAHD58GA0aNMC4ceNQUFAAoGQFK+aeztzDBau6devqXd16//79iheHY+7pzLm4uCA5ORlA4f+248eP67SfPXsWtra2ejljC5xERMZgoZKInlo1a9bEpk2bDLYfO3bMYKHS2MKasX2aS66il0tFF2KrVauG3377TW/6iBEjUKtWLezZs4c55rQcHBx0rtZqb2+P9PR07f3MzEzY2NiYPGdO81rZc0Vu3bqF2NhYNG7cGCdPnoSVldVjC1bMPZ05lUqFy5cvAyj8X3ry5EmddkPrGnNPZ65v374YNGgQAKBHjx6YMmWKTvvMmTPRqFEjvZyxBU4iImOoTX2OTCIiUwkODpaUlBSD7SqVSmDgemOenp6yYcMG0Wg0irejR4+Wa5/mkqvo5WJsf8bm/P39JTk5WW/6woULJSYmRrp06cIcc1re3t6SlpamvX/w4EGpU6eO9n5WVpbiFW4rOmdO81rZc0Xs7e1l+fLlMmnSJImOjpYHDx4YfCxzzLVp00aaNm0qN2/elDNnzui0nT9/3uDFWJh7+nKzZ8+WHTt2SEREhNSuXVs++eQTeeGFF2TIkCESEREh06ZNk1mzZunlOnToIIsWLRIRkYiICFm/fr1O+7p166R+/fqK80lEVFolu8QpEVElNGHCBMnNzTXYXr9+fdm1a5diW1FhLSYmRrHdUGHN2D7NJVfRy8XY/ozNdevWTVavXi39+vXTa1u4cKFoNBpZvHgxc8yJiMjw4cN1Cg0NGzbUaU9MTFS88nNF58xpXit77lG9evWSli1bSkpKinh5eT328cw9fbn4+Hid+/b29jr3t2zZIi+88AJzzImISM2aNeXYsWMya9Ys2bJliwCQw4cPy4ULF6RFixayf/9+adasmV5u9uzZ0qJFC4mIiJBmzZrJJ598Irt375aAgAA5c+aMHDp0SDZu3KiXIyIyhgqGDhciIiKD9u7dK7m5udK+fXvF9tzcXElOTpaIiIgKnjPTqujlYmx/fP+IiIiISu7GjRvaAue5c+dEo9GIp6entGjRQsaOHatY4CQiMgYLlURERERERERERGRyPEclERERERERERERmRwLlURERERERERERGRyLFQSERERERERERGRybFQSURERE+EAQMGSNeuXU09G2avoKBA6tevLwcOHDD1rJRJadaHiRMnyqhRo/7ZGSIiIiKif5ylqWeAiIiIKj+VSlVse3x8vMyfP19MfY2/AQMGyI0bN2TTpk0mnY+yWLx4sfj4+Eh4eLipZ6XCjB8/XurWrStjx46VunXrmnp2iIiIiMhIPKKSiIiI/nEXL17U3ubNmycODg4608aPHy+Ojo7i5ORk6lk1awBk4cKFMmjQIFPPSoVydXWVdu3ayaJFi0w9K0RERERUBixUEhER0T+uRo0a2pujo6OoVCqdafb29no/9Y2MjJRRo0ZJXFycODs7i4eHhyQkJEhubq4MHDhQqlWrJvXr15fExESdvn755Rfp0KGD2Nvbi4eHh/Tr10+uXr2qbV+/fr00atRIbG1txcXFRaKjoyU3N1emTZsmy5cvl2+//VZUKpWoVCrZvXu3iIi8/fbb0qBBA7Gzs5O6devK1KlT5d69e9rnnDZtmjRp0kSWLl0qderUEXt7e3njjTfkwYMH8tFHH0mNGjXE3d1dPvjgA515ValUsmjRIunQoYPY2tpK3bp1Zf369dr2goICGTlypHh6eoqNjY14eXnJhx9+aHA5p6SkSHp6unTq1KnEz3Hjxg0ZPHiwuLm5iYODg0RFRUlqaqrO827ZskVCQkLExsZGXF1dpVu3btq269evS2xsrDg7O4udnZ106NBB0tLStO1ffvmlODk5yfbt2yUgIEDs7e2lffv2cvHiRe1jHjx4IG+++aY4OTmJi4uLvPXWW3pH1xp634p07txZ1qxZY3DZEBEREdGTj4VKIiIiemItX75cXF1d5fDhwzJq1CgZPny49OjRQ8LDw+Xo0aPStm1b6devn+Tl5YlIYdEtKipKgoKCJDk5WbZt2yaXLl2Snj17ikjhkZ29e/eW1157TU6dOiW7d++Wf/3rXwJAxo8fLz179tQW0S5evKj9+XS1atXkyy+/lN9++03mz58vCQkJMnfuXJ15TU9Pl8TERNm2bZusXr1alixZIp06dZI//vhDfv75Z5k9e7ZMmTJFkpKSdHJTp06V7t27S2pqqvTp00d69eolp06dEhGRTz/9VDZv3izr1q2TM2fOyKpVq8Tb29vg8tq7d680aNBAqlWrpp32uOfo0aOHXL58WRITEyUlJUWaNm0qbdq0kWvXromIyNatW6Vbt27SsWNHOXbsmOzYsUOaN2+uzQ8YMECSk5Nl8+bNcvDgQQEgHTt21Cnk5uXlyccffywrV66UPXv2SFZWlowfP17b/sknn8iXX34pS5culX379sm1a9dk48aN2vbi3rcizZs3lz/++EMyMzMNLh8iIiIiesKBiIiIqAItW7YMjo6OetP79++PmJgY7f2IiAi0bNlSe//+/fuoWrUq+vXrp5128eJFiAgOHjwIAJgxYwbatm2r87wXLlyAiODMmTNISUmBiCAzM1Nx3h6dB0P+/e9/Izg4WHs/Pj4ednZ2uHnzpnZau3bt4O3tjQcPHmin+fn54cMPP9TeFxEMGzZM57lDQ0MxfPhwAMCoUaMQFRUFjUbz2HkCgDFjxiAqKkpnWnHPsXfvXjg4OODu3bs60+vVq4fPP/8cABAWFoY+ffoo9vf7779DRLB//37ttKtXr8LW1hbr1q0DUPh+iwjOnj2rfcxnn30GDw8P7X1PT0989NFH2vv37t1DrVq1tO/F4943AMjJyYGIYPfu3QYfQ0RERERPNh5RSURERE+sxo0ba/+2sLAQFxcXadSokXaah4eHiIhcvnxZRERSU1Nl165dYm9vr735+/uLSOERj88995y0adNGGjVqJD169JCEhAS5fv36Y+dj7dq10qJFC+3P1KdMmSJZWVk6j/H29tY5ktHDw0MCAwNFrVbrTCua1yJhYWF694uOqBwwYIAcP35c/Pz8ZPTo0fLDDz8UO5937twRGxsbnWnFPUdqaqrcvn1bXFxcdJZZRkaGpKeni4jI8ePHpU2bNor9nTp1SiwtLSU0NFQ7zcXFRfz8/LSvQUTEzs5O6tWrp73v6empXQ45OTly8eJFneewtLSUZs2aae+X5H2ztbUVEdEeXUtERERE5oeFSiIiInpiWVlZ6dxXqVQ604quJq7RaERE5Pbt29K5c2c5fvy4zi0tLU1atWolFhYW8uOPP0piYqIEBgbKggULxM/PTzIyMgzOw8GDB6VPnz7SsWNH+e677+TYsWMyefJkKSgoKNW8Fk0rmteSaNq0qWRkZMiMGTPkzp070rNnT3n55ZcNPt7V1VWvgFfcc9y+fVs8PT31lteZM2dkwoQJIvK/AmBZKC0HlOIK7yV534p+qu7m5lbm+SUiIiIi02ChkoiIiCqNpk2byq+//ire3t5Sv359nVvVqlVFpLBI1qJFC5k+fbocO3ZMrK2ttedDtLa2lgcPHug854EDB8TLy0smT54szZo1E19fXzl//ny5zfOhQ4f07gcEBGjvOzg4yCuvvCIJCQmydu1a+eabb7RFuUcFBQXJ6dOn9YqAhp6jadOmkp2dLZaWlnrLy9XVVUQKj2rdsWOHYn8BAQFy//59nfNu/v3333LmzBkJDAws0et3dHQUT09Pnee4f/++pKSk6DyuuPdNpPAiSlZWVvLss8+WqF8iIiIievJYmnoGiIiIiMrLiBEjJCEhQXr37i1vvfWWVK9eXc6ePStr1qyR//u//5Pk5GTZsWOHtG3bVtzd3SUpKUmuXLmiLQx6e3vL9u3b5cyZM+Li4iKOjo7i6+srWVlZsmbNGgkJCZGtW7fqFMjK6uuvv5ZmzZpJy5YtZdWqVXL48GFZsmSJiIjMmTNHPD09JSgoSNRqtXz99ddSo0YNcXJyUnyu1q1by+3bt+XXX3+Vhg0bPvY5oqOjJSwsTLp27SofffSRNGjQQP766y/tBXSaNWsm8fHx0qZNG6lXr5706tVL7t+/L99//728/fbb4uvrKzExMfL666/L559/LtWqVZOJEyfKM888IzExMSVeBmPGjJFZs2aJr6+v+Pv7y5w5c+TGjRva9qSkpGLfN5HCCwm98MIL5XIEKBERERGZBo+oJCIiokqjZs2asn//fnnw4IG0bdtWGjVqJHFxceLk5CRqtVocHBxkz5490rFjR2nQoIFMmTJFPvnkE+nQoYOIiLz++uvi5+cnzZo1Ezc3N9m/f7906dJFxo4dKyNHjpQmTZrIgQMHZOrUqeU2z9OnT5c1a9ZI48aNZcWKFbJ69Wrt0YjVqlWTjz76SJo1ayYhISGSmZkp33//vc55Lx/m4uIi3bp1k1WrVmmnFfccKpVKvv/+e2nVqpUMHDhQGjRoIL169ZLz589rz/8ZGRkpX3/9tWzevFmaNGkiUVFRcvjwYe3zL1u2TIKDg+Wll16SsLAwASDff/+93s+9izNu3Djp16+f9O/fX8LCwqRatWrSrVs3bfvj3jcRkTVr1sjrr79e4j6JiIiI6MmjQmlOEERERERE5UalUsnGjRula9eu5facJ06ckBdffFHS09PF3t6+3J73SZaYmCjjxo2TEydOiKUlfzBEREREZK54RCURERFRJdK4cWOZPXt2sRcIqmxyc3Nl2bJlLFISERERmTkeUUlERERkIv/EEZVEREREROaKXzsTERERmQi/LyYiIiIi+h/+9JuIiIiIiIiIiIhMjoVKIiIiIiIiIiIiMjkWKomIiIiIiIiIiMjkWKgkIiIiIiIiIiIik2OhkoiIiIiIiIiIiEyOhUoiIiIiIiIiIiIyORYqiYiIiIiIiIiIyORYqCQiIiIiIiIiIiKTY6GSiIiIiIiIiIiITO7/Ac4YbxTQUA4XAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hiphop\n",
        "Interestingly enough, Eminem's Lose Yourself track opens up with a melody, therefore correctly classifying it as 'classical'. At the 30 seconds mark, approx, a guitar riff is played which ressembles that of a blues riff (personal opinion). After the first minute, approx, the actual beat of the track kicks in, classifying it correctly as hip hop. The 'classical' predictions is due to the fact that during the whole track, melodies of classical music (piano instrument) can be heard.\\\n",
        "\n",
        "### Blues\n",
        "While correctly the model disregards the hiphop genre, the complete absence of rock_metal_hardrock is not tha pleasant, even if the track does not ressemble such. The classical misclassification might be due to the heavy presence of piano on the background and the consistent low decibels of the track. The periodic presence of blues classification is a good sign that model can identify these signals. Unfortunately, if we were to label this track as one genre, we would have to misclassify it as a 'classical' song, which is incorrect.\n",
        "(**Edit**: Some times a model is created which classifies blues over classical for this specific track. This behavior doesn't reflect well the accuracy and f1 score of the test dataset)\n",
        "\n",
        "### Classical\n",
        "I can't say much about this, since the model corectly classifies the whole track as 'classical'.\n",
        "\n",
        "### Rock-Metal-HardRock\n",
        "This is where things go completely wrong and the model shows the incapability of classifying such a genre.\n",
        "\n",
        "### Overall, more data would provide way better results in my opinion, since CNNs weaknesses and strengths come from having large data to capture complex patters"
      ],
      "metadata": {
        "id": "zGuERpWmWT_k"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e5UE1TNKa3t5"
      },
      "execution_count": 31,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}